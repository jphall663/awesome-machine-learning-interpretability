# Awesome Machine Learning Interpretability [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A maintained and curated list of practical and awesome responsible machine learning resources.

If you want to contribute to this list (*and please do!*), read over the [contribution guidelines](contributing.md), send a [pull request](https://github.com/jphall663/awesome-machine-learning-interpretability/compare), or file an [issue](https://github.com/jphall663/awesome-machine-learning-interpretability/issues/new).

If something you contributed or found here is missing, please check the [archive](https://github.com/jphall663/awesome-machine-learning-interpretability/blob/master/archive).

![](HR-logo-350x100.png)

Maintenance and curation sponsored by [HallResearch.ai](https://www.hallresearch.ai).

## Contents

* **Community and Official Guidance Resources**
  * [Community Frameworks and Guidance](#community-frameworks-and-guidance)
    * [Infographics and Cheat Sheets](#infographics-and-cheat-sheets)
    * [AI Red-Teaming Resources](#ai-red-teaming-resources)
    * [Generative AI Explainability](#generative-ai-explainability)
    * [University Policies and Guidance](#university-policies-and-guidance)
  * [Official Policy, Frameworks, and Guidance](#official-policy-frameworks-and-guidance)
    * [Australia](#australia)
    * [Brazil](#brazil)
    * [Canada](#canada)
    * [China](#china)
    * [Colombia](#colombia)
    * [Costa Rica](#costa-rica)
    * [Denmark](#denmark)
    * [Finland](#finland)
    * [France](#france)
    * [Germany](#germany)
    * [Hong Kong](#hong-kong)
    * [Iceland](#iceland)
    * [India](#india)
    * [Ireland](#ireland)
    * [Jamaica](#jamaica)
    * [Japan](#japan)
    * [Kenya](#kenya)
    * [Malaysia](#malaysia)
    * [Mexico](#mexico)
    * [Moldova](#moldova)
    * [Netherlands](#netherlands)
    * [New Zealand](#new-zealand)
    * [Nigeria](#nigeria)
    * [Norway](#norway)
    * [Philippines](#philippines)
    * [Sierra Leone](#sierra-leone)
    * [Singapore](#singapore)
    * [South Africa](#south-africa)
    * [South Korea](#south-korea)
    * [Switzerland](#switzerland)
    * [Tanzania](#tanzania)
    * [Ukraine](#ukraine)
    * [United Kingdom](#united-kingdom)
    * [United States - Federal Government](#united-states-federal-government)
    * [United States - State Governments](#united-states-state-governments)
    * [Indigenous and Tribal Governments and Nations](#indigenous-and-tribal-governments-and-nations)
    * [International and Multilateral Frameworks](#international-and-multilateral-frameworks)
    * [European Union Policies and Regulations](#european-union-policies-and-regulations)
      * [Council of Europe](#council-of-europe)
      * [European Commission and Parliament](#european-commission-and-parliament)
      * [European Council](#european-council)
      * [European Data Protection Authorities](#european-data-protection-authorities)
      * [European Union entites (various)](#european-union-entities-various)
    * [OECD](#oecd)
    * [OSCE](#osce)
    * [NATO](#nato)    
    * [United Nations](#united-nations)
  * [Documents in Legal Genres](#documents-in-legal-genres)

* **Education Resources**
  * [Comprehensive Software Examples and Tutorials](#comprehensive-software-examples-and-tutorials)
  * [Free-ish Books](#free-ish-books)
  * [Glossaries and Dictionaries](#glossaries-and-dictionaries)
  * [Open-ish Classes](#open-ish-classes)
  * [Podcasts and Channels](#podcasts-and-channels)

* **AI Incidents, Critiques, and Research Resources**
  * [AI Incident Information Sharing Resources](#ai-incident-information-sharing-resources)
    * [Bibliography of Papers on AI Incidents and Failures](#bibliography-of-papers-on-ai-incidents-and-failures)
  * [AI Law, Policy, and Guidance Trackers](#ai-law-policy-and-guidance-trackers)
  * [Challenges and Competitions](#challenges-and-competitions)
  * [Critiques of AI](#critiques-of-ai)
    * [Environmental Costs of AI](#environmental-costs-of-ai)
    * [Language Diversity and Resource Gaps](#language-diversity-and-resource-gaps)
    * [AI Slop Genre](#ai-slop-genre)
    * [Measurement Critiques](#measurement-critiques)
  * [Groups and Organizations](#groups-and-organizations)
  * [Curated Bibliographies](#curated-bibliographies)
  * [List of Lists](#list-of-lists)
  * [Platforms](#platforms)

* **Technical Resources**
  * [Benchmarks](#benchmarks)
  * [Common or Useful Datasets](#common-or-useful-datasets)
  * [Domain-specific Software](#domain-specific-software)
  * [Machine Learning Environment Management Tools](#machine-learning-environment-management-tools)
  * [Personal Data Protection Tools](#personal-data-protection-tools)
  * [Open Source/Access Responsible AI Software Packages](#open-sourceaccess-responsible-ai-software-packages)
    * [Browser](#browser)
    * [C/C++](#cc)
    * [JavaScript](#javascript)
    * [Python](#python)
    * [R](#r)

* **Archived**
  * [Archived: Official Policy, Frameworks, and Guidance](#archived-official-policy-frameworks-and-guidance)

* **Citing Awesome Machine Learning Interpretability**
  * [Citation](#citing-awesome-machine-learning-interpretability)

## Community and Official Guidance Resources

### Community Frameworks and Guidance

This section is for responsible ML guidance put forward by organizations or individuals, not for official government guidance.

* [2024 State of the AI Regulatory Landscape](https://drive.google.com/file/d/13gyYbBixU75QwFQDTku0AMIovbeTp9_g/view)
* [8 Principles of Responsible ML](https://ethical.institute/principles.html)
* [A Brief Overview of AI Governance for Responsible Machine Learning Systems](https://arxiv.org/pdf/2211.13130.pdf)
* [A checklist for auditing AI systems](https://ictinstitute.nl/a-checklist-for-auditing-ai-systems/) | ICT Institute			
* [A Digital Pandemic: Uncovering the Role of 'Yahoo Boys' in the Surge of Social Media-Enabled Financial Sextortion Targeting Minors](https://networkcontagion.us/wp-content/uploads/Yahoo-Boys_1.2.24.pdf) | Network Contagion Research Institute (NCRI), January 2024			
* [Acceptable Use Policies for Foundation Models](https://github.com/kklyman/aupsforfms) | ![](https://img.shields.io/github/stars/kklyman/aupsforfms?style=social)			
* [Access Now, Regulatory Mapping on Artificial Intelligence in Latin America: Regional AI Public Policy Report](https://www.accessnow.org/wp-content/uploads/2024/07/TRF-LAC-Reporte-Regional-IA-JUN-2024-V3.pdf)			
* [Ada Lovelace Institute, Code and Conduct: How to Create Third-Party Auditing Regimes for AI Systems](https://www.adalovelaceinstitute.org/report/code-conduct-ai/)			
* [Adversarial ML Threat Matrix](https://github.com/mitre/advmlthreatmatrix) | ![](https://img.shields.io/github/stars/mitre/advmlthreatmatrix?style=social)			
* [Ahead of the Curve: Governing AI Agents Under the EU AI Act](https://thefuturesociety.org/wp-content/uploads/2023/04/Report-Ahead-of-the-Curve-Governing-AI-Agents-Under-the-EU-AI-Act-4-June-2025.pdf) | Amin Oueslati and Robin Staes-Polet, The Future Society, June 2025
* [AI Act Governance: Best Practices for Implementing the EU AI Act](https://www.appliedai.de/uploads/files/Best-Practices-for-Implementing-the-EU-AI-Act_2025-07-02-092027_vwvf.pdf) | Initiative for Applied Artificial Intelligence, June 2025
* [AI alignment vs AI ethical treatment: Ten challenges](https://www.globalprioritiesinstitute.org/wp-content/uploads/Bradley-and-Saad-AI-alignment-vs-AI-ethical-treatment_-Ten-challenges.pdf) | Adam Bradley and Bradford Saad, Global Priorities Institute, July 2024
* [AI Assurance: A Repeatable Process for Assuring AI-enabled Systems](https://www.mitre.org/sites/default/files/2024-06/PR-24-1768-AI-Assurance-A-Repeatable-Process-Assuring-AI-Enabled-Systems.pdf) | MITRE, June 2024			
* [AI Canon](https://a16z.com/ai-canon/) | Andreessen Horowitz (a16z)
* [AI Decision-Making and the Courts: A guide for Judges, Tribunal Members, and Court Administrators](https://aija.org.au/wp-content/uploads/2023/12/AIJA_AI-DecisionMakingReport_2023update.pdf) | The Australasian Institute of Judicial Administration Inc., published June 2022 and revised and republished December 2023
* [AI Ethics & Governance 2025: A Framework for Malaysia's Tech Industry](https://www.pikom.org.my/2025/PIKOM_AI_ethic_and_governance_2025.pdf) | PIKOM, May 2025
* [AI Ethics and Governance in Practice](https://www.turing.ac.uk/research/research-projects/ai-ethics-and-governance-practice) | The Alan Turing Institute
* [AI Ethics and Governance in Practice: AI Safety in Practice](https://www.turing.ac.uk/news/publications/ai-ethics-and-governance-practice-ai-safety-practice) | The Alan Turing Institute
  * [AI Safety in Practice](https://www.turing.ac.uk/sites/default/files/2024-06/aieg-ati-6-safetyv1.2.pdf) 
* [AI-Generated Disinformation in Europe and Africa: Use Cases, Solutions and Transnational Learning](https://www.kas.de/documents/285576/0/Study+_+AI-Generated+Disinformation+in+Europe+and+Africa+-+Ebook+%281%29.pdf/a51f9394-e955-21a1-df30-38585122303c?version=1.0&t=1739539822992) | Konrad Adenauer Stiftung, January 31, 2025
* [AI-Generated Algorithmic Virality](https://aiforensics.org/uploads/GenAI%20Report.pdf) | AI Forensics, June 2025
* [AI Governance: A Framework for Responsible and Compliant Artificial Intelligence](https://www.aigl.blog/content/files/2025/09/AI-GOVERNANCE-A-Framework-for-Responsible-and-Compliant-Artificial-Intelligence.pdf) | Sołtysiński Kawecki & Szlęzak, September 2025
* [AI Governance Alliance Briefing Paper Series](https://www3.weforum.org/docs/WEF_AI_Governance_Alliance_Briefing_Paper_Series_2024.pdf) | World Economic Forum, January 2024
* [AI Governance and the EU's Strategic Role in 2025](https://cadmus.eui.eu/server/api/core/bitstreams/cb201cb1-d7e1-40aa-96fb-023c5b22c22f/content) | Florence School of Transnational Governance, Marta Cantero Gamito, August 2025
* [AI Governance InternationaL Evaluation AGILE Index 2025](https://agile-index.ai/AGILE-Index-Report-2025-EN.pdf) | July 2025
* [AI Governance Needs Sociotechnical Expertise: Why the Humanities and Social Sciences Are Critical to Government Efforts](https://datasociety.net/wp-content/uploads/2024/05/DS_AI_Governance_Policy_Brief.pdf)			
* [AI in Africa](https://landscapestudy.tiiny.site/) | Global Center on AI Governance, AI in Africa: A Landscape Study, April 2025
* [AI in the Public Service: From Principles to Practice](https://oxcaigg.oii.ox.ac.uk/wp-content/uploads/sites/11/2021/12/AI-in-the-Public-Service-Final.pdf) | Oxford Commission on AI & Good Governance			
* [AI Inventories: Practical Challenges for Organizational Risk Management](https://tinyurl.com/mrxrdc3y) | Responsible AI Institute and Chevron			
* [AI Liability Along the Value Chain](https://wp.table.media/wp-content/uploads/2025/04/01152117/AI-Liability-Along-the-Value-Chain_Beatriz-Arcila.pdf) | Mozilla, 2025
* [AI Model Registries: A Foundational Tool for AI Governance](https://arxiv.org/pdf/2410.09645) | September 2024
* [AI Model Risk Management Framework](https://cloudsecurityalliance.org/artifacts/ai-model-risk-management-framework) | Cloud Security Alliance and AI Technology and Risk Working Group, July 23, 2024
* [An Overview of Catastrophic AI Risks](https://arxiv.org/pdf/2306.12001) | Dan Hendrycks, Mantas Mazeika, and Thomas Woodside, October 9, 2023
* [AI Policy](https://taylorandfrancis.com/our-policies/ai-policy/) | Taylor & Francis			
* [AI Red-Teaming Is Not a One-Stop Solution to AI Harms: Recommendations for Using Red-Teaming for AI Accountability](https://datasociety.net/wp-content/uploads/2023/10/Recommendations-for-Using-Red-Teaming-for-AI-Accountability-PolicyBrief.pdf) | Data & Society
* [AI-Relevant Regulatory Precedents: A Systematic Search Across All Federal Agencies](https://www.iaps.ai/research/ai-relevant-regulatory-precedent)
* [AI Risk Atlas: Taxonomy and Tooling for Navigating AI Risks and Resources](https://arxiv.org/pdf/2503.05780)
* [AI Safety Governance, the Southeast Asian Way](https://www.brookings.edu/wp-content/uploads/2025/08/GS_08252025_AISA_report.pdf) | Brookings Center for Technology Innovation, AI Safety Asia (AISA), August 2025
* [AI Snake Oil](https://www.aisnakeoil.com/)			
* [AI Standards Hub](https://www.turing.ac.uk/research/research-projects/ai-standards-hub) | The Alan Turing Institute
* [AI Sustainability Outlook: The Challenges, Potential, and Path Forward](https://www.salesforce.com/en-us/wp-content/uploads/sites/4/documents/company/sustainability/salesforce-ai-sustainability-outlook.pdf) | Salesforce
* [AI Verify](https://aiverifyfoundation.sg/what-is-ai-verify/)			
  * [AI Verify Foundation](https://aiverifyfoundation.sg/what-is-ai-verify/)
  * [Cataloguing LLM Evaluations](https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf)
  * [Generative AI: Implications for Trust and Governance](https://aiverifyfoundation.sg/downloads/Discussion_Paper.pdf)
  * [Model Governance Framework for Generative AI](https://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf)
* [AI Won't Replace the General: Algorithms, Decision-making and Battlefield Command](https://www.turing.ac.uk/sites/default/files/2025-09/turing_final_report_ai_wont_replace_the_general_2025.pdf) | The Alan Turing Institute, September 2025
* [Architectural Risk Analysis of Large Language Models](https://berryvilleiml.com/results/BIML-LLM24.pdf) | Berryville Institute of Machine Learning, requires free account
* [Artificial Intelligence Controls Matrix Bundle](https://cloudsecurityalliance.org/artifacts/ai-controls-matrix#)
* [Artificial Intelligence Impact Assessment](https://ecp.nl/wp-content/uploads/2018/11/Artificial-Intelligence-Impact-Assesment.pdf) | ECP Platform voor de InformatieSamenleving, November 2018
* [Artificial Intelligence in Africa: Challenges and Opportunities](https://www.policycenter.ma/sites/default/files/2024-09/PB_23_24%20%28Azeroual%29%20%28EN%29.pdf) | Policy Center for the New South, Fahd Azaroual, May 2024
* [Artificial Intelligence Tools Versus Practice in Conflict Prediction: The Case of Mali](https://hcss.nl/wp-content/uploads/2020/04/Artificial-Intelligence-Tools-Versus-Practice-in-Conflict-Prediction-The-Case-of-Mali.pdf) | The Hague Centre for Strategic Studies, April 29, 2020
* [Artificial Intelligence in the Securities Industry](https://www.finra.org/sites/default/files/2020-06/ai-report-061020.pdf) | Financial Industry Regulatory Authority
* [Assessing the Implementation of Federal AI Leadership and Compliance Mandates](https://hai.stanford.edu/sites/default/files/2025-01/HAI-RegLab-White-Paper-Federal-AI-Leadership-and-Compliance.pdf) | Stanford University Human-Centered Artificial Intelligence (HAI)
* [AuditBoard: 5 AI Auditing Frameworks to Encourage Accountability](https://www.auditboard.com/blog/ai-auditing-frameworks/)
* [Auditing machine learning algorithms: A white paper for public auditors](https://www.auditingalgorithms.net/index.html)
* AWS
  * [Data Privacy FAQ](https://aws.amazon.com/compliance/data-privacy-faq/)
  * [Privacy Notice](https://aws.amazon.com/privacy/)
  * [What is Data Governance?](https://aws.amazon.com/what-is/data-governance/)
* [Best Practice Tools: Examples supporting responsible AI maturity](https://www.gsma.com/solutions-and-impact/connectivity-for-good/external-affairs/wp-content/uploads/2024/09/GSMA-ai4i_Best-Practice-Tools_v7.pdf) | GSMA, September 2024
* [BIML Interactive Machine Learning Risk Framework](https://berryvilleiml.com/interactive/) | Berryville Institute of Machine Learning, requires free account
* [Boston University AI Task Force Report on Generative AI in Education and Research](https://www.bu.edu/hic/files/2024/04/BU-AI-Task-Force-Report.pdf)
* [Brendan Bycroft's LLM Visualization](https://bbycroft.net/llm)
* [Casey Flores, AIGP Study Guide](https://www.linkedin.com/feed/update/urn:li:activity:7201048113090809856?utm_source=share&utm_medium=member_desktop)
* [Cataloguing LLM Evaluations, Draft for Discussion](https://aiverifyfoundation.sg/downloads/Cataloguing_LLM_Evaluations.pdf) | Infocomm Media Development Authority (Singapore) and AI Verify Foundation, October 2023
* [CEN-CENELEC JTC21 AI Standards: Complete Detailed Overview](https://jtc21.eu/wp-content/uploads/2025/06/CEN-CENELEC-JTC21-AI-Standards-Complete-Detailed-Overview.pdf)
* Censius
  * [AI Audit](https://censius.ai/wiki/ai-audit)
  * [An In-Depth Guide To Help You Start Auditing Your AI Models](https://censius.ai/blogs/ai-audit-guide)
* [Center for AI and Digital Policy Reports](https://www.caidp.org/reports/)
* Center for Democracy and Technology (CDT)
  * [AI Policy & Governance](https://cdt.org/area-of-focus/ai-policy-governance/)
  * [Applying Sociotechnical Approaches to AI Governance in Practice](https://cdt.org/insights/applying-sociotechnical-approaches-to-ai-governance-in-practice/)
  * [Assessing AI: Surveying the Spectrum of Approaches to Understanding and Auditing AI Systems](https://cdt.org/wp-content/uploads/2025/01/2025-01-15-CDT-AI-Gov-Lab-Auditing-AI-report.pdf) | January 2025
  * [In Deep Trouble: Surfacing Tech-Powered Sexual Harassment in K-12 Schools](https://cdt.org/insights/report-in-deep-trouble-surfacing-tech-powered-sexual-harassment-in-k-12-schools/)
* Center for Security and Emerging Technology (CSET)
  * [Adding Structure to AI Harm: An Introduction to CSET's AI Harm Framework](https://cset.georgetown.edu/publication/adding-structure-to-ai-harm/)
  * [AI Accidents: An Emerging Threat: What Could Happen and What to Do, CSET Policy Brief, July 2021](https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Accidents-An-Emerging-Threat.pdf)
  * [AI Incident Collection: An Observational Study of the Great AI Experiment](https://cset.georgetown.edu/publication/ai-incident-collection-an-observational-study-of-the-great-ai-experiment/)
  * [Chinese Critiques of Large Language Models: Finding the Path to General Intelligence](https://cset.georgetown.edu/wp-content/uploads/CSET-Chinese-Critiques-of-Large-Language-Models-Finding-the-Path-to-General-Artificial-Intelligence.pdf) | January 2025
  * [CSET's Harm Taxonomy for the AI Incident Database](https://github.com/georgetown-cset/CSET-AIID-harm-taxonomy) | ![](https://img.shields.io/github/stars/georgetown-cset/CSET-AIID-harm-taxonomy?style=social)
  * [CSET Publications](https://cset.georgetown.edu/publications/)
  * [Putting Explainable AI to the Test: A Critical Look at AI Evaluation Approaches](https://cset.georgetown.edu/wp-content/uploads/CSET-Putting-Explainable-AI-to-the-Test.pdf) | February 2025
  * [Repurposing the Wheel: Lessons for AI Standards](https://cset.georgetown.edu/publication/repurposing-the-wheel/)
  * [Translating AI Risk Management Into Practice](https://cset.georgetown.edu/article/translating-ai-risk-management-into-practice/)
  * [Understanding AI Harms: An Overview](https://cset.georgetown.edu/article/understanding-ai-harms-an-overview/)
* [Character Flaws: School Shooters, Anorexia Coaches, and Sexualized Minors: A Look at Harmful Character Chatbots and the Communities That Build Them](https://public-assets.graphika.com/reports/graphika-report-character-flaws.pdf) | Graphika Atlas Report, March 2025
* [Children & AI Design Code: A Protocol for the development and use of AI systems that impact children](https://5rightsfoundation.com/wp-content/uploads/2025/03/5rights_AI_CODE_DIGITAL.pdf)
* [Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing](https://dl.acm.org/doi/abs/10.1145/3351095.3372873)
* [Coalition for Content Provenance and Authenticity](https://c2pa.org/) | (C2PA)
* [Countries With Draft AI Legislation or Frameworks](https://dominiquesheltonleipzig.com/country-legislation-frameworks/) | Dominique Shelton Leipzig
* [Data Provenance Explorer](https://www.dataprovenance.org/)
* [Dealing with Bias and Fairness in AI/ML/Data Science Systems](https://docs.google.com/presentation/d/17o_NzplYua5fcJFuGcy1V1-5GFAHk7oHAF4dN44NkUE)
* [Debugging Machine Learning Models](https://debug-ml-iclr2019.github.io/) | ICLR workshop proceedings
* [Deepfake Pornography Goes to Washington: Measuring the Prevalence of AI-Generated Non-Consensual Intimate Imagery Targeting Congress](https://static1.squarespace.com/static/6612cbdfd9a9ce56ef931004/t/67586997eaec5c6ae3bb5e24/1733847451191/ASP+DFP+Report.pdf) | American Sunlight Project, December 11, 2024
* [Demos, AI – Trustworthy By Design: How to build trust in AI systems, the institutions that create them and the communities that use them](https://demos.co.uk/research/ai-trustworthy-by-design-how-to-build-trust-in-ai-systems-the-institutions-that-create-them-and-the-communities-that-use-them/)
* [Digital Policy Alert, The Anatomy of AI Rules: A systematic comparison of AI rules across the globe](https://digitalpolicyalert.org/ai-rules/the-anatomy-of-AI-rules)
* [Distill](https://distill.pub)
* [Doing AI Differently: Rethinking the foundations of AI via the humanities](https://www.turing.ac.uk/sites/default/files/2025-07/doing_ai_differently_white_paper.pdf) | Alan Turing Institute, July 31, 2025
* [Emotional Manipulation by AI Companions](https://www.hbs.edu/ris/Publication%20Files/26005_951004f6-0b0b-432b-846a-5f95c103d07c.pdf) | Harvard Business School, 2025
* [Estimating the usage and utility of LLMs in the US general public](https://rethinkpriorities.org/wp-content/uploads/2025/07/RP_-Estimating-the-usage-and-utility-of-LLMs-in-the-US-general-public.pdf) | Rethink Priorities, June 2025
* [Ethical and social risks of harm from Language Models](https://www.deepmind.com/publications/ethical-and-social-risks-of-harm-from-language-models)
* [Ethics for people who work in tech](https://ethicsforpeoplewhoworkintech.com/)
* [EU AI Act Handbook](https://www.whitecase.com/sites/default/files/2025-06/wc-eu-ai-act-handbook-finalprint.pdf) | White & Case, June 2025
* [EU AI Act – Provider Only: Certification Scheme v1.5](https://forhumanity.center/site/wp-content/uploads/2025/03/Excerpt-EU-Artificial-Intelligence-Act-Provider-only-v1.5.pdf) | ForHumanity, March 2025
* [Evidence of CCP Censorship, Propaganda in U.S. LLM Responses](https://cdn.prod.website-files.com/67919c3b2972e57c613c2ea2/685b1a27a830fb5b6e7ff511_Sentinel%20Brief%20-%20Evidence%20of%20CCP%20Censorship%20in%20LLM%20Responses.pdf) | Sentinel Brief
* [Explainable AI in Finance: Addressing the Needs of Diverse Stakeholders](https://rpc.cfainstitute.org/sites/default/files/docs/research-reports/wilson_explainableaiinfinance_online.pdf) | Cheryll-Ann Wilson, CFA Institute, Research & Policy Center, August 2025
* [Fairly's Global AI Regulations Map](https://github.com/fairlyAI/global-ai-regulations-map/blob/dev/README.md) | ![](https://img.shields.io/github/stars/fairlyAI/global-ai-regulations-map?style=social)
* [Fairness and Bias in Algorithmic Hiring: A Multidisciplinary Survey](https://dl.acm.org/doi/10.1145/3696457)
* [Fake Friend: How ChatGPT betrays vulnerable teens by encouraging dangerous behavior](https://counterhate.com/wp-content/uploads/2025/08/Fake-Friend_CCDH_FINAL-public.pdf) | Center for Countering Digital Hate, 2025
* [FATML Principles and Best Practices](https://www.fatml.org/resources/principles-and-best-practices)
* [Federation of American Scientists, A NIST Foundation To Support The Agency’s AI Mandate](https://fas.org/publication/nist-foundation/)
* [First of its kind Generative AI Evaluation Sandbox for Trusted AI by AI Verify Foundation and IMDA](https://www.imda.gov.sg/resources/press-releases-factsheets-and-speeches/press-releases/2023/generative-ai-evaluation-sandbox) | Infocomm Media Development Authority (Singapore)
* [Forging Global Cooperation on AI Risks: Cyber Policy as a Governance Blueprint](https://parispeaceforum.org/app/uploads/2025/02/forging-global-cooperation-on-ai-risks-cyber-policy-as-a-governance-blueprint.pdf) | Paris Peace Forum, February 2025
* [ForHumanity Body of Knowledge](https://forhumanity.center/bok/)
* [Framework for Identifying Highly Consequential AI Use Cases](https://www.scsp.ai/wp-content/uploads/2023/11/SCSP_JHU-HCAI-Framework-Nov-6.pdf) | Special Competitive Studies Project and Johns Hopkins University Applied Physics Laboratory
* [From Principles to Practice: An interdisciplinary framework to operationalise AI ethics](https://www.ai-ethics-impact.org/resource/blob/1961130/c6db9894ee73aefa489d6249f5ee2b9f/aieig---report---download-hb-data.pdf)
* [Gage Repeatability and Reproducibility](https://asq.org/quality-resources/gage-repeatability)
* [Gen-AI: Artificial Intelligence and the Future of Work](https://www.imf.org/-/media/Files/Publications/SDN/2024/English/SDNEA2024001.ashx) | International Monetary Fund
* [Generative AI: A New Threat for Online Child Sexual Exploitation and Abuse](https://unicri.org/sites/default/files/2024-09/Generative-AI-New-Threat-Online-Child-Abuse.pdf) | United Nations Interregional Crime and Justice Research Institute (UNICRI)  Centre for AI and Robotics, Bracket Foundation, and Value for Good, September 2024
* [Generative AI Vendor Risk Assessment Guide](https://www.fsisac.com/hubfs/Knowledge/AI/FSISAC_GenerativeAI-VendorEvaluation&QualitativeRiskAssessment.pdf) | Future Society, FS-ISAC, February 2024,
* [Global AI Governance Law and Policy: Canada, EU, Singapore, UK and US](https://iapp.org/media/pdf/resource_center/global_ai_governance_law_policy_series.pdf) | IAPP
* Google
  * [Data governance in the cloud - part 1 - People and processes](https://cloud.google.com/blog/products/data-analytics/data-governance-and-operating-model-for-analytics-pt1)
  * [Data Governance in the Cloud - part 2 - Tools](https://cloud.google.com/blog/products/data-analytics/data-governance-in-the-cloud-part-2-tools)
  * [Evaluating social and ethical risks from generative AI](https://deepmind.google/discover/blog/evaluating-social-and-ethical-risks-from-generative-ai/)
  * [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy)
  * [Perspectives on Issues in AI Governance](https://ai.google/static/documents/perspectives-on-issues-in-ai-governance.pdf)
  * [Principles and best practices for data governance in the cloud](https://services.google.com/fh/files/misc/principles_best_practices_for_data-governance.pdf)
  * [Responsible AI Framework](https://cloud.google.com/responsible-ai)
  * [Responsible AI practices](https://ai.google/responsibility/responsible-ai-practices/)
  * [Testing and Debugging in Machine Learning](https://developers.google.com/machine-learning/testing-debugging)
  * [The Data Cards Playbook](https://sites.research.google/datacardsplaybook/)
* [Governing Artificial Intelligence From Ethical Principles Toward Organizational AI Governance Practices](https://www.utupub.fi/bitstream/handle/10024/179166/Annales%20E%20124%20Birkstedt%20DISS.pdf?sequence=1) | Teemu Birkstedt, University of Turku, 2024
* [A Guide to AI in Schools: Perspectives for the Perplexed](https://tsl.mit.edu/wp-content/uploads/2025/08/GuideToAIInSchools.pdf)
* [Guide for Australian Business: Understanding 42001](https://cdn.prod.website-files.com/6420f704f2602a2ee7f79d26/662aefb77b3077382ff25eef_understanding%2042001%20ai%20management%20system%20standard%20whitepaper.pdf) | Standards Australia and National Artificial Intelligence Centre
* [Guide for Preparing and Responding to Deepfake Events: From the OWASP Top 10 for LLM Applications Team](https://genai.owasp.org/resource/guide-for-preparing-and-responding-to-deepfake-events/) | OWASP, Version 1, September 2024
* [Guidelines for AI in parliaments](https://www.ipu.org/file/20632/download) | Inter-Parliamentary Union, December 2024
* [Guidelines on the Application of the Definition of an AI System in the AI Act: ELI Proposal for a Three-Factor Approach](https://www.europeanlawinstitute.eu/fileadmin/user_upload/p_eli/Publications/ELI_Response_on_the_definition_of_an_AI_System.pdf) | European Law Institute, Response of the ELI to the EU Commission's Consultation, November 1, 2024
* [HackerOne Blog](https://www.hackerone.com/vulnerability-and-security-testing-blog)
* [How Can We Tackle AI-Fueled Misinformation and Disinformation in Public Health?](https://www.bu.edu/ceid/2024/04/25/how-can-we-tackle-ai-fueled-misinformation-and-disinformation-in-public-health/) | Brown University
* [How do I cite generative AI in MLA style?](https://style.mla.org/citing-generative-ai/) | MLA
* [How to Perform an AI Audit for UK Organisations](https://www.haptic-networks.com/cyber-security/how-to-perform-an-ai-audit/) | Haptic Networks
* [Human-Calibrated Automated Testing and Validation of Generative Language Models: An Overview](https://arxiv.org/pdf/2411.16391) | Agus Sudjianto, Aijun Zhang, Srinivas Neppalli, Tarun Joshi, and Michael Malohlava, December 7, 2024
* IBM
  * [AI ethics in action: An enterprise guide to progressing trustworthy AI](https://www.ibm.com/downloads/documents/us-en/10c31775c6d400ed)
  * [Design for AI](https://www.ibm.com/design/ai/fundamentals/)
  * [Principles and Practices for Building More Trustworthy AI, October 6, 2021](https://newsroom.ibm.com/Principles-and-Practices-for-Building-More-Trustworthy-AI)
* [Identifying and Overcoming Common Data Mining Mistakes](https://support.sas.com/resources/papers/proceedings/proceedings/forum2007/073-2007.pdf)
* IEEE
  * [A Flexible Maturity Model for AI Governance Based on the NIST AI Risk Management Framework](https://ieeeusa.org/product/a-flexible-maturity-model-for-ai-governance/)
  * [An Overview of Artificial Intelligence Ethics](https://ieeexplore.ieee.org/document/9844014)
  * [P3119 Standard for the Procurement of Artificial Intelligence and Automated Decision Systems](https://standards.ieee.org/ieee/3119/10729/)  
  * [Std 1012-1998 Standard for Software Verification and Validation](https://people.eecs.ku.edu/~hossein/Teaching/Stds/1012.pdf)
  * [The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, General Principles](https://standards.ieee.org/wp-content/uploads/import/documents/other/ead_general_principles_v2.pdf)
* [Implementing the AI Act in Belgium: Scope of Application and Authorities](https://data-en-maatschappij.ai/uploads/Policy-brief-Implementing-the-AI-act-in-Belgium_2024-12-23-115650_shpg.pdf) | Data & Society Knowledge Centre, December 2024
* [Independent Audit of AI Systems](https://forhumanity.center/independent-audit-of-ai-systems/)
* [Information System Contingency Planning Guidance](https://www.isaca.org/resources/isaca-journal/issues/2021/volume-3/information-system-contingency-planning-guidance) | Larry G. Wlosinski, April 30, 2021
* [Institute for AI Policy and Strategy](https://www.iaps.ai/ourresearch) | (IAPS)
  * [AI Agent Governance: A Field Guide](https://static1.squarespace.com/static/64edf8e7f2b10d716b5ba0e1/t/6801438c58c2692374995db0/1744913293841/Agent+Governance_+A+Field+Guide.pdf) | April 2025
  * [Key questions for the International Network of AI Safety Institutes](https://www.iaps.ai/research/international-network-aisis)
  * [Mapping Technical Safety Research at AI Companies: A literature review and incentives analysis](https://arxiv.org/pdf/2409.07878)
  * [Understanding the First Wave of AI Safety Institutes: Characteristics, Functions, and Challenges](https://www.iaps.ai/research/understanding-aisis)
* [Institute of Internal Auditors](https://www.theiia.org/en/pages/search-results/?keyword=artificial+intelligence)
* [Internal auditor's AI safety checklist](https://www.crowe.com/insights/asset/i/internal-auditors-ai-safety-checklist) | Crowe LLP, Convergence Analysis, May 2024
* [International AI Safety Report: The International Scientific Report on the Safety of Advanced AI](https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf) | AI Action Summit, January 2025
* ISACA
  * [Auditing Artificial Intelligence](https://ec.europa.eu/futurium/en/system/files/ged/auditing-artificial-intelligence.pdf)
  * [Auditing Guidelines for Artificial Intelligence](https://www.isaca.org/resources/news-and-trends/newsletters/atisaca/2020/volume-26/auditing-guidelines-for-artificial-intelligence)
  * [Capability Maturity Model Integration Resources](https://cmmiinstitute.com/)
* [ISO/IEC 42001:2023, Information technology — Artificial intelligence — Management system](https://www.iso.org/standard/81230.html)
* [ITI's AI Security Policy Principles](https://www.itic.org/documents/artificial-intelligence/ITI_AI-Security-Principles_102124_FINAL.pdf) | Information Technology Industry (ITI) Council, October 2024
* [Just Security's Artificial Intelligence Archive](https://www.justsecurity.org/99958/just-securitys-artificial-intelligence-archive/)
* [Key Considerations When Using Artificial Intelligence in the Public Sector](https://www.aaas.org/sites/default/files/2025-01/Key%20Considerations%20AI%20for%20Public%20Sector.pdf) | EPI Center and AAAS, February 2025
* [Know Your Data](https://knowyourdata.withgoogle.com/)
* [Language Model Risk Cards: Starter Set](https://github.com/leondz/lm_risk_cards) | ![](https://img.shields.io/github/stars/leondz/lm_risk_cards?style=social)
* [Large language models explained with a minimum of math and jargon](https://www.understandingai.org/p/large-language-models-ed-with)
* [LC Labs AI Planning Framework](https://github.com/LibraryOfCongress/labs-ai-framework) | ![](https://img.shields.io/github/stars/LibraryOfCongress/labs-ai-framework?style=social), Library of Congress
* [Learning from other domains to advance AI evaluation and testing](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Learning-from-other-Domains-to-Advance-AI-Evaluation-and-Testing_-v3-1.pdf) | Microsoft
* [Llama 2 Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/)
* [LLM Visualization](https://bbycroft.net/llm)
* [Machine Learning Quick Reference: Algorithms](https://support.sas.com/rnd/app/data-mining/enterprise-miner/pdfs/Machine_Learning_Quick_Ref_Algorithms_Mar2017.pdf)
* [Machine Learning Quick Reference: Best Practices](https://support.sas.com/rnd/app/data-mining/enterprise-miner/pdfs/Machine_Learning_Quick_Ref_Best_Practices.pdf)
* [Manifest MLBOM Wiki](https://github.com/manifest-cyber/mlbom)
  * [Towards Traceability in Data Ecosystems using a Bill of Materials Model](https://arxiv.org/pdf/1904.04253.pdf)
* [Mapping AI Risk Mitigations: Evidence Scan and Draft Mitigation Taxonomy](https://cdn.prod.website-files.com/669550d38372f33552d2516e/6887e58496902e3bcad04a5a_Mapping%20AI%20Risk%20Mitigations.pdf) | MIT AI Risk Index, FutureTech, and MIT, July 2025
* Microsoft
  * [2025 Responsible AI Transparency Report: How we build, support our customers, and grow](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/2025-Responsible-AI-Transparency-Report.pdf)
  * [Advancing AI responsibly](https://unlocked.microsoft.com/responsible-ai/)
  * [Azure AI Content Safety](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety)
     * [Harm categories in Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/harm-categories?tabs=warning)
     * [Microsoft Responsible AI Standard, v2](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE5cmFl)
  * [GDPR and Generative AI: A Guide for Public Sector Organizations](https://wwps.microsoft.com/blog/gdpr-genai)
  * [How Microsoft names threat actors](https://learn.microsoft.com/en-us/unified-secops-platform/microsoft-threat-actor-naming)
  * [Taxonomy of Failure Mode in Agentic AI Systems](https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Taxonomy-of-Failure-Mode-in-Agentic-AI-Systems-Whitepaper.pdf)
* [Mitigating the risk of generative AI models creating Child Sexual Abuse Materials: An analysis by child safety nonprofit Thorn](https://partnershiponai.org/wp-content/uploads/2024/11/case-study-thorn.pdf) | Partnership on AI and Thorn
* [Model Transparency Ratings](https://aimodelratings.com/) | Trustible
* [model-cards-and-datasheets](https://github.com/ivylee/model-cards-and-datasheets) | ![](https://img.shields.io/github/stars/ivylee/model-cards-and-datasheets?style=social)
* [Multi-Agent Risks from Advanced AI](https://www.cs.toronto.edu/~nisarg/papers/Multi-Agent-Risks-from-Advanced-AI.pdf) | Cooperative AI Foundation, February 2025
* [Navigating AI Compliance Part 1 Tracing Failure Patterns in History](https://securityandtechnology.org/wp-content/uploads/2024/12/Navigating-AI-Compliance.pdf) | Institute for Security and Technology (IST), December 2024
* [Navigating AI Compliance Part 2 Risk Mitigation Strategies for Safeguarding Against Future Failures](https://securityandtechnology.org/wp-content/uploads/2025/03/Navigating-AI-Compliance-Part-2-Risk-Mitigation-Strategies-for-Safeguarding-Against-Future-Failures.pdf) | Institute for Security and Technology (IST), March 2025
* [Navigating the AI Frontier: A Primer on the Evolution and Impact of AI Agents](https://reports.weforum.org/docs/WEF_Navigating_the_AI_Frontier_2024.pdf) | World Economic Forum and Capgemini, December 2024
* [NewsGuard AI Tracking Center](https://www.newsguardtech.com/special-reports/ai-tracking-center/)
* [On Risk Assessment and Mitigation for Algorithmic Systems](https://drive.google.com/file/d/1ZMt7igUcKUq00yakCnbxBCcaA7vajAix/view) | Integrity Institute Report, February 2024
* [Open Sourcing Highly Capable Foundation Models](https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models)
* OpenAI
  * [Building an early warning system for LLM-aided biological threat creation](https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation)
  * [Disrupting malicious uses of AI: June 2025](https://cdn.openai.com/threat-intelligence-reports/5f73af09-a3a3-4a55-992e-069237681620/disrupting-malicious-uses-of-ai-june-2025.pdf)
  * [How to implement LLM guardrails](https://cookbook.openai.com/examples/how_to_use_guardrails) | OpenAI Cookbook
  * [Evals](https://github.com/openai/evals) | ![](https://img.shields.io/github/stars/openai/evals?style=social)
* [Opportunities to Strengthen U.S. Biosecurity from AI-Enabled Bioterrorism: What Policymakers Should Know](https://csis-website-prod.s3.amazonaws.com/s3fs-public/2025-08/250806_Adamson_AI-Enabled_Bioterrorism.pdf?VersionId=vS8T0bGMLr_lU7RTcXs8im_ndJuGz0cM) | Center for Strategic and International Studies, August 2025
* [Organization and Training of a Cyber Security Team](http://ieeexplore.ieee.org/document/1245662)
* [Our Data Our Selves, Data Use Policy](https://ourdataourselves.tacticaltech.org/data-use-policy/)
* [OWASP AI Testing Guide](https://owasp.org/www-project-ai-testing-guide/)
* [PAIR Explorables: Datasets Have Worldviews](https://pair.withgoogle.com/explorables/dataset-worldviews/)
* Partnership On AI
 * [ABOUT ML Reference Document](https://partnershiponai.org/paper/about-ml-reference-document/)
 * [Guidance for Safe Foundation Model Deployment: A Framework for Collective Action](https://partnershiponai.org/modeldeployment/)
 * [Responsible Practices for Synthetic Media: A Framework for Collective Action](https://syntheticmedia.partnershiponai.org/)
* [PwC's Responsible AI](https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence/what-is-responsible-ai.html)
* [Raising Standards: Data and Artificial Intelligence in Southeast Asia](https://asiasociety.org/sites/default/files/inline-files/ASPI_RaisingStandards_report_fin_web_0.pdf) | Asia Society Policy Institute, Elina Noor and Mark Bryan Manantan, July 2022
* RAND Corporation
  * [A Primer for Developers and Policymakers](https://www.rand.org/pubs/research_reports/RRA3084-1.html)
  * [Analyzing Harms from AI-Generated Images and Safeguarding Online Authenticity](https://www.rand.org/pubs/perspectives/PEA3131-1.html)
  * [Strengthening Emergency Preparedness and Response for AI Loss of Control Incidents](https://www.rand.org/content/dam/rand/pubs/research_reports/RRA3800/RRA3847-1/RAND_RRA3847-1.pdf) | RAND Europe, July 30, 2025 
  * [US Tort Liability for Large-Scale Artificial Intelligence Damages, A Primer for Developers and Policymakers](https://www.rand.org/pubs/research_reports/RRA3084-1.html)
* [Ravit Dotan's Projects](https://www.techbetter.ai/projects-1)
* [Real People in Fake Porn: How a Federal Right of Publicity Could Assist in the Regulation of Deepfake Pornography](https://www.americanbar.org/content/dam/aba/publications/Jurimetrics/spring-2024/real-people-in-fake-porn-how-a-federal-right-of-publicity-could-assist-in-the-regulation-of-deepfake-pornography.pdf)
* [Recommendations for the Independent International Scientific Panel on AI and the Global Dialogue on AI Governance](https://drive.google.com/file/d/17mBzqt7foXThI9xcAP8gsTKan34Zk5Mv/view) |  Simon Institute for Longterm Governance, February 2025
* [Regulating Under Uncertainty: Governance Options for Generative AI](https://fsi9-prod.s3.us-west-1.amazonaws.com/s3fs-public/2024-12/GenAI_Report_REV_Master_%20as%20of%20Dec%2012.pdf) | Stanford Cyber Policy Center, Florence G'Sell, September 2024
* [Responsible Data Stewardship in Practice](https://www.turing.ac.uk/sites/default/files/2024-06/aieg-ati-5-datastewardshipv1.2.pdf) | The Alan Turing Institute
* [Responsible Enterprise AI in the Agentic Era](https://www.infosys.com/iki/documents/responsible-enterprise-ai-agentic-era.pdf) | Infosys
* [Risk Taxonomy and Thresholds for Frontier AI Frameworks](https://www.frontiermodelforum.org/uploads/2025/06/FMF-Technical-Report-on-Frontier-Risk-Taxonomy-and-Thresholds.pdf) | Frontier Model Forum, June 18, 2025
* [Risk Tiers: Towards a Gold Standard for Advanced AI](https://aigi.ox.ac.uk/wp-content/uploads/2025/06/AIGI-gold-standard-risk-tiers-convening.pdf) | AI Governance Initiative, Oxford Martin School, and the University of Oxford, June 2025
* [Safe and Reliable Machine Learning](https://www.dropbox.com/s/sdu26h96bc0f4l7/FAT19-AI-Reliability-Final.pdf?dl=0)
* [Sample AI Incident Response Checklist](https://bnh-ai.github.io/resources/)
* [SHRM Generative Artificial Intelligence AI Chatbot Usage Policy](https://www.shrm.org/resourcesandtools/tools-and-samples/policies/pages/chatgpt-generative-ai-usage.aspx)
* Stanford University
  * [Adverse Event Reporting for AI: Developing the Information Infrastructure Government Needs to Learn and Act](https://hai.stanford.edu/assets/files/hai-reglab-issue-brief-adverse-event-reporting-for-ai.pdf) | July 2025
  * [Open Problems in Technical AI Governance: A repository of open problems in technical AI governance](https://taig.stanford.edu/)
  * [Responsible AI at Stanford: Enabling innovation through AI best practices](https://uit.stanford.edu/security/responsibleai)
* [State of Agentic AI Security and Governance: OWASP Gen AI Security Project Agentic Security Initiative](https://genai.owasp.org/download/50592/?tmstv=1754459367) | Version 1.0, July 2025
* [State of AI Safety in China](https://concordia-ai.com/wp-content/uploads/2025/07/State-of-AI-Safety-in-China-2025.pdf) | Concordia AI, July 2025
* [Summary Report: Workshop on the Geopolitics of Critical Minerals and the AI Supply Chain](https://www.ias.edu/sites/default/files/Critical-Minerals-Workshop_Summary-Report.pdf) | Institute for Advanced Study, August 2025
* [Synthetic Data: The New Data Frontier](https://reports.weforum.org/docs/WEF_Synthetic_Data_2025.pdf) | World Economic Forum, September 2025
* [System cards](https://ai.meta.com/tools/system-cards/) | Meta
* [Taskade: AI Audit PBC Request Checklist Template](https://www.taskade.com/templates/engineering/audit-pbc-request-checklist)
* [Tech Policy Press - Artificial Intelligence](https://www.techpolicy.press/category/artificial-intelligence/)
* [TechTarget: 9 questions to ask when auditing your AI systems](https://www.techrepublic.com/article/9-questions-to-ask-when-auditing-your-ai-systems/)
* [The AI Act between Digital and Sectoral Regulations](https://www.bertelsmann-stiftung.de/fileadmin/files/user_upload/The_AI_Act_between_Digital_and_Sectoral_Regulations__2024_en.pdf) | Bertelsmann Stiftung, December 2024
* [The AI Act is coming: EU reaches political agreement on comprehensive regulation of artificial intelligence](https://www.engage.hoganlovells.com/knowledgeservices/news/the-ai-act-is-coming-eu-reaches-political-agreement-on-comprehensive-regulation-of-artificial-intelligence?nav=FRbANEucS95NMLRN47z%2BeeOgEFCt8EGQ71hKXzqW2Ec%3D&key=BcJlhLtdCv6%2FJTDZxvL23TQa3JHL2AIGr93BnQjo2SkGJpG9xDX7S2thDpAQsCconWHAwe6cJTmX%2FZxLGrXbZz2L%2BEiiz68X&uid=iZAX%2FROFT6Q%3D) | Hogan Lovells
* [The Complete Guide to Crowdsourced Security Testing, Government Edition](https://www.synack.com/wp-content/uploads/2022/09/Crowdsourced-Security-Landscape-Government.pdf) | Synack
* [The Ethics of AI Ethics: An Evaluation of Guidelines](https://link.springer.com/content/pdf/10.1007/s11023-020-09517-8.pdf)
* [The Ethics of Developing, Implementing, and Using Advanced Warehouse Technologies: Top-Down Principles Versus The Guidance Ethics Approach](https://journals.open.tudelft.nl/jhtr/article/view/7098/6136)
* [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/)
* [The Future Is Now: Artificial Intelligence and the Legal Profession](https://www.ibanet.org/document?id=The-future-is%20now-AI-and-the-legal-profession-report) | International Bar Association and the Center for AI and Digital Policy
* [The Implications of Artificial Intelligence in Cybersecurity: Shifting the Offense-Defense Balance](https://securityandtechnology.org/virtual-library/reports/the-implications-of-artificial-intelligence-in-cybersecurity/) | Institute for Security and Technology (IST)
* [The Landscape of ML Documentation Tools](https://huggingface.co/docs/hub/model-card-landscape-analysis) | Hugging Face
* [The Rise of Generative AI and the Coming Era of Social Media Manipulation 3.0: Next-Generation Chinese Astroturfing and Coping with Ubiquitous AI](https://www.rand.org/pubs/perspectives/PEA2679-1.html)
* [Toward an evaluation science for generative AI systems](https://arxiv.org/pdf/2503.05336)
* [Towards Effective Governance of Foundation Models and Generative AI](https://thefuturesociety.org/towards-effective-governance-of-foundation-models-and-generative-ai/) | Future Society
* [Transformed by AI: How Generative Artificial Intelligence Could Affect Work in the UK—And How to Manage It](https://ippr-org.files.svdcdn.com/production/Downloads/Transformed_by_AI_March24_2024-03-27-121003_kxis.pdf) | Institute for Public Policy Research (IPPR)
* [Troubleshooting Deep Neural Networks](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)
* [Trustible, Enhancing the Effectiveness of AI Governance Committees](https://www.trustible.ai/post/enhancing-the-effectiveness-of-ai-governance-committees)
* [Twitter Algorithmic Bias Bounty](https://hackerone.com/twitter-algorithmic-bias?type=team)
* [Understanding data governance in AI: Mapping governance](https://theodi.org/insights/reports/understanding-data-governance-in-ai-mapping-governance/) | Open Data Institute
* [Unite.AI: How to perform an AI Audit in 2023](https://www.unite.ai/how-to-perform-an-ai-audit-in-2023/)
* [University of California, Berkeley, Information Security Office, How to Write an Effective Website Privacy Statement](https://security.berkeley.edu/how-write-effective-website-privacy-statement)
* [University of Washington Tech Policy Lab, Data Statements](https://techpolicylab.uw.edu/data-statements/)
* [US Open-Source AI Governance: Balancing Ideological and Geopolitical Considerations with China Competition](https://cdn.prod.website-files.com/65af2088cac9fb1fb621091f/67aaca031ed677c879434284_Final_US%20Open-Source%20AI%20Governance.pdf) | Center for AI Policy, February 2025
* [Warning Signs: The Future of Privacy and Security in an Age of Machine Learning](https://fpf.org/wp-content/uploads/2019/09/FPF_WarningSigns_Report.pdf)
* [What Are High-Risk AI Systems Within the Meaning of the EU’s AI Act, and What Requirements Apply to Them?](https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20240717-what-are-highrisk-ai-systems-within-the-meaning-of-the-eus-ai-act-and-what-requirements-apply-to-them) | WilmerHale
* [When Not to Trust Your Explanations](https://docs.google.com/presentation/d/10a0PNKwoV3a1XChzvY-T1mWudtzUIZi3sCMzVwGSYfM/edit)
* [Who Should Develop Which AI Evaluations?](https://oms-www.files.svdcdn.com/production/downloads/reports/Who%20should%20develop%20which%20AI%20evaluations.pdf?dm=1737016728)
* [Why We Need to Know More: Exploring the State of AI Incident Documentation Practices](https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700)
* World Economic Forum
  * [AI Governance on the Ground: Canada’s Algorithmic Impact Assessment Process and Algorithm has evolved](https://www.worldprivacyforum.org/2024/08/ai-governance-on-the-ground-series-canada/)
  * [AI Value Alignment: Guiding Artificial Intelligence Towards Shared Human Goals](https://www.weforum.org/publications/ai-value-alignment-guiding-artificial-intelligence-towards-shared-human-goals/)
  * [Responsible AI Playbook for Investors](https://www.weforum.org/publications/responsible-ai-playbook-for-investors/)
  * [Risky Analysis: Assessing and Improving AI Governance Tools](https://www.worldprivacyforum.org/wp-content/uploads/2023/12/WPF_Risky_Analysis_December_2023_fs.pdf)
* [Worldwide AI Ethics: A Review of 200 Guidelines and Recommendations for AI Governance](https://arxiv.org/pdf/2206.11922)
* [You Created A Machine Learning Application Now Make Sure It's Secure](https://www.oreilly.com/ideas/you-created-a-machine-learning-application-now-make-sure-its-secure)
* [YouTube's Anorexia Algorithm: How YouTube Recommends Eating Disorders Videos to Young Girls](https://counterhate.com/wp-content/uploads/2024/12/CCDH.YoutubeED.Nov24.Report_FINAL.pdf) | Center for Countering Digital Hate (CCDH)
* University of California, Berkeley, Center for Long-Term Cybersecurity
  * [AI Risk-Management Standards Profile for General-Purpose AI and Foundation Models](https://cltc.berkeley.edu/wp-content/uploads/2025/01/Berkeley-AI-Risk-Management-Standards-Profile-for-General-Purpose-AI-and-Foundation-Models-v1-1.pdf) | Version 1.1, January 2025
  * [Decision Points in AI Governance: Three Case Studies Explore Efforts to Operationalize AI Principles](https://cltc.berkeley.edu/wp-content/uploads/2020/05/Decision_Points_AI_Governance.pdf)
  * [Intolerable Risk Threshold Recommendations for Artificial Intelligence: Key Principles, Considerations, and Case Studies to Inform Frontier AI Safety Frameworks for Industry and Government](https://cltc.berkeley.edu/wp-content/uploads/2025/02/Intolerable-Risk-Threshold-Recommendations-for-Artificial-Intelligence.pdf) | February 2025
  * [A Taxonomy of Trustworthiness for Artificial Intelligence](https://cltc.berkeley.edu/wp-content/uploads/2023/12/Taxonomy_of_AI_Trustworthiness_tables.pdf) | January 2023  

#### Infographics and Cheat Sheets

* [Foundation Model Development Cheatsheet](https://fmcheatsheet.org/)
* Future of Privacy Forum
  * [EU AI Act: A Comprehensive Implementation & Compliance Timeline](https://fpf.org/resource/eu-ai-act-a-comprehensive-implementation-compliance-timeline/)
  * [The Spectrum of Artificial Intelligence](https://fpf.org/wp-content/uploads/2021/01/FPF_AIEcosystem_illo_03.pdf)
* [Generative AI framework and Generative AI value tree modelling diagram](https://media.licdn.com/dms/image/v2/D4D22AQEKqP2a6_rsCw/feedshare-shrink_1280/B4DZP0cUWFHUAo-/0/1734972885448?e=1738195200&v=beta&t=PMJq6Ti1lisMMkyhnWojcdDt_DAlmYtV6MUQbqWu4hc)
* [Global Index for AI Safety: AGILE Index on Global AI Safety Readiness Feb 2025](https://agile-index.ai/Global-Index-For-AI-Safety-Report-EN.pdf)
* IAPP
  * [EU AI Act Cheat Sheet](https://iapp.org/media/pdf/resource_center/eu_ai_act_cheat_sheet.pdf)
  * [EU AI Act Compliance Matrix](https://iapp.org/resources/article/eu-ai-act-compliance-matrix/)
* [Machine Learning Attack_Cheat_Sheet](https://resources.oreilly.com/examples/0636920415947/-/blob/master/Attack_Cheat_Sheet.png)
* [Navigating the EU AI Act: A Process Map for making AI Systems available](https://www.appliedai-institute.de/assets/files/EU_AI_Act_Compliance_Journey.pdf) | AppliedAI Institute
* [Oliver Patel's Cheat Sheets](https://www.linkedin.com/in/oliver-patel/recent-activity/images/)

#### AI Red-Teaming Resources

##### Papers

* [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/pdf/2411.00640)
* [Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)
* [GenAI Red Teaming Guide: A Practical Approach to Evaluating AI Vulnerabilities](https://genai.owasp.org/download/44859/?tmstv=1737593350) | OWASP Version 1.0, January 23, 2025
* [Identifying and Eliminating CSAM in Generative ML Training Data and Models](https://purl.stanford.edu/kh752sm9123)
* [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419)
* [LLM Agents can Autonomously Exploit One-day Vulnerabilities](https://arxiv.org/abs/2404.08144)
  * [No, LLM Agents can not Autonomously Exploit One-day Vulnerabilities](https://struct.github.io/auto_agents_1_day.html)
* [Red Teaming for GenAI Harms: Revealing the Risks and Rewards for Online Safety](https://www.ofcom.org.uk/siteassets/resources/documents/consultations/discussion-papers/red-teaming/red-teaming-for-gen-ai-harms.pdf?v=370762) | Ofcom, July 23, 2024
* [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/abs/2209.07858)
* [Red Teaming of Advanced Information Assurance Concepts](https://ieeexplore.ieee.org/document/821513)

##### Tools and Guidance

* [@dotey on X/Twitter exploring GPT prompt security and prevention measures](https://x.com/dotey/status/1724623497438155031?s=20)
* [0xeb / GPT-analyst](https://github.com/0xeb/gpt-analyst/) | ![](https://img.shields.io/github/stars/0xeb/gpt-analyst?style=social)
* [0xk1h0 / ChatGPT "DAN" and other "Jailbreaks"](https://github.com/0xk1h0/ChatGPT_DAN) | ![](https://img.shields.io/github/stars/0xk1h0/ChatGPT_DAN?style=social)
* [A Safe Harbor for AI Evaluation and Red Teaming](https://arxiv.org/pdf/2403.04893)
* [ACL 2024 Tutorial: Vulnerabilities of Large Language Models to Adversarial Attacks](https://llm-vulnerability.github.io/)
* [Azure's PyRIT](https://github.com/Azure/PyRIT) | ![](https://img.shields.io/github/stars/Azure/PyRIT?style=social)
* [Berkeley Center for Long-Term Cybersecurity](https://cltc.berkeley.edu/publication/benchmark-early-and-red-team-often-a-framework-for-assessing-and-managing-dual-use-hazards-of-ai-foundation-models/)
* [CDAO frameworks, guidance, and best practices for AI test & evaluation](https://gitlab.jatic.net/home/frameworks)
* [ChatGPT_system_prompt](https://github.com/LouisShark/chatgpt_system_prompt) | ![](https://img.shields.io/github/stars/LouisShark/chatgpt_system_prompt?style=social)
* [coolaj86 / Chat GPT "DAN" and other "Jailbreaks"](https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516) | ![](https://img.shields.io/github/stars/coolaj86?style=social)
* [CSET, What Does AI-Red Teaming Actually Mean?](https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean/)
* [DAIR Prompt Engineering Guide](https://www.promptingguide.ai/)
  * [DAIR Prompt Engineering Guide GitHub](https://github.com/dair-ai/Prompt-Engineering-Guide) | ![](https://img.shields.io/github/stars/dair-ai/Prompt-Engineering-Guide?style=social)
* [Extracting Training Data from ChatGPT](https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html)
* [Frontier Model Forum: What is Red Teaming?](https://www.frontiermodelforum.org/uploads/2023/10/FMF-AI-Red-Teaming.pdf)
* [Generative AI Red Teaming Challenge: Transparency Report 2024](https://drive.google.com/file/d/1JqpbIP6DNomkb32umLoiEPombK2-0Rc-/view)
* [HackerOne, An Emerging Playbook for AI Red Teaming with HackerOne](https://www.hackerone.com/thought-leadership/ai-safety-red-teaming)
* [Humane Intelligence, SeedAI, and DEFCON AI Village, Generative AI Red Teaming Challenge: Transparency Report 2024](https://drive.google.com/file/d/1JqpbIP6DNomkb32umLoiEPombK2-0Rc-/view)
* [In-The-Wild Jailbreak Prompts on LLMs](https://github.com/verazuo/jailbreak_llms) | ![](https://img.shields.io/github/stars/verazuo/jailbreak_llms?style=social)
* [Learn Prompting, Prompt Hacking](https://learnprompting.org/docs/category/-prompt-hacking)
  * [MiesnerJacob / learn-prompting, Prompt Hacking](https://github.com/MiesnerJacob/learn-prompting/blob/main/08.%F0%9F%94%93%20Prompt%20Hacking.ipynb) | ![](https://img.shields.io/github/stars/MiesnerJacob/learn-prompting?style=social)
* [leeky: Leakage/contamination testing for black box language models](https://github.com/mjbommar/leeky) | ![](https://img.shields.io/github/stars/mjbommar/leeky?style=social)
* [LLM Security & Privacy](https://github.com/chawins/llm-sp) | ![](https://img.shields.io/github/stars/chawins/llm-sp?style=social)
* [Membership Inference Attacks and Defenses on Machine Learning Models Literature](https://github.com/HongshengHu/membership-inference-machine-learning-literature) | ![](https://img.shields.io/github/stars/HongshengHu/membership-inference-machine-learning-literature?style=social)
* [Lakera AI's Gandalf](https://gandalf.lakera.ai/)
* [leondz / garak](https://github.com/leondz/garak) | ![](https://img.shields.io/github/stars/leondz/garak?style=social)
* [Microsoft AI Red Team building future of safer AI](https://www.microsoft.com/en-us/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/)
* [OpenAI Red Teaming Network](https://openai.com/blog/red-teaming-network)
* [r/ChatGPTJailbreak](https://www.reddit.com/r/ChatGPTJailbreak/)
  * [developer mode fixed](https://www.reddit.com/r/ChatGPTJailbreak/comments/144905t/developer_mode_fixed/)
* [Y Combinator, ChatGPT Grandma Exploit](https://news.ycombinator.com/item?id=35630801)

#### Generative AI Explainability

* [AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models](http://sameersingh.org/files/papers/allennlp-interpret-demo-emnlp19.pdf)
* Anthropic
  * [Chain-of-thought Faithfulness](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-cot)
  * [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
  * [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
  * [Tracing the thoughts of a large language model](https://www.anthropic.com/research/tracing-thoughts-language-model)
* [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
* [Backpack Language Models](https://arxiv.org/pdf/2305.16765)
* Jay Alammar
  * [Finding the Words to Say: Hidden State Visualizations for Language Models](https://jalammar.github.io/hidden-states/)
  * [Interfaces for Explaining Transformer Language Models](https://jalammar.github.io/explaining-transformers/)
* [Neuronpedia](https://www.neuronpedia.org/)
* [Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph](https://openreview.net/forum?id=dWYRjT501w)[![Static Badge](https://img.shields.io/badge/pyPI-latent--explorer-red)](https://github.com/Ipazia-AI/latent-explorer)

#### University Policies and Guidance

* [Columbia Business School, Generative AI Policy](https://students.business.columbia.edu/office-of-student-affairs/academic-advising-and-student-success/academic-integrity/generative-ai-policy)
* [Columbia University, Considerations for AI Tools in the Classroom](https://ctl.columbia.edu/resources-and-technology/resources/ai-tools/)
* [Columbia University, Generative AI Policy](https://provost.columbia.edu/content/office-senior-vice-provost/ai-policy)
* [Georgetown University, Artificial Intelligence and Homework Support Policies](https://cndls.georgetown.edu/resources/syllabus-policies/ai-and-homework-support/)
* [Georgetown University, Artificial Intelligence Generative Resources](https://guides.library.georgetown.edu/ai)
* [Georgetown University, Teaching with AI](https://cndls.georgetown.edu/resources/ai/)
* [George Washington University, Faculty Resources: Generative AI](https://guides.himmelfarb.gwu.edu/faculty/generative-AI)
* [George Washington University, Guidelines for Using Generative Artificial Intelligence at the George Washington University April 2023](https://provost.gwu.edu/sites/g/files/zaxdzs5926/files/2023-04/generative-artificial-intelligence-guidelines-april-2023.pdf)
* [George Washington University, Guidelines for Using Generative Artificial Intelligence in Connection with Academic Work](https://provost.gwu.edu/guidelines-using-generative-artificial-intelligence-connection-academic-work-0)
* [Harvard Business School, 2.1.2 Using ChatGPT & Artificial Intelligence Tools](https://www.hbs.edu/mba/handbook/standards-of-conduct/academic/Pages/chatgpt-and-ai.aspx)
* [Harvard Graduate School of Education, HGSE AI Policy](https://registrar.gse.harvard.edu/AI-policy)
* [Harvard University, AI Guidance & FAQs](https://oue.fas.harvard.edu/ai-guidance)
* [Harvard University, Guidelines for Using ChatGPT and other Generative AI tools at Harvard](https://provost.harvard.edu/guidelines-using-chatgpt-and-other-generative-ai-tools-harvard)
* [Massachusetts Institute of Technology, Guidance for use of Generative AI tools](https://ist.mit.edu/ai-guidance)
* [Massachusetts Institute of Technology, Generative AI & Your Course](https://tll.mit.edu/teaching-resources/course-design/gen-ai-your-course/)
* [Stanford Graduate School of Business, Course Policies on Generative AI Use](https://tlhub.stanford.edu/docs/course-policies-on-generative-ai-use/)
* [Stanford University, Artificial Intelligence Teaching Guide](https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide)
* [Stanford University, Creating your course policy on AI](https://teachingcommons.stanford.edu/teaching-guides/artificial-intelligence-teaching-guide/creating-your-course-policy-ai)
* [Stanford University, Generative AI Policy Guidance](https://communitystandards.stanford.edu/generative-ai-policy-guidance)
* [Stanford University, Responsible AI at Stanford](https://uit.stanford.edu/security/responsibleai)
* [University of California, AI Governance and Transparency](https://ai.universityofcalifornia.edu/governance-transparency/)
* [University of California, Applicable Law and UC Policy](https://ai.universityofcalifornia.edu/governance-transparency/applicable-law-and-policy.html)
* [University of California, Legal Alert: Artificial Intelligence Tools](https://www.ucop.edu/ethics-compliance-audit-services/_files/compliance/ai/ai-alert.pdf)
* [University of California, Berkeley, AI at UC Berkeley](https://technology.berkeley.edu/AI)
* [University of California, Berkeley, Appropriate Use of Generative AI Tools](https://ethics.berkeley.edu/privacy/appropriate-use-generative-ai-tools)
* [University of California, Irvine, Generative AI for Teaching and Learning](https://dtei.uci.edu/generative-ai/)
* [University of California, Irvine, Statement on Generative AI Detection](https://aisc.uci.edu/resources/Statement%20on%20Turnitin%20AI%20detection.pdf)
* [University of California, Los Angeles, Artificial Intelligence Tools and Academic Use](https://guides.library.ucla.edu/c.php?g=1308287&p=9702196)
* [University of California, Los Angeles, ChatGPT and AI Resources](https://online.ucla.edu/chatgpt-and-ai-resources/)
* [University of California, Los Angeles, Generative AI](https://genai.ucla.edu/)
* [University of California, Los Angeles, Guiding Principles for Responsible Use](https://genai.ucla.edu/guiding-principles-responsible-use)
* [University of California, Los Angeles, Teaching Guidance for ChatGPT and Related AI Developments](https://senate.ucla.edu/news/teaching-guidance-chatgpt-and-related-ai-developments)
* [University of Notre Dame, AI Recommendations for Instructors](https://honorcode.nd.edu/ai-recommendations-for-instructors/)
* [University of Notre Dame, AI@ND Policies and Guidelines](https://ai.nd.edu/policies-and-guidelines/)
* [University of Notre Dame, Generative AI Policy for Students](https://honorcode.nd.edu/generative-ai-policy-for-students-august-2023/)
* [University of Southern California, Using Generative AI in Research](https://libguides.usc.edu/generative-AI/home)
* [University of Washington, AI+Teaching](https://teaching.washington.edu/course-design/ai/)
* [University of Washington, AI+Teaching, Sample syllabus statements regarding student use of artificial intelligence](https://teaching.washington.edu/course-design/ai/sample-ai-syllabus-statements/)
* [Yale University, AI at Yale](https://ai.yale.edu/)
* [Yale University, AI Guidance for Teachers](https://poorvucenter.yale.edu/AIguidance)
* [Yale University, Yale University AI guidelines for staff](https://yaledata.yale.edu/yale-university-ai-guidelines-staff)
* [Yale University, Guidelines for the Use of Generative AI Tools](https://provost.yale.edu/news/guidelines-use-generative-ai-tools)

### Official Policy, Frameworks, and Guidance

This section serves as a repository for policy documents, regulations, guidelines, and recommendations that govern the ethical and responsible use of artificial intelligence and machine learning technologies. From international legal frameworks to specific national laws, the resources cover a broad spectrum of topics such as fairness, privacy, ethics, and governance.

#### Australia

* Department of Industry, Science and Resources
  * [AI Governance: Leadership insights and the Voluntary AI Safety Standard in practice](https://www.industry.gov.au/news/ai-governance-leadership-insights-and-voluntary-ai-safety-standard-practice) | Department of Industry, Science and Resources
  * [Artificial Intelligence Model Clauses](https://www.buyict.gov.au/sys_attachment.do?sys_id=e535e2ca935caa10438b39cdfaba103d) | Digital Transformation Agency, Version 2.0, March 2025
  * [Australia’s AI Ethics Principles](https://www.industry.gov.au/publications/australias-artificial-intelligence-ethics-framework/australias-ai-ethics-principles)
  * [Guidance for AI Adoption](https://www.industry.gov.au/publications/guidance-for-ai-adoption) | National Artificial Intelligence Centre, October 21, 2025
    * [Guidance for AI Adoption: Foundations v1.0](https://www.industry.gov.au/sites/default/files/2025-10/guidance-for-ai-adoption-foundations.pdf) | National Artificial Intelligence Centre, October 2025
    * [Guidance for AI Adoption: Implementation practices v1.0](https://www.industry.gov.au/sites/default/files/2025-10/guidance-for-ai-adoption-implementation-practices.pdf) | National Artificial Intelligence Centre, October 2025 
  * [Introducing mandatory guardrails for AI in high-risk settings: proposals paper](https://consult.industry.gov.au/ai-mandatory-guardrails)
  * [The AI Impact Navigator](https://www.industry.gov.au/publications/ai-impact-navigator) | October 21, 2024
  * [Voluntary AI Safety Standard](https://www.industry.gov.au/sites/default/files/2024-09/voluntary-ai-safety-standard.pdf) |  August 2024
* Digital Transformation Agency
  * [Evaluation of the whole-of-government trial of Microsoft 365 Copilot: Summary of evaluation findings](https://www.digital.gov.au/sites/default/files/documents/2024-10/Copilot%20Microsoft%20365%20summary%20of%20evaluation%20findings.pdf) | October 23, 2024
  * [Policy for the responsible use of AI in government](https://www.digital.gov.au/sites/default/files/documents/2024-08/Policy%20for%20the%20responsible%20use%20of%20AI%20in%20government%20v1.1.pdf) | September 2024, Version 1.1
* Office of the Australian Information Commissioner
  * [Guidance on privacy and developing and training generative AI models](https://www.oaic.gov.au/privacy/privacy-guidance-for-organisations-and-government-agencies/guidance-on-privacy-and-developing-and-training-generative-ai-models)
  * [Guidance on privacy and the use of commercially available AI products](https://www.oaic.gov.au/privacy/privacy-guidance-for-organisations-and-government-agencies/guidance-on-privacy-and-the-use-of-commercially-available-ai-products)
* [National framework for the assurance of artificial intelligence in government](https://www.finance.gov.au/sites/default/files/2024-06/National-framework-for-the-assurance-of-AI-in-government.pdf)
* [Technical standard for government’s use of artificial intelligence](https://www.digital.gov.au/policy/ai/AI-technical-standard) | Digital Transformation Agency (DTA)
* [Testing the Reliability, Validity, and Equity of Terrorism Risk Assessment Instruments](https://www.homeaffairs.gov.au/foi/files/2023/fa-230400097-document-released-part-1.PDF)
* [Understanding Responsibilities in AI Practices](https://www.digital.nsw.gov.au/policy/artificial-intelligence/nsw-artificial-intelligence-assessment-framework/responsibilities) | NSW Digital Strategy

#### Brazil

* [Autoridade Nacional de Proteção de Dados, Technology Radar – short version in English, no. 1: Generative Artificial Intelligence](https://www.gov.br/anpd/pt-br/documentos-e-publicacoes/documentos-de-publicacoes/radar-tecnologico-inteligencia-artificial-generativa-versao-em-lingua-inglesa.pdf) | (ANPD, Brazilian Data Protection Authority)

#### Canada

* [A Regulatory Framework for AI: Recommendations for PIPEDA Reform](https://www.priv.gc.ca/en/about-the-opc/what-we-do/consultations/completed-consultations/consultation-ai/reg-fw_202011/)
* [An Act to enact the Consumer Privacy Protection Act, the Personal Information and Data Protection Tribunal Act and the Artificial Intelligence and Data Act and to make consequential and related amendments to other Acts](https://www.parl.ca/legisinfo/en/bill/44-1/c-27)
* [AI in Canada](https://oecd.ai/en/dashboards/countries/Canada) | OECD AI policies in Canada
* [Algorithmic Impact Assessment tool](https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai/algorithmic-impact-assessment.html)
* [Artificial Intelligence and Data Act](https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act)
* [The Artificial Intelligence and Data Act Companion document](https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act-aida-companion-document) | (AIDA)
* [Directive on Automated Decision Making](https://www.tbs-sct.gc.ca/pol/doc-eng.aspx?id=32592) | (Canada)
* [E-23 – Model Risk Management](https://www.osfi-bsif.gc.ca/en/guidance/guidance-library/draft-guideline-e-23-model-risk-management) | (Draft Guideline)
* [Guideline E-23 – Model Risk Management 2027](https://www.osfi-bsif.gc.ca/en/guidance/guidance-library/guideline-e-23-model-risk-management-2027) | Office of the Superintendent of Financial Institutions
* [Responsible use of artificial intelligence in government](https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai.html) | Government of Canada
* [Transparency for machine learning-enabled medical devices: Guiding principles](https://www.canada.ca/en/health-canada/services/drugs-health-products/medical-devices/transparency-machine-learning-guiding-principles.html) | Health Canada

#### China

* [人工智能全球治理行动计划](https://www.gov.cn/yaowen/liebiao/202507/content_7033929.htm) | Action Plan on Global Governance of Artificial Intelligence, July 26, 2025

#### Colombia

* [Presidency of the Republic of Colombia, Marco Ético para la Inteligencia Artificial en Colombia](https://minciencias.gov.co/sites/default/files/marco-etico-ia-colombia-2021.pdf) | Ethical Framework for Artificial Intelligence in Colombia, November 2021

#### Costa Rica

* [Ministerio de Ciencia, Innovación, Tecnología y Telecomunicaciones](https://cambioclimatico.go.cr/wp-content/uploads/2023/06/Plan-Nacional-Ciencia-Tecnologia-Innovacion-2022-2027.pdf) | MICITT, Plan Nacional de Ciencia, Tecnología e Innovación 2022–2027

#### Denmark

* [National Strategy for Artificial Intelligence](https://en.digst.dk/media/lz0fxbt4/305755_gb_version_final-a.pdf) | Ministry of Finance and Ministry of Industry, Business and Financial Affairs, March 2019

#### Denmark
* Ministry of Finance and Ministry of Industry, Business and Financial Affairs, March 2019 | [National Strategy for Artificial Intelligence](https://en.digst.dk/media/lz0fxbt4/305755_gb_version_final-a.pdf)

#### Finland

* [Finland's Age of Artificial Intelligence: Turning Finland into a leading country in the application of artificial intelligence. Objective and recommendations for measures](https://julkaisut.valtioneuvosto.fi/bitstream/handle/10024/160391/TEMrap_47_2017_verkkojulkaisu.pdf) | Ministry of Economic Affairs and Employment

#### France
* [Challenges and opportunities of artificial intelligence in the fight against information manipulation](https://www.sgdsn.gouv.fr/files/files/Publications/20250207_NP_SGDSN_VIGINUM_Rapport%20menace%20informationnelle%20IA_EN_0.pdf) | VIGINUM, February 7, 2025
* [Gouvernance des algorithmes d’intelligence artificielle dans le secteur financier](https://acpr.banque-france.fr/sites/default/files/medias/documents/20200612_gouvernance_evaluation_ia.pdf) | (France)

#### Germany

* Bundesamt für Sicherheit in der Informationstechnik
  * [Generative AI Models - Opportunities and Risks for Industry and Authorities](https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/KI/Generative_AI_Models.html)
  * [German-French recommendations for the use of AI programming assistants](https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/KI/ANSSI_BSI_AI_Coding_Assistants.html)
* [Germany AI Strategy Report](https://ai-watch.ec.europa.eu/countries/germany/germany-ai-strategy-report_en)
* [OECD-Bericht zu Künstlicher Intelligenz in Deutschland](https://www.ki-strategie-deutschland.de/files/downloads/OECD-Bericht_K%C3%BCnstlicher_Intelligenz_in_Deutschland.pdf)
* [Recommendations of the Data Ethics Commission for the Federal Government's Strategy on Artificial Intelligence,](https://www.bmi.bund.de/SharedDocs/downloads/EN/themen/it-digital-policy/recommendations-data-ethics-commission.pdf?__blob=publicationFile&v=3) |  Daten Ethik Kommission, October 9, 2018

#### Hong Kong

* [Artificial Intelligence: Model Personal Data Protection Framework](https://www.pcpd.org.hk/english/resources_centre/publications/files/ai_protection_framework.pdf) | Office of the Privacy Commissioner for Personal Data, June 2024

#### Iceland

* Ministry of Higher Education, Industry, and Innovation
  * [Aðgerðaáætlun um gervigreind 2024-2026](https://samradapi.island.is/api/Documents/1d4c7cba-fd9c-ef11-9bc7-005056bcce7e) | Action Plan for Artificial Intelligence 2024-2026 | November 2024
  * [Efnahagsleg tækifæri gervigreindar á Íslandi](https://samradapi.island.is/api/Documents/1e4c7cba-fd9c-ef11-9bc7-005056bcce7e) | Economic Opportunities of Artificial Intelligence in Iceland, Statistics Iceland

#### India

* [AI Governance Framework for India 2025-26](https://www.aigl.blog/content/files/2025/09/AI-Governance-Framework-for-India.pdf) | National Cyber and AI Center
* [Stakeholders consultation on "Draft Standard for the Schema and Taxonomy of an AI Incident Database in Telecommunications and Critical Digital Infrastructure"](https://www.tec.gov.in/pdf/consultations/TEC_57090.pdf) | Telecommunications Engineering Centre, May 27, 2025

#### Ireland

* [AI - Here for Good: A National Artificial Intelligence Strategy for Ireland](https://enterprise.gov.ie/en/publications/publication-files/national-ai-strategy.pdf)
* [AI Standards & Assurance Roadmap: Action under 'AI - Here for Good,' the National Artificial Intelligence Strategy for Ireland](https://www.nsai.ie/images/uploads/general/NSAI_AI_report_digital_links.pdf) | National Standards Authority of Ireland, Top Team on Standards in AI, July 2023
* [Artificial Intelligence: Friend or Foe? Summary and Public Policy Considerations](https://www.gov.ie/pdf/?file=https://assets.gov.ie/295620/f11c6c66-4012-49fa-bb7f-8f14130f6fa5.pdf) | Department of Finance and Department of Enterprise, Trade and Employment, June 2024
* [Interim Guidelines for Use of AI in the Public Service](https://assets.gov.ie/280459/73ce75af-0015-46af-a9f6-b54f0a3c4fd0.pdf) | Department of Public Expenditure, NDP Delivery and Reform, February 2024
* [Ireland's National AI Strategy: AI - Here for Good](https://enterprise.gov.ie/en/publications/publication-files/national-ai-strategy-refresh-2024.pdf) | Refresh 2024

#### Jamaica

* [National Artificial Intelligence Policy Recommendations](https://opm.gov.jm/wp-content/uploads/2025/02/National-Artificial-Intelligence-Task-Force-Policy-Recommendations-Final-1.pdf) | National Artificial Intelligence Task Force, September 2024

#### Japan

* Japan AI Safety Institute
  * [Guide to Evaluation Perspectives on AI Safety](https://aisi.go.jp/assets/pdf/ai_safety_eval_v1.01_en.pdf) | (Version 1.01), September 25, 2024
  * [Guide to Red Teaming Methodology on AI Safety](https://aisi.go.jp/assets/pdf/ai_safety_RT_v1.00_en.pdf) | (Version 1.00), September 25, 2024
 
#### Kenya

* [Diplomat's Playbook on Artificial Intelligence—Shaping a Safe, Secure, Inclusive, and Trustworthy AI Future: Kenya's Strategic Leadership in AI Global Diplomacy](https://mfa.go.ke/sites/default/files/2025-01/DIPLOMATS%20AI%20PLAYBOOK%20FINAL.pdf) | Ministry of Foreign and Diaspora Affairs, State Department for Foreign Affairs, and the Office of the Special Envoy on Technology
* [Kenya Artificial Intelligence Strategy 2025-2030](https://ict.go.ke/sites/default/files/2025-03/Kenya%20AI%20Strategy%202025%20-%202030.pdf) | March 2025

#### Malaysia

* [The National Guidelines on AI Governance & Ethics](https://mastic.mosti.gov.my/publication/the-national-guidelines-on-ai-governance-ethics/)

#### Mexico

* [Recomendaciones para el Tratamiento de Datos Personales Derivado del Uso de la Inteligencia Artificial](https://home.inai.org.mx/wp-content/documentos/DocumentosSectorPublico/RecomendacionesPDP-IA.pdf) | Instituto Nacional de Transparencia, Acceso a la Información y Protección de Datos Personales (INAI), June 2024

#### Moldova

* Ministry of Economic Development and Digitalization
  * [Cartea Albă cu Privire la Inteligența Artificială și Guvernanța Datelor](https://drive.google.com/file/d/1MDEGQ3snOiYXeM5G1YZfV8yH6ZFWxVTJ/view) | 2024
  * [White Book on Artificial Intelligence and Data Governance](https://drive.google.com/file/d/1d2VmubZJjwVjzxUT4gjJE7DXTinzdyfO/view?usp=sharing) | 2024

#### Netherlands

* [AI Act Guide](https://www.government.nl/binaries/government/documenten/publications/2025/09/04/ai-act-guide/ai-act-guide.pdf) | Ministry of Economic Affairs
* [AI Impact Assessment: The tool for a responsible AI project](https://www.government.nl/binaries/government/documenten/publications/2023/03/02/ai-impact-assessment/2024-IWM-AI-Impact-assessment-2.0-EN.pdf) | Ministry of Infrastructure and Water Management
* Autoriteit Persoonsgegevens
  * [Call for input on prohibition on AI systems for emotion recognition in the areas of workplace or education institutions](https://www.autoriteitpersoonsgegevens.nl/en/documents/call-for-input-on-prohibition-on-ai-systems-for-emotion-recognition-in-the-areas-of-workplace-or-education-institutions) | October 31, 2024
  * [AScraping bijna altijd illegal](https://www.autoriteitpersoonsgegevens.nl/actueel/ap-scraping-bijna-altijd-illegaal) | Dutch Data Protection Authority, "Scraping is always illegal"
* [General principles for the use of Artificial Intelligence in the financial sector](https://www.dnb.nl/media/jkbip2jc/general-principles-for-the-use-of-artificial-intelligence-in-the-financial-sector.pdf)

#### New Zealand

* [Accredited Employer Work Visa: Use of Adept for Automated Processing of Migrant Gateway](https://www.mbie.govt.nz/dmsdocument/28176-accredited-employer-work-visa-use-of-adept-for-automated-processing-of-migrant-gateway) | Ministry of Business, Innovation & Employment, June 28, 2022
* [Advanced AI evaluations at AISI: May update](https://www.aisi.gov.uk/work/advanced-ai-evaluations-may-update) | AI Safety Institute (AISI)
* [Algorithm Assessment Report](https://www.data.govt.nz/assets/Uploads/Algorithm-Assessment-Report-Oct-2018.pdf) | Internal Affairs and Stats NZ, October 2018
* [Algorithm Charter for Aotearoa New Zealand](https://data.govt.nz/assets/data-ethics/algorithm/Algorithm-Charter-2020_Final-English-1.pdf)
* [Algorithm impact assessment user guide: Algorithm Charter for Aotearoa New Zealand](https://www.data.govt.nz/assets/data-ethics/algorithm/AIA-user-guide.pdf) | December 2023
* [Artificial intelligence frameworks and regulation: An intelligence perspective](https://igis.govt.nz/assets/Uploads/FINAL-Part-1_-Global-AI-frameworks-and-regulation.pdf) | Inspector-General of Intelligence and Security, August 2024
* [Automated decision-making in MSD: Proposed legislative and policy framework](https://www.msd.govt.nz/documents/about-msd-and-our-work/publications-resources/official-information-responses/2022/july/07072022-requesting-the-document-automated-decision-making-in-msd-proposed-legislative-and-policy-framework-memo-.pdf) | Ministry of Social Development, May 5, 2021
* [Automated Decision Making Standard](https://www.msd.govt.nz/documents/about-msd-and-our-work/publications-resources/official-information-responses/2022/july/07072022-requesting-the-document-automated-decision-making-in-msd-proposed-legislative-and-policy-framework-document-.pdf) | March 1, 2023
* [Callaghan Innovation, EU AI Fact Sheet 4, High-risk AI systems](https://www.callaghaninnovation.govt.nz/assets/documents/Resource-EU-AI-Act-Support/EU-AI-Policy-Fact-Sheet-4-High-Risk-AI-Systems.pdf)
* [Discussion Paper: International Data Ethics Frameworks](https://data.govt.nz/assets/Uploads/Discussion-paper-International-data-ethics-frameworks-March-2020.pdf) | Prepared on behalf of the Government Chief Data Steward for the Data Ethics Advisory Group (DEAG)
* [Government Use of Artificial Intelligence in New Zealand: Final Report on Phase 1 of the New Zealand Law Foundation's Artificial Intelligence and Law in New Zealand Project](https://www.data.govt.nz/assets/data-ethics/algorithm/NZLF-report.pdf) | 2019
* [Initial advice on Generative Artificial Intelligence in the public service](https://www.digital.govt.nz/assets/Standards-guidance/Technology-and-architecture/Generative-AI/Joint-System-Leads-tactical-guidance-on-public-service-use-of-GenAI-September-2023.pdf) | July 2023
* [New Zealand Income Insurance: service model and automated decision making](https://www.mbie.govt.nz/dmsdocument/26492-nzii-service-model-and-adm-pdf) | Ministry of Business, Innovation & Employment
* [New Zealand's Strategy for Artificial Intelligence: Investing with confidence](https://www.mbie.govt.nz/assets/new-zealands-strategy-for-artificial-intelligence.pdf) | Ministry of Business, Innovation & Employment, Accelerating Private Sector AI Adoption and Innovation, July 2025
* [Public Scrutiny of Automated Decisions: Early Lessons and Emerging Methods](https://www.data.govt.nz/assets/Uploads/Public-Scrutiny-of-Automated-Decisions.pdf) | An Upturn and Omidyar Network Report

#### Nigeria

* [NAIS National Artificial Intelligence Strategy](https://ncair.nitda.gov.ng/wp-content/uploads/2024/08/National-AI-Strategy_01082024-copy.pdf) | National Center for Artificial Intelligence & Robotics (NCAIR) and the National Information Technology Development Agency (NITDA), August 2024

#### Norway

* [Artificial Intelligence and Democratic Elections — International Experiences and National Recommendations](https://www.regjeringen.no/contentassets/23f8fd1726634f589724d96b49fe994c/en_rapport-ekspertgruppe-ki-og-valg.pdf) | Ministry of Local Government and Regional Development, Expert Group on Artificial Intelligence and Elections, February 2025
* [National Strategy for Artificial Intelligence](https://www.regjeringen.no/contentassets/1febbbb2c4fd4b7d92c67ddd353b6ae8/en-gb/pdfs/ki-strategi_en.pdf) | Ministry of Local Government and Modernisation

#### Philippines

#### Sierra Leone

* [Artificial Intelligence and Automation: Preserving Human Agency in a World of Automation](https://statehouse.gov.sl/wp-content/uploads/2025/01/H.E.KEYNOTE-ACE-EDUCATION-WEEK-2025-JAN-2025-3.pdf) | Keynote Statement on the Annual Celebration of Education Week (ACE Week) 2025 by His Excellency, Dr. Julius Maada Bio, President of the Republic of Sierra Leone at Makeni, Sierra Leone, Friday, 24th January 2025

#### Singapore

* [Artificial Intelligence Model Risk Management: Observations from a Thematic Review](https://www.mas.gov.sg/-/media/mas-media-library/publications/monographs-or-information-paper/imd/2024/information-paper-on-ai-risk-management-final.pdf) | Monetary Authority of Singapore, Information Paper, December 2024
* Personal Data Protection Commission (PDPC)
  * [Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.pdf)
  * [Compendium of Use Cases: Practical Illustrations of the Model AI Governance Framework](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgaigovusecases.pdf)
  * [Model Artificial Intelligence Governance Framework](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf) |  (Second Edition)
  * [Privacy Enhancing Technology: Proposed Guide on Synthetic Data Generation](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/other-guides/proposed-guide-on-synthetic-data-generation.pdf)
* [The Singapore Consensus on Global AI Safety Research Priorities](https://aisafetypriorities.org/)

#### South Africa

* [Computer Applications Technology: Learner Guidelines for Practical Assessment Tasks, Grade 12, 2025](https://www.education.gov.za/Portals/0/CD/2025%20PATs/Computer%20Applications%20Technology%20PAT%20GR%2012%202025%20Learner%20Guidelines%20Eng.pdf?ver=2025-02-11-163720-737) | Department of Basic Education
* [South Africa's Artificial Intelligence Planning: Adoption of AI by Government](https://www.dcdt.gov.za/images/phocadownload/AI_Government_Summit/National_AI_Government_Summit_Discussion_Document.pdf) | Department of Communications & Digital Technologies and the Artificial Intelligence Institute of South Africa, October 2023

#### South Korea

* [AI Safety Institute of Korea](https://www.aisi.re.kr/eng)
* [Basic Act on the Promotion of Artificial Intelligence Development and Establishment of a Trust Framework](https://likms.assembly.go.kr/bill/billDetail.do?billId=PRC_R2V4H1W1T2K5M1O6E4Q9T0V7Q9S0U0) | National Assembly, 인공지능 발전과 신뢰 기반 조성 등에 관한 기본법안, (대안,Alternative Draft)
* [인공지능 발전과 신뢰 기반 조성 등에 관한 기본법](https://www.law.go.kr/%25EB%25B2%2595%25EB%25A0%25B9/%25EC%259D%25B8%25EA%25B3%25B5%25EC%25A7%2580%25EB%258A%25A5%2520%25EB%25B0%259C%25EC%25A0%2584%25EA%25B3%25BC%2520%25EC%258B%25A0%25EB%25A2%25B0%2520%25EA%25B8%25B0%25EB%25B0%2598%2520%25EC%25A1%25B0%25EC%2584%25B1%2520%25EB%2593%25B1%25EC%2597%2590%2520%25EA%25B4%2580%25ED%2595%259C%2520%25EA%25B8%25B0%25EB%25B3%25B8%25EB%25B2%2595/%2820676,20250121%29) | Ministry of Science and ICT, Framework Act on the Development of Artificial Intelligence and Creation of a Trust Foundation, January 21, 2025
* [생성형 인공지능(AI) 개발·활용을 위한 개인정보 처리 안내서(안)](https://www.aitimes.kr/news/download.php?subUploadDir=202508/&savefilename=35952_300.pdf&filename=%EC%83%9D%EC%84%B1%ED%98%95%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5(AI)%20%EA%B0%9C%EB%B0%9C%C2%B7%ED%99%9C%EC%9A%A9%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EA%B0%9C%EC%9D%B8%EC%A0%95%EB%B3%B4%20%EC%B2%98%EB%A6%AC%20%EC%95%88%EB%82%B4%EC%84%9C.pdf&idxno=300) | Personal Information Protection Commission, Guidelines on Personal Information Processing for the Development and Utilization of Generative Artificial Intelligence (Draft), August 2025

#### Switzerland

* [Digital Switzerland Strategy 2025](https://digital.swiss/userdata/uploads/strategie-dch-en.pdf)

#### Tanzania

* [Artificial Intelligence Readiness Assessment Report](https://tanzania.un.org/sites/default/files/2025-07/National%20AI%20Readiness%20Report.pdf) | UNESCO, 2025

#### Ukraine

* Ministry of Digital Transformation
  * [White Paper on Artificial Intelligence Regulation in Ukraine: Vision of the Ministry of Digital Transformation of Ukraine](https://thedigital.gov.ua/storage/uploads/files/page/community/docs/%D0%91%D1%96%D0%BB%D0%B0_%D0%BA%D0%BD%D0%B8%D0%B3%D0%B0_%D0%B7_%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8E%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F_%D0%A8%D0%86_%D0%B2_%D0%A3%D0%BA%D1%80%D0%B0%D1%97%D0%BD%D1%96_%D0%90%D0%9D%D0%93%D0%9B.pdf) | Version for Consultation
  * [Дорожня карта з регулювання штучного інтелекту в Україні: Bottom-Up Підхід](https://cms.thedigital.gov.ua/storage/uploads/files/page/community/docs/%D0%94%D0%BE%D1%80%D0%BE%D0%B6%D0%BD%D1%8F_%D0%BA%D0%B0%D1%80%D1%82%D0%B0_%D0%B7_%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8E%D0%B2%D0%B0%D0%BD%D0%BD%D1%8F_%D0%A8%D0%86_%D0%B2_%D0%A3%D0%BA%D1%80%D0%B0%D1%97%D0%BD%D1%96_compressed.pdf)
* [Guidelines on the Responsible Use of Artificial Intelligence in the News Media](https://webportal.nrada.gov.ua/wp-content/uploads/2024/05/Ukraine-AI-Guidelines-for-Media.pdf) | Ministry of Digital Transformation, Ministry of Culture and Information Policy, and National Council of Television and Radio Broadcasting

#### United Kingdom

* [AI and the Law: A Discussion Paper](https://cloud-platform-e218f50a4812967ba1215eaecede923f.s3.amazonaws.com/uploads/sites/54/2025/07/AI-paper-PDF.pdf) | Law Commission, 2025
* [AI Safety Institute, Safety cases at AISI](https://www.aisi.gov.uk/work/safety-cases-at-aisi) | (AISI)
* [Artificial Intelligence Playbook for the UK Government](https://assets.publishing.service.gov.uk/media/67aca2f7e400ae62338324bd/AI_Playbook_for_the_UK_Government__12_02_.pdf) | Government Digital Service and Department for Science, Innovation & Technology, February 2025
* [Beginner's guide to measurement GPG118](https://www.npl.co.uk/gpgs/beginners-guide-to-measurement) | National Physical Laboratory (NPL)
* Department for Science, Innovation and Technology
  * [The Bletchley Declaration by Countries Attending the AI Safety Summit](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023) | 1-2 November 2023
  * [Evaluation of the Cyber AI Hub programme | January 8, 2025](https://www.gov.uk/government/publications/evaluation-of-the-northern-ireland-cyber-ai-hub-programme/evaluation-of-the-cyber-ai-hub-programme)
  * [Frontier AI: capabilities and risks](https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper)
  * [Generative Artificial Intelligence in the Education System](https://www.niassembly.gov.uk/globalassets/documents/raise/publications/2022-2027/2025/education/2725.pdf) | Northern Ireland Assembly, Research and Information Service, March 20, 2025
  * [Global Coalition on Telecommunications: principles on AI adoption in the telecommunications industry](https://www.gov.uk/government/publications/global-coalition-on-telecommunications-principles-on-ai-adoption-in-the-telecommunications-industry/global-coalition-on-telecommunications-principles-on-ai-adoption-in-the-telecommunications-industry) | January 16, 2025
  * [Introduction to AI Assurance](https://www.gov.uk/government/publications/introduction-to-ai-assurance)
* [Information Commissioner's Office, AI tools in recruitment](https://ico.org.uk/action-weve-taken/audits-and-overview-reports/ai-tools-in-recruitment/) | ICO, November 6, 2024
* [International Scientific Report on the Safety of Advanced AI](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai) | Department for Science, Innovation and Technology and AI Safety Institute
* [Media literacy](https://publications.parliament.uk/pa/ld5901/ldselect/ldcomm/163/163.pdf) | House of Lords Communications and Digital Committee, 3rd Report of Session 2024-25
* [Northern Ireland response to the AI Council AI Roadmap](https://matrixni.org/wp-content/uploads/2021/04/NI-Response-to-UK-AI-roadmap.pdf)
* [Online Harms White Paper: Full government response to the consultation](https://www.gov.uk/government/consultations/online-harms-white-paper)
* [Parliamentary Office of Science and Technology](https://researchbriefings.files.parliament.uk/documents/POST-PN-0735/POST-PN-0735.pdf) | (POST), POSTnote 735, Energy Security and AI
* [The Public Sector Bodies Accessibility Regulations 2018](https://www.legislation.gov.uk/uksi/2018/852/contents/made) | (Websites and Mobile Applications)
* [Red Teaming for GenAI Harms: Revealing the Risks and Rewards for Online Safety | July 23, 2024](https://www.ofcom.org.uk/siteassets/resources/documents/consultations/discussion-papers/red-teaming/red-teaming-for-gen-ai-harms.pdf?v=370762) | Ofcom
* [The safe and effective use of AI in education: Leadership toolkit video transcripts](https://assets.publishing.service.gov.uk/media/6842e04ee5a089417c8060c5/Leadership_Toolkit_-_Transcript.pdf) | Department of Education, June 2025
* [Trusted third-party AI assurance roadmap](https://www.gov.uk/government/publications/trusted-third-party-ai-assurance-roadmap/trusted-third-party-ai-assurance-roadmap) | Department for Science, Innovation & Technology, September 3, 2025
* [US AISI and UK AISI Joint Pre-Deployment Test: Anthropic's Claude 3.5 Sonnet](https://www.nist.gov/system/files/documents/2024/11/19/Upgraded%20Sonnet-Publication-US.pdf) | October 2024 Release
* [US AISI and UK AISI Joint Pre-Deployment Test: OpenAI o1](https://www.nist.gov/system/files/documents/2024/12/18/US_UK_AI%20Safety%20Institute_%20December_Publication-OpenAIo1.pdf) | December 2024
* [Use of AI in Legislatures](https://www.niassembly.gov.uk/globalassets/documents/raise/publications/2022-2027/2025/clg/3325.pdf) | Northern Ireland Assembly, September 2024

#### United States (Federal Government)

**Bureau of Labor Statistics**
* [Bureau of Labor Statistics Report to the Committees on Appropriations of the House of Representatives and the Senate on Measuring the Effects of New Technologies on the American Workforce](https://www.bls.gov/bls/congressional-reports/measuring-the-effects-of-new-technologies-on-the-american-workforce.pdf)
* [Incorporating AI impacts in BLS employment projections: occupational case studies](https://www.bls.gov/opub/mlr/2025/article/incorporating-ai-impacts-in-bls-employment-projections.htm) | February 2025

**Consumer Financial Protection Bureau (CFPB)**  
* [12 CFR Part 1002 - Equal Credit Opportunity Act](https://www.consumerfinance.gov/policy-compliance/rulemaking/regulations/1002/) | (Regulation B)
* [Chatbots in consumer finance](https://www.consumerfinance.gov/data-research/research-reports/chatbots-in-consumer-finance/chatbots-in-consumer-finance/)
* [Innovation spotlight: Providing adverse action notices when using AI/ML models](https://www.consumerfinance.gov/about-us/blog/innovation-spotlight-providing-adverse-action-notices-when-using-ai-ml-models/)

**Commodity Futures Trading Commission (CFTC)**  
* [A Primer on Artificial Intelligence in Securities Markets](https://www.cftc.gov/media/2846/LabCFTC_PrimerArtificialIntelligence102119/download)
* [Responsible Artificial Intelligence in Financial Markets](https://www.cftc.gov/PressRoom/PressReleases/8905-24)

**Congressional Budget Office**
* [H.R. 9720, AI Incident Reporting and Security Enhancement Act](https://www.cbo.gov/system/files/2024-10/hr9720.pdf)

**Congressional Research Service**
* [Artificial Intelligence in Health Care](https://crsreports.congress.gov/product/pdf/R/R48319) | December 30, 2024
* [Artificial Intelligence and Machine Learning in Financial Services](https://crsreports.congress.gov/product/pdf/R/R47997) | April 3, 2024
* [Artificial Intelligence: Background, Selected Issues, and Policy Considerations](https://crsreports.congress.gov/product/pdf/R/R46795) | May 19, 2021
* [Artificial Intelligence: Overview, Recent Advances, and Considerations for the 118th Congress](https://www.energy.gov/sites/default/files/2023-09/Artificial%20Intelligence%20Overview%2C%20Recent%20Advances%2C%20and%20Considerations%20for%20the%20118th%20Congress.pdf) | August 4, 2023
* [Highlights of the 2023 Executive Order on Artificial Intelligence for Congress](https://crsreports.congress.gov/product/pdf/R/R47843/2) | November 17, 2023

**Copyright Office**
* [Copyright and Artificial Intelligence Part 1 Digital Replicas](https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-1-Digital-Replicas-Report.pdf) |  July 2024
* [Copyright and Artificial Intelligence Part 2 Copyrightability](https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-2-Copyrightability-Report.pdf) |  January 2025
* [Copyright and Artificial Intelligence Part 3 Generative AI Training](https://www.copyright.gov/ai/Copyright-and-Artificial-Intelligence-Part-3-Generative-AI-Training-Report-Pre-Publication-Version.pdf) | May 2025

**Data.gov**
* [Privacy Policy and Data Policy](https://data.gov/privacy-policy/)

**Defense Advanced Research Projects Agency (DARPA)**
* [Explainable Artificial Intelligence](https://www.darpa.mil/program/explainable-artificial-intelligence) |  (XAI) (Archived)

**Defense Technical Information Center**  
* [Computer Security Technology Planning Study](https://apps.dtic.mil/sti/citations/AD0758206) | October 1, 1972

**Department of Agriculture (USDA)**
* [Fiscal Year 2025-2026 AI Strategy](https://www.usda.gov/sites/default/files/documents/fy-2025-2026-usda-ai-strategy.pdf)

**Department of Commerce**
* [Artificial intelligence](https://www.commerce.gov/issues/artificial-intelligence)
* [Bureau of Industry and Security](https://www.bis.gov/)
  * [Department of Commerce Rescinds Biden-Era Artificial Intelligence Diffusion Rule, Strengthens Chip-Related Export Controls](https://media.bis.gov/sites/default/files/documents/05.07%20Recission%20of%20AI%20Diffusion%20Press%20Release.pdf) | Bureau of Industry and Security, May 12, 2025
  * [Framework for Artificial Intelligence Diffusion](https://public-inspection.federalregister.gov/2025-00636.pdf)
* [Intellectual property](https://www.commerce.gov/issues/intellectual-property)
* [National Telecommunications and Information Administration](https://www.ntia.gov/) | (NTIA)
  * [AI Accountability Policy Report](https://www.ntia.gov/issues/artificial-intelligence/ai-accountability-policy-report)
  * [AI System Documentation](https://www.ntia.gov/issues/artificial-intelligence/ai-accountability-policy-report/developing-accountability-inputs-a-deeper-dive/information-flow/ai-system-documentation)
  * [Internet Policy Task Force, Commercial Data Privacy and Innovation in the Internet Economy: A Dynamic Policy Framework](https://www.ntia.doc.gov/files/ntia/publications/iptf_privacy_greenpaper_12162010.pdf)
  * [NTIA Artificial Intelligence Accountability Policy Report](https://www.ntia.gov/sites/default/files/publications/ntia-ai-report-final.pdf) | March 2024
* [National Institute of Standards and Technology](https://www.nist.gov/)
  * [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations](https://csrc.nist.gov/pubs/ai/100/2/e2023/final) | NIST AI 100-2 E2023
  * [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations updated](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2025.pdf) | NIST AI 100-2e2025
  * [Artificial Intelligence Risk Management Framework](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf) | AI 100-1 (NIST AI RMF 1.0)
  * [Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf) | NIST AI 600-1
  * [Assessing Risks and Impacts of AI](https://ai-challenges.nist.gov/aria/library) | (ARIA)
  * [De-identification Tools](https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools)
  * [Engineering Statistics Handbook](https://doi.org/10.18434/M32189)
  * [Four Principles of Explainable Artificial Intelligence](https://www.nist.gov/system/files/documents/2020/08/17/NIST%20Explainable%20AI%20Draft%20NISTIR8312%20%281%29.pdf) | Draft NISTIR 8312, 2020-08-17
  * [Four Principles of Explainable Artificial Intelligence](https://www.nist.gov/publications/four-principles-explainable-artificial-intelligence) | NISTIR 8312, 2021-09-29
  * [Guide for Conducting Risk Assessments](https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-30r1.pdf) | NIST Special Publication 800-30 Revision 1,
  * [Measurement Uncertainty](https://www.nist.gov/itl/sed/topic-areas/measurement-uncertainty)
    * [International Bureau of Weights and Measures, Evaluation of measurement data—Guide to the expression of uncertainty in measurement](https://www.bipm.org/documents/20126/2071204/JCGM_100_2008_E.pdf/cb0ef43f-baa5-11cf-3f85-4dcd86f77bd6) | (BIPM)
  * [Psychological Foundations of Explainability and Interpretability in Artificial Intelligence](https://nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8367.pdf)
* National Oceanic and Atmospheric Administration (NOAA)
  * [NOAA Artificial Intelligence Strategy: Analytics for Next-Generation Earth Science](https://sciencecouncil.noaa.gov/wp-content/uploads/2023/04/2020-AI-Strategy.pdf) | February 2020
* Office of the Under Secretary for Economic Affairs
  * [Generative Artificial Intelligence and Open Data](https://www.commerce.gov/sites/default/files/2025-01/GenerativeAI-Open-Data.pdf) | Guidelines and Best Practices, Version 1, January 16, 2025
* [Outline: Proposed Zero Draft for a Standard on AI Testing, Evaluation, Verification, and Validation](https://www.nist.gov/system/files/documents/2025/07/15/Outline_%20Proposed%20Zero%20Draft%20for%20a%20Standard%20on%20AI%20TEVV-for-web.pdf)
* [SP 800-53 Control Overlays for Securing AI Systems](https://csrc.nist.gov/csrc/media/Projects/cosais/documents/NIST-Overlays-SecuringAI-concept-paper.pdf) | NIST
* [U.S. Artificial Intelligence Safety Institute](https://www.nist.gov/aisi) | (USAISI)
  * [US AISI and UK AISI Joint Pre-Deployment Test: Anthropic's Claude 3.5 Sonnet](https://www.nist.gov/system/files/documents/2024/11/19/Upgraded%20Sonnet-Publication-US.pdf) | October 2024 Release
  * [US AISI and UK AISI Joint Pre-Deployment Test: OpenAI o1](https://www.nist.gov/system/files/documents/2024/12/18/US_UK_AI%20Safety%20Institute_%20December_Publication-OpenAIo1.pdf) | December 2024

**Department of Defense**  
* [AI Data Security](https://media.defense.gov/2025/May/22/2003720601/-1/-1/0/CSI_AI_DATA_SECURITY.PDF) | Joint Cybersecurity Information, Best Practices for Securing Data Used to Train & Operate AI Systems, May 2025, Ver. 1.0
* [AI Principles: Recommendations on the Ethical Use of Artificial Intelligence](https://media.defense.gov/2019/Oct/31/2002204458/-1/-1/0/DIB_AI_PRINCIPLES_PRIMARY_DOCUMENT.PDF)
* [Audit of Governance and Protection of Department of Defense Artificial Intelligence Data and Technology](https://media.defense.gov/2020/Jul/01/2002347967/-1/-1/1/DODIG-2020-098.PDF)
* [Content Credentials: Strengthening Multimedia Integrity in the Generative AI Era](https://media.defense.gov/2025/Jan/29/2003634788/-1/-1/0/CSI-CONTENT-CREDENTIALS.PDF) | January 2025
* [Chief Data and Artificial Intelligence Officer Assessment and Assurance](https://gitlab.jatic.net/home) | (CDAO)
    * [RAI Toolkit](https://rai.tradewindai.com/)
* Department of the Army
  * [Proceedings of the Thirteenth Annual U.S. Army Operations Research Symposium](https://apps.dtic.mil/sti/pdfs/ADA007126.pdf) | Volume 1, October 29 to November 1, 1974
* [Guidelines for secure AI system development](https://media.defense.gov/2023/Nov/27/2003346994/-1/-1/0/GUIDELINES-FOR-SECURE-AI-SYSTEM-DEVELOPMENT.PDF)
* [U.S. Department of Defense Responsible Artificial Intelligence Strategy and Implementation Pathway](https://media.defense.gov/2022/Jun/22/2003022604/-1/-1/0/Department-of-Defense-Responsible-Artificial-Intelligence-Strategy-and-Implementation-Pathway.PDF) | June 2022

**Department of Education**
* [Inventory of U.S. Department of Education AI Use Cases](https://www.ed.gov/about/ed-overview/artificial-intelligence-ai-guidance)
* [Office of Educational Technology](https://tech.ed.gov/)
  * [Designing for Education with Artificial Intelligence: An Essential Guide for Developers](https://tech.ed.gov/designing-for-education-with-artificial-intelligence/)
  * [Empowering Education Leaders: A Toolkit for Safe, Ethical, and Equitable AI Integration](https://tech.ed.gov/education-leaders-ai-toolkit/) | October 2024

**Department of Energy**
* [Artificial Intelligence and Technology Office](https://www.energy.gov/ai/artificial-intelligence-technology-office)
  * [AI Risk Management Playbook](https://www.energy.gov/ai/doe-ai-risk-management-playbook-airmp) | (AIRMP)
  * [AI Use Case Inventory](https://www.energy.gov/sites/default/files/2023-07/DOE_2023_AI_Use_Case_Inventory_0.pdf) | (DOE Use Cases Releasable to Public in Accordance with E.O. 13960)
  * [Digital Climate Solutions Inventory](https://www.energy.gov/sites/default/files/2022-09/Digital_Climate_Solutions_Inventory.pdf)
  * [Generative Artificial Intelligence Reference Guide](https://www.energy.gov/sites/default/files/2024-06/Generative%20AI%20Reference%20Guide%20v2%206-14-24.pdf)

**Department of Health and Human Services**
* [Strategic Plan for the Use of Artificial Intelligence in Health Human Services and Public Health Strategic Plan](https://irp.nih.gov/system/files/media/file/2025-03/2025-hhs-ai-strategic-plan_full_508.pdf) | January 2025

**Department of Homeland Security**  
* [Acquisition and Use of Artificial Intelligence and Machine Learning Technologies by DHS Components](https://www.dhs.gov/sites/default/files/2023-09/23_0913_mgmt_139-06-acquistion-use-ai-technologies-dhs-components.pdf) | Policy Statement 139-06, August 8, 2023
* [Artificial Intelligence and Autonomous Systems](https://www.dhs.gov/science-and-technology/artificial-intelligence)
* [Artificial Intelligence Safety and Security Board](https://www.dhs.gov/artificial-intelligence-safety-and-security-board)
* [Department of Homeland Security Artificial Intelligence Roadmap 2024](https://www.dhs.gov/sites/default/files/2024-03/24_0315_ocio_roadmap_artificialintelligence-ciov3-signed-508.pdf)
* [DHS Has Taken Steps to Develop and Govern Artificial Intelligence, But More Action is Needed to Ensure Appropriate Use](https://www.oig.dhs.gov/sites/default/files/assets/2025-02/OIG-25-10-Jan25.pdf) | Office of Inspector General, OIG-25-10, January 30, 2025
* [DHS Playbook for Public Sector Generative Artificial Intelligence Deployment](https://www.dhs.gov/sites/default/files/2025-01/25_0106_ocio_dhs-playbook-for-public-sector-generative-artificial-intelligence-deployment-508-signed.pdf) | January 2025
* [Roles and Responsibilities Framework for Artificial Intelligence in Critical Infrastructure](https://www.dhs.gov/sites/default/files/2024-11/24_1114_dhs_ai-roles-and-responsibilities-framework-508.pdf) | November 14, 2024
* [Safety and Security Guidelines for Critical Infrastructure Owners and Operators](https://www.dhs.gov/publication/safety-and-security-guidelines-critical-infrastructure-owners-and-operators)
* [The Department of Homeland Security Simplified Artificial Intelligence Use Case Inventory](https://www.dhs.gov/ai/use-case-inventory)
  * [AI at DHS: A Deep Dive into our Use Case Inventory](https://www.dhs.gov/news/2024/12/16/ai-dhs-deep-dive-our-use-case-inventory)
* [Use of Commercial Generative Artificial Intelligence Tools](https://www.dhs.gov/sites/default/files/2023-11/23_1114_cio_use_generative_ai_tools.pdf)

**Department of Justice**  
* [Artificial Intelligence Strategy for the U.S. Department of Justice](https://www.justice.gov/d9/pages/attachments/2021/02/04/doj_artificial_intelligence_strategy_december_2020.pdf) | December 2020
* [Civil Rights Division, Artificial Intelligence and Civil Rights](https://www.justice.gov/crt/ai)
* [Privacy Act of 1974](https://www.justice.gov/opcl/privacy-act-1974)
  * [Overview of The Privacy Act of 1974](https://www.justice.gov/opcl/overview-privacy-act-1974-2020-edition) | (2020 Edition)
* [Shaping the Department's Artificial Intelligence Efforts 2021-2025](https://www.justice.gov/archives/media/1385331/dl?inline)

**Department of Labor**
* [Artificial Intelligence Use Case Inventory](https://www.dol.gov/agencies/oasam/centers-offices/ocio/ai-inventory)
* [Validation of Employee Selection Procedures](https://web.archive.org/web/20250103095140/https://www.dol.gov/agencies/ofccp/faqs/employee-selection-procedures) | Office of Federal Contract Compliance Programs (archived)

**Department of State**
* [Artificial Intelligence](https://www.state.gov/artificial-intelligence/)
* [AI Inventory 2024](https://2021-2025.state.gov/department-of-state-ai-inventory-2024/) |  (Archived Content)
* [Enterprise Artificial Intelligence Strategy FY2024-FY-2025 Empowering Diplomacy through Responsible AI](https://www.state.gov/wp-content/uploads/2023/11/Department-of-State-Enterprise-Artificial-Intelligence-Strategy.pdf) | October 2023

**Department of the Treasury**  
* Internal Revenue Service (IRS)
  * [Interim Policy for AI Governance](https://www.irs.gov/pub/foia/ig/spder/raas-10-0325-0001-public.pdf) | March 11, 2025
* [Managing Artificial Intelligence-Specific Cybersecurity Risks in the Financial Services Sector](https://home.treasury.gov/system/files/136/Managing-Artificial-Intelligence-Specific-Cybersecurity-Risks-In-The-Financial-Services-Sector.pdf) | March 2024

**Department of Veterans Affairs**
* [Building the Future: VA’s Strategy for Adopting High-Impact Artificial Intelligence to Improve Services for Veterans](https://department.va.gov/ai/building-the-future-vas-strategy-for-adopting-high-impact-artificial-intelligence-to-improve-services-for-veterans/)

**Equal Employment Opportunity Commission (EEOC)**
* [EEOC Letter](https://www.bennet.senate.gov/public/_cache/files/0/a/0a439d4b-e373-4451-84ed-ba333ce6d1dd/672D2E4304D63A04CC3465C3C8BF1D21.letter-to-chair-dhillon.pdf) | from U.S. senators re: hiring software
* [Questions and Answers to Clarify and Provide a Common Interpretation of the Uniform Guidelines on Employee Selection Procedures](https://www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines)

**Executive Office of the President of the United States**
* [Consumer Data Privacy in a Networked World: A Framework for Protecting Privacy and Promoting Innovation in the Global Digital Economy, February 2012](https://obamawhitehouse.archives.gov/sites/default/files/privacy-final.pdf) | Obama White House Archives
* [Fact Sheet Eliminating Barriers for Federal Artificial Intelligence Use and Procurement](https://www.whitehouse.gov/wp-content/uploads/2025/02/AI-Memo-Fact-Sheet.pdf)
* [Framework to Advance AI Governance and Risk Management in National Security](https://ai.gov/wp-content/uploads/2024/10/NSM-Framework-to-Advance-AI-Governance-and-Risk-Management-in-National-Security.pdf)
* [Winning the Race: America's AI Action Plan](https://whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf) | July 2025

**Federal Aviation Administration (FAA)**
* [Roadmap for Artificial Intelligence Safety Assurance](https://www.faa.gov/media/82891) | Version I, July 23, 2024

**Federal Deposit Insurance Corporation (FDIC)**  

**Federal Housing Finance Agency (FHFA)**
* [Advisory Bulletin AB 2013-07 Model Risk Management Guidance](https://www.fhfa.gov/sites/default/files/2023-03/ab_2013-07_model_risk_management_guidance.pdf)

**Federal Reserve**
* [Supervisory Guidance on Model Risk Management](https://www.federalreserve.gov/supervisionreg/srletters/sr1107a1.pdf)

**Federal Trade Commission (FTC)**
* [Business Blog](https://www.ftc.gov/business-guidance/blog)
  * [2021-01-11 Facing the facts about facial recognition](https://www.ftc.gov/business-guidance/blog/2021/01/facing-facts-about-facial-recognition)  
  * [2022-07-11 Location, health, and other sensitive information: FTC committed to fully enforcing the law against illegal use and sharing of highly sensitive data](https://www.ftc.gov/business-guidance/blog/2022/07/location-health-and-other-sensitive-information-ftc-committed-fully-enforcing-law-against-illegal)
  * [2023-07-25 Protecting the privacy of health information: A baker’s dozen takeaways from FTC cases](https://www.ftc.gov/business-guidance/blog/2023/07/protecting-privacy-health-information-bakers-dozen-takeaways-ftc-cases)
  * [2023-08-16 Can’t lose what you never had: Claims about digital ownership and creation in the age of generative AI](https://www.ftc.gov/business-guidance/blog/2023/08/cant-lose-what-you-never-had-claims-about-digital-ownership-creation-age-generative-ai)
  * [2023-08-22 For business opportunity sellers, FTC says “AI” stands for “allegedly inaccurate”](https://www.ftc.gov/business-guidance/blog/2023/08/business-opportunity-sellers-ftc-says-ai-stands-allegedly-inaccurate)
  * [2023-09-15 Updated FTC-HHS publication outlines privacy and security laws and rules that impact consumer health data](https://www.ftc.gov/business-guidance/blog/2023/09/updated-ftc-hhs-publication-outlines-privacy-security-laws-rules-impact-consumer-health-data)
  * [2023-09-18 Companies warned about consequences of loose use of consumers’ confidential data](https://www.ftc.gov/business-guidance/blog/2023/09/companies-warned-about-consequences-loose-use-consumers-confidential-data)
  * [2023-09-27 Could PrivacyCon 2024 be the place to present your research on AI, privacy, or surveillance?](https://www.ftc.gov/business-guidance/blog/2023/09/could-privacycon-2024-be-place-present-your-research-ai-privacy-or-surveillance)
  * [2022-05-20 Security Beyond Prevention: The Importance of Effective Breach Disclosures](https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2022/05/security-beyond-prevention-importance-effective-breach-disclosures)
  * [2023-02-01 Security Principles: Addressing underlying causes of risk in complex systems](https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2023/02/security-principles-addressing-underlying-causes-risk-complex-systems)
  * [2023-06-29 Generative AI Raises Competition Concerns](https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2023/06/generative-ai-raises-competition-concerns)
  * [2023-12-19 Coming face to face with Rite Aid’s allegedly unfair use of facial recognition technology](https://www.ftc.gov/business-guidance/blog/2023/12/coming-face-face-rite-aids-allegedly-unfair-use-facial-recognition-technology)
* [Children's Online Privacy Protection Rule](https://www.ftc.gov/legal-library/browse/rules/childrens-online-privacy-protection-rule-coppa) | ("COPPA")
* [Privacy Policy](https://www.ftc.gov/policy-notices/privacy-policy)

**Food and Drug Administration**
* [Artificial Intelligence-Enabled Device Software Functions: Lifecycle Management and Marketing Submission Recommendations](https://www.fda.gov/media/184856/download) | Draft Guidance for Industry and FDA Staff, January 7, 2025
* [Artificial Intelligence/Machine Learning-Based: Software as a Medical Device Action Plan](https://www.fda.gov/media/145022/download) | SaMD, updated January 2021
* [Software as a Medical Device guidance](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/software-medical-device-samd-clinical-evaluation) | SaMD guidance, December 8, 2017

**General Services Administration**
* [AI Guide for Government](https://coe.gsa.gov/coe/ai-guide-for-government/introduction/)

**Government Accountability Office (GAO)**
* [Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities, GAO-21-519SP](https://www.gao.gov/products/gao-21-519sp) | June 2021
  * [Highlights of GAO-21-519SP](https://www.gao.gov/assets/gao-21-519sp-highlights.pdf)
* [Artificial Intelligence: Generative AI Use and Management at Federal Agencies](https://www.gao.gov/assets/gao-25-107653.pdf) | July 2025
* [Artificial Intelligence: Use and Oversight in Financial Services GAO-25-107197](https://www.gao.gov/assets/gao-25-107197.pdf) | May 19, 2025
* [Considerations for the Use of Artificial Intelligence to Support Regulatory Decision-Making for Drug and Biological Products Draft Guidance](https://www.fda.gov/media/184830/download) | January 2025
* [Fraud and Improper Payments: Data Quality and a Skilled Workforce Are Essential for Unlocking the Benefits of Artificial Intelligence](https://www.gao.gov/assets/gao-25-108412.pdf)
* [Generative AI's Environmental and Human Effects](https://www.gao.gov/assets/gao-25-107172.pdf) | Technology Assessment, Artificial Intelligence, April 2025
* [Veteran Suicide: VA Efforts to Identify Veterans at Risk through Analysis of Health Record Information](https://www.gao.gov/assets/gao-22-105165.pdf)

**NASA**
* [Examining Proposed Uses of LLMs to Produce or Assess Assurance Arguments](https://ntrs.nasa.gov/api/citations/20250001849/downloads/NASA-TM-20250001849.pdf) | NASA/TM–20250001849, March 2025
* [NASA Framework for the Ethical Use of Artificial Intelligence](https://ntrs.nasa.gov/api/citations/20210012886/downloads/NASA-TM-20210012886.pdf) | NASA/TM-20210012886, April 2021

**National Archives**
* [Potential Labor Market Impacts of Artificial Intelligence: An Empirical Analysis](https://bidenwhitehouse.archives.gov/wp-content/uploads/2024/07/Potential-Labor-Market-Impacts-of-Artificial-Intelligence-An-Empirical-Analysis-July-2024.pdf)

**National Association of Attorneys General**
* [Letter to Congress Opposing AI Preemption Amendment](https://www.scag.gov/media/opvgxagq/2025-05-15-letter-to-congress-re-proposed-ai-preemption-_final.pdf) | Letter by state attorneys general opposing federal preemption of state AI regulation, May 16, 2025

**National Endowment for the Humanities**
* [Policy on the Use of Artificial Intelligence for NEH Grant Proposals](https://www.neh.gov/sites/default/files/2024-10/NEH.AI_.Policy-10.23.24.pdf) | October 23, 2024

**National Security Agency (NSA)**
* [Central Security Service, Artificial Intelligence Security Center](https://www.nsa.gov/AISC/)

**National Security Commission on Artificial Intelligence**  
* [Final Report](https://assets.foleon.com/eu-central-1/de-uploads-7e3kk3/48187/nscai_full_report_digital.04d6b124173c.pdf)

**Office of the Comptroller of the Currency (OCC)**  
* [2021 Model Risk Management Handbook](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)
* [AI in Financial Services Remarks at NFHA Responsible AI Symposium](https://occ.gov/news-issuances/speeches/2025/pub-speech-2025-38.pdf) | Rodney E. Hood, April 29, 2025

**Office of the Director of National Intelligence (ODNI)**
* [The AIM Initiative: A Strategy for Augmenting Intelligence Using Machines](https://www.dni.gov/files/ODNI/documents/AIM-Strategy.pdf)
* [Artificial Intelligence Ethics Framework for the Intelligence Community v 1.0 as of June 2020](https://www.dni.gov/files/ODNI/documents/AI_Ethics_Framework_for_the_Intelligence_Community_10.pdf)
* [Principles of Artificial Intelligence Ethics for the Intelligence Community](https://www.intel.gov/principles-of-artificial-intelligence-ethics-for-the-intelligence-community)
* [Annual Threat Assessment of the U.S. Intelligence Community](https://www.dni.gov/files/ODNI/documents/assessments/ATA-2025-Unclassified-Report.pdf) | March 2025

**Office of Management and Budget (OMB)**
* [M-25-21 Memorandum for the Heads of Executive Departments and Agencies - Accelerating Federal Use of AI through Innovation, Governance, and Public Trust](https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-21-Accelerating-Federal-Use-of-AI-through-Innovation-Governance-and-Public-Trust.pdf) | April 3, 2025
* [M-25-22 Memorandum for the Heads of Executive Departments and Agencies - Driving Efficient Acquisition of Artificial Intelligence in Government](https://www.whitehouse.gov/wp-content/uploads/2025/02/M-25-22-Driving-Efficient-Acquisition-of-Artificial-Intelligence-in-Government.pdf) | April 3, 2025

**Office of Personnel Management**
* [The Artificial Intelligence Classification Policy and Talent Acquisition Guidance - The AI in Government Act of 2020](https://chcoc.gov/sites/default/files/The%20Artificial%20Intelligence%20Classification%20Policy%20and%20Talent%20Acquisition%20Guidance%20-%20The%20AI%20in%20Government%20Act%20of%202020.pdf) | April 29, 2024

**Securities and Exchange Commission (SEC)**  
* [Investor Advisory Committee Meeting Agenda for Thursday](https://www.sec.gov/about/advisory-committees/investor-advisory-committee/iac030625-agenda) | March 6, 2025
* [SEC Charges Two Investment Advisers with Making False and Misleading Statements About Their Use of Artificial Intelligence](https://www.sec.gov/news/press-release/2024-36)

**Social Security Administration (SSA)**  
* [Compliance Plan for OMB Memoranda M-24-10](https://www.ssa.gov/ai/policy/SSA%20M-24-10%20Compliance%20Plan.pdf) | September 2024

**United States Patent and Trademark Office (USPTO)**
* [Artificial Intelligence Strategy](https://www.uspto.gov/sites/default/files/documents/uspto-ai-strategy.pdf) | January 2025
* [Public Views on Artificial Intelligence and Intellectual Property Policy](https://www.uspto.gov/sites/default/files/documents/USPTO_AI-Report_2020-10-07.pdf)

**United States Congress, House of Representatives**
* [Bipartisan House Task Force Report on Artificial Intelligence](https://republicans-science.house.gov/_cache/files/a/a/aa2ee12f-8f0c-46a3-8ff8-8e4215d6a72b/E4AF21104CB138F3127D8FF7EA71A393.ai-task-force-report-final.pdf) | 118th Congress, December 2024
* [Letter to Inflection AI re: AI Censorship](https://judiciary.house.gov/sites/evo-subsites/republicans-judiciary.house.gov/files/evo-media-document/2025-03-13-jdj-to-inflection-ai-white-re-ai-censorship-1%29.pdf) | U.S. House Judiciary Committee, March 13, 2025

**United States Congress, Senate**
* [Decoupling America’s Artificial Intelligence Capabilities from China Act](https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf) | U.S. Senate, 119th Congress, introduced by Senator Josh Hawley, January 2025
* [Driving U.S. Innovation in Artificial Intelligence: A Roadmap for Artificial Intelligence Policy in the United States Senate](https://www.schumer.senate.gov/imo/media/doc/Roadmap_Electronic1.32pm.pdf) | The Bipartisan Senate AI Working Group, May 2024
* [Letter to DOJ Re FARA AI Violation](https://www.commerce.senate.gov/services/files/55267EFF-11A8-4BD6-BE1E-61452A3C48E3) | Senator Ted Cruz to Attorney General Merrick Garland, Committee on Commerce, Science, and Transportation, 2024.11.21
* [Letter to Sundar Pichai concerning Google's decision to reverse its previous safety and ethical commitments on its development of AI products](https://www.markey.senate.gov/imo/media/doc/letter_to_google_on_ai_principles_revisions2.pdf) | Letter from Senators Edward J. Markey, Jeffrey A. Merkley, and Peter Welch, February 19, 2025

**United States Web Design System (USWDS)**
* [Design principles](https://designsystem.digital.gov/design-principles/)

#### United States (State Governments)

**Alabama**
* [Artificial Intelligence Governance Policy AI-GV-P1](https://oit.alabama.gov/wp-content/uploads/2025/01/Artificial-Intelligence-Governance-Policy.pdf) | State of Alabama, Office of Information Technology, Version 1, Effective January 31, 2025
* [Generative AI Task Force Final Report](https://governor.alabama.gov/assets/2025/03/GenAI-TaskForce-Report_Final_20250321.pdf) | State of Alabama, Governor's Task Force on Generative Artificial Intelligence: Providing for the Responsible and Productive Use of Generative Artificial Intelligence in State Government, March 2025

**California**
* [California Consumer Privacy Act](https://oag.ca.gov/privacy/ccpa) | (CCPA)
* California Department of Justice
  * [How to Read a Privacy Policy](https://www.oag.ca.gov/privacy/facts/online-privacy/privacy-policy)
  * [Office of the Attorney General, California Attorney General's Legal Advisory on the Application of Existing California Laws to Artificial Intelligence](https://oag.ca.gov/system/files/attachments/press-docs/Legal%20Advisory%20-%20Application%20of%20Existing%20CA%20Laws%20to%20Artificial%20Intelligence.pdf)
* [California Department of Technology, GenAI Executive Order](https://cdt.ca.gov/technology-innovation/artificial-intelligence-community/genai-executive-order/)
* [California Privacy Protection Agency, Draft Risk Assessment and Automated Decisionmaking Technology Regulations](https://cppa.ca.gov/meetings/materials/20240308_item4_draft_risk.pdf) | CPPA, March 2024
* [The California Report on Frontier AI Policy](https://www.gov.ca.gov/wp-content/uploads/2025/06/June-17-2025-%E2%80%93-The-California-Report-on-Frontier-AI-Policy.pdf) | Joint California Policy Working Group on AI Frontier Models, June 17, 2025
* [Office of the Attorney General of California, California Privacy Rights Act](https://www.oag.ca.gov/system/files/initiatives/pdfs/19-0021A1%20%28Consumer%20Privacy%20-%20Version%203%29_1.pdf) | (CPRA)
* [Sonoma County Administrative Policy 9-6 Information Technology Artificial Intelligence Policy](https://sonomacounty.ca.gov/Main%20County%20Site/Administrative%20Support%20%26%20Fiscal%20Services/HR/Employee%20Resources/Administrative%20Policy%20Manual/9-6%20AI%20Policy/AI%20Policy.pdf) | September 10, 2024

**Colorado**
* [Report and Recommendations: Artificial Intelligence Impact Task Force](https://leg.colorado.gov/sites/default/files/images/report_and_recommendations_5.pdf) | Colorado Legislative Council Staff, February 2025

**Connecticut**
* [State of Connecticut Judicial Branch JBAPPM Policy 1013 Artificial Intelligence Responsible Use Framework, Meaningful Guardrails + Workforce Empowerment and Education + Purposeful Use = Responsible AI Innovation](https://www.jud.ct.gov/faq/CTJBResponsibleAIPolicyFramework2.1.24.pdf) | Version 1.0, February 1, 2024
* [State of Connecticut Policy AI-01 AI Responsible Use Framework, Meaningful Guardrails + Workforce Empowerment and Education + Purposeful Use = Responsible AI Innovation](https://portal.ct.gov/-/media/OPM/Fin-General/Policies/CT-Responsible-AI-Policy-Framework-Final-02012024.pdf) | Version 1.0, February 1, 2024

**Florida**
* [Provenance of Digital Content Florida HB 369 Bill Analysis](https://www.flsenate.gov/Session/Bill/2025/369/Analyses/h0369e.COM.PDF) | Florida House of Representatives, April 2025
* [Report on Miami-Dade County's Policy on Artificial Intelligence–Directive No. 231203](https://documents.miamidade.gov/mayor/memos/03.22.24-Report-on-Miami-Dade-Countys-Policy-on-Artificial-Intelligence-Directive-No-231203.pdf) | Miami-Dade County, March 22, 2024
* [Second Report on Miami-Dade County's Policy on Artificial Intelligence Directive No. 231203](https://www.miamidade.gov/technology/library/artificial-intelligence-report-2025.pdf) | Miami-Dade County, April 8, 2025

**Illinois**
* [Illinois Supreme Court Policy on Artificial Intelligence](https://ilcourtsaudio.blob.core.windows.net/antilles-resources/resources/e43964ab-8874-4b7a-be4e-63af019cb6f7/Illinois%20Supreme%20Court%20AI%20Policy.pdf) | Effective January 1, 2025

**Indiana**
* [State of Indiana Artificial Intelligence](https://www.in.gov/mph/cdo/files/State-of-Indiana-Artificial-Intelligence-Policy.pdf) | Version 1.1, December 2024

**Kentucky**
* [080.101 AI/Gen AI Policy Version 1.1](https://www.chfs.ky.gov/agencies/os/oats/polstand/080101AIGen%20AI.pdf) | Cabinet for Health and Family Services, February 27, 2025
* [Artificial Intelligence Guidance Brief 2024](https://www.education.ky.gov/districts/tech/Documents/AI%20Guidance%20Brief.pdf) | Kentucky Department of Education
* [Research Report No. 491 Executive Branch Use of Artificial Intelligence Technology](https://apps.legislature.ky.gov/lrc/publications/ResearchReports/RR491.pdf) | Legislative Research Commission

**Maine**
* [Generative Artificial Intelligence Policy](https://www.maine.gov/oit/sites/maine.gov.oit/files/inline-files/GenAIPolicy.pdf) | Maine State Government, Department of Administrative and Financial Services, Office of Information Technology (OIT), issued July 19, 2024, revised February 28, 2025

**Massachusetts**
* [Enterprise Use and Development of Generative Artificial Intelligence Policy](https://www.mass.gov/doc/enterprise-use-and-development-of-generative-artificial-intelligence-policy/download) | Executive Office of Technology Services and Security (EOTSS), Enterprise Privacy Office, GenAI Policy, AI.001, January 31, 2025

**Mississippi**
* [Mississippi Department of Education, Artificial Intelligence Guidance for K-12 Classrooms](https://www.mdek12.org/sites/default/files/Offices/MDE/OTSS/DL/ai_guidance_final.pdf)

**Nebraska**
* [Nebraska Information Technology Commission 8-609 Artificial intelligence policy](https://nitc.nebraska.gov/standards/8-609.pdf) | November 8, 2024

**New Jersey**
* [Legal Practice Preliminary Guidelines on the Use of Artificial Intelligence by New Jersey Lawyers](https://www.njcourts.gov/sites/default/files/notices/2024/01/n240125a.pdf)

**New York**
* [Acceptable Use of Artificial Intelligence Technologies](https://its.ny.gov/system/files/documents/2025/04/nys-p24-001-acceptable-use-of-artificial-intelligence-technologies.pdf) | Office of Information Technology Services, March 11, 2025
* [The New York City Artificial Intelligence Action Plan](https://www.nyc.gov/assets/oti/downloads/pdf/reports/artificial-intelligence-action-plan.pdf) | October 2023
* [New York City Automated Decision Systems Task Force Report](https://www.nyc.gov/assets/adstaskforce/downloads/pdf/ADS-Report-11192019.pdf) | November 2019
* [New York State Emerging Technology Advisory Board: Recommendations for making NY a leader in responsible AI](https://filecache.mediaroom.com/mr5mr_ibmnewsroom/198517/IBM-ETAB-Report-white-paper-DIGITAL-20241212%5B30%5D.pdf)
* [New York State Artificial Intelligence Governance](https://www.osc.ny.gov/files/state-agencies/audits/pdf/sga-2025-23s50.pdf) | Office of the New York State Comptroller, Report 2023-S-50, April 2025
* [Use of External Consumer Data and Information Sources in Underwriting for Life Insurance](https://www.dfs.ny.gov/industry_guidance/circular_letters/cl2019_01)

**North Carolina**
* [AI Accelerator](https://it.nc.gov/resources/artificial-intelligence/ai-accelerator) | Department of Information Technology
* [North Carolina State Government Responsible Use of Artificial Intelligence Framework](https://it.nc.gov/documents/nc-state-government-responsible-use-artificial-intelligence-framework/download?attachment) | August 2024

**South Carolina**
* [South Carolina State Agencies Artificial Intelligence Strategy](https://admin.sc.gov/sites/admin/files/Documents/OED/Final%20SC%20AI%20Strategy.pdf) | June 2024

**North Dakota**
* [State of North Dakota Artificial Intelligence Policy](https://www.ndit.nd.gov/sites/www/files/documents/Policies/artificial_intelligence_policy_2024.pdf)

**Pennsylvania**
* [Artificial Intelligence Policy](https://www.pa.gov/content/dam/copapwp-pagov/en/oa/documents/policies/it-policies/artificial%20intelligence%20policy.pdf) | Office of Administration, March 11, 2025
* [Lessons from Pennsyklvania's Generative AI Pilot with ChatGPT](https://www.pa.gov/content/dam/copapwp-pagov/en/oa/documents/programs/information-technology/documents/openai-pilot-report-2025.pdf) | March 2025

**Tennessee**
* [Artificial Intelligence and Generative AI Policy ISM 20](https://www.nashville.gov/sites/default/files/2024-04/ISM-20-Artificial-Intelligence-and-Generative-Artificial-Intelligence-Use.pdf?ct=1713207273) | Metropolitan Government of Nashville and Davidson County, Department of Information Technology Services, Version 1.0, April 15, 2024
* [Enterprise Artificial Intelligence policy 200-POL-007](https://www.tn.gov/content/dam/tn/finance/artificial-intelligence/Enterprise_Artificial_Intelligence_Policy.pdf) | Department of Finance and Administration, Strategic Technology Solutions, Version 1.0, October 30, 2024

**Texas**
* [Artificial Intelligence Strategic Plan Fiscal Years 2025-2027](https://www.txdot.gov/content/dam/docs/str/ai-strategic-plan-09-20-2024.pdf) | Texas Department of Transportation
* [Federal Reserve Bank of Dallas, Regulation B, Equal Credit Opportunity, Credit Scoring Interpretations: Withdrawal of Proposed Business Credit Amendments](https://fraser.stlouisfed.org/files/docs/historical/frbdal/circulars/frbdallas_circ_19820603_no82-063.pdf) | June 3, 1982

**Utah**
* [Artificial Intelligence Framework for Utah P-12 Education](https://www.utah.gov/pmn/files/1116147.pdf) | March 2024
* [Questions from the Commission on Protecting Privacy and Preventing Discrimination](https://auditor.utah.gov/wp-content/uploads/sites/6/2021/02/Office-of-the-State-Auditor-Questions-to-help-Procuring-Agencies-_-Entities-with-Software-Procurement-Feb-1-2021-Final.pdf)

**Virginia**
* [Policy Standards for the Utilization of Artificial Intelligence by the Commonwealth of Virginia](https://www.vita.virginia.gov/media/vitavirginiagov/it-governance/ea/pdf/Utilization-of-Artificial-Intelligence-by-COV-Policy-Standard.pdf)

**Washington**
* [City of Seattle Generative Artificial Intelligence Policy POL-209](https://seattle.gov/documents/Departments/SeattleIT/City-of-Seattle-Generative-Artificial-Intelligence-Policy.pdf)
* [Washington Technology Solutions Reports & Documents](https://watech.wa.gov/about/reports-documents)
  * [Guidelines for Deployment of Generative AI](https://watech.wa.gov/sites/default/files/2024-12/Equity%20Analysis%20Guidelines%20for%20Deployment%20of%20Generative%20AI-WaTech_2024.pdf) | December 2024
  * [Implementing risk assessments for high-risk AI systems](https://watech.wa.gov/sites/default/files/2025-01/EO%2024-01%20Risk%20Guidance_Final.pdf) | December 2024
  * [Initial procurement guidelines for public sector procurement, deployment, and monitoring of Generative AI Technology](https://watech.wa.gov/sites/default/files/2024-11/Initial%20Procurement%20Guidelines%20for%20GenAI%20Final.pdf) | September 2024
  * [Interim Guidelines for Purposeful and Responsible Use of Generative Artificial Intelligence](https://watech.wa.gov/sites/default/files/2023-09/State%2520Agency%2520Generative%2520AI%2520Guidelines%25208-7-23%2520.pdf) | August 8, 2023
  * [Office of Privacy and Data Protection Performance Report](https://watech.wa.gov/sites/default/files/2024-12/OPDP%202024%20Performance%20Report%20Final%2012-1-24.pdf) | December 1, 2024
  * [Responsible AI in the Public Sector: How the Washington State Government Uses & Governs Artificial Intelligence](https://watech.wa.gov/sites/default/files/2025-01/Responsible%20AI%20in%20the%20Public%20Sector%20-%20WaTech%20%20UC%20Berkeley%20Report%20-%20Final_.pdf) | January 31, 2025
  * [State of Washington Generative Artificial Intelligence Report](https://watech.wa.gov/sites/default/files/2024-10/WA_State_GenAIReport_FINAL.pdf) | September 2024

**Wyoming**
* Wyoming Department of Education (WDE)
 * [AI Guidance Resources](https://wde.instructure.com/courses/826)
 * [Guidance for Wyoming School Districts on Developing Artificial Intelligence Use Policy](https://edu.wyoming.gov/wp-content/uploads/2024/06/Guidance-for-AI-Policy-Development.pdf)

#### International and Multilateral Frameworks

#### European Union Policies and Regulations

#### Council of Europe

* [Democracy and the Rule of Law](https://rm.coe.int/1680afae3c) | Council of Europe Framework Convention on Artificial Intelligence and Human Rights
* [Discussion paper on Draft Recommendation on AI literacy](https://rm.coe.int/discussion-paper-on-draft-recommendation-on-ai-literacy/1680b5b6f2) | Wayne Holmes, February 25, 2025
* [European Audiovisual Observatory, IRIS, AI and the audiovisual sector: navigating the current legal landscape](https://rm.coe.int/iris-2024-3-ia-legal-landscape/1680b1e999)
* [Guidelines on the Responsible Implementation of Artificial Intelligence Systems in Journalism](https://rm.coe.int/cdmsi-2023-014-guidelines-on-the-responsible-implementation-of-artific/1680adb4c6)
* [On the Use of Artificial Intelligence in the Framework of the Syrian War](https://www.genocideprevention.eu/files/On_the_Use_of_Artificial_Intelligence_in_the_framework_of_the_Syrian_War.pdf) | Budapest Centre for Mass Atrocities Prevention, July 2021
* [Privacy and Data Protection Risks in Large Language Models](https://rm.coe.int/privacy-and-data-protection-risks-in-large-language-models-llms-v1-0/1680b631dd) | Consultative Committee of the Convention for the Protection of Individuals with Regard to Automatic Processing of Personal Data, Convention 108, June 17, 2025
* [Recommendation CM/Rec-2020-1 of the Committee of Ministers to member States on the human rights impacts of algorithmic systems](https://search.coe.int/cm?i=09000016809e1154) | Adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies
* [The Framework Convention on Artificial Intelligence](https://www.coe.int/en/web/artificial-intelligence/the-framework-convention-on-artificial-intelligence)
  * [Explanatory Report to the Council of Europe Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law](https://rm.coe.int/1680afae67)

#### European Commission and Parliament

* [AI Act: Commission issues draft guidance and reporting template on serious AI incidents, and seeks stakeholders' feedback](https://digital-strategy.ec.europa.eu/en/consultations/ai-act-commission-issues-draft-guidance-and-reporting-template-serious-ai-incidents-and-seeks)
* [AI-driven Innovation in Medical Imaging: Focus on Lung Cancer and Cardiovascular Diseases](https://publications.jrc.ec.europa.eu/repository/bitstream/JRC142224/JRC142224_01.pdf)
* [Assessment List for Trustworthy Artificial Intelligence for self-assessment - Shaping Europe’s digital future - European Commission](https://ec.europa.eu/digital-single-market/en/news/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment) | (ALTAI)
* [Addressing AI risks in the workplace: Workers and algorithms](https://tinyurl.com/38sxrjtk) | European Parliament
* [Artificial Intelligence and Civil Liability: A European Perspective](https://www.europarl.europa.eu/RegData/etudes/STUD/2025/776426/IUST_STU%282025%29776426_EN.pdf) | European Parliament, Policy Department for Justice, Civil Liberties and Institutional Affairs, Directorate-General for Citizens' Rights, Justice and Institutional Affairs, July 2025
* [Civil liability regime for artificial intelligence](https://www.europarl.europa.eu/doceo/document/TA-9-2020-0276_EN.pdf)
* European Commission
  * [Analysis of the preliminary AI standardisation work plan in support of the AI Act](https://publications.jrc.ec.europa.eu/repository/handle/JRC132833)
  * [Communication from the Commission, Artificial Intelligence for Europe](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM%3A2018%3A237%3AFIN) | (4/25/2018)
  * [Data Protection Certification Mechanisms: Study on Articles 42 and 43 of the Regulation 2016/679](https://commission.europa.eu/publications/study-data-protection-certification-mechanisms_en?prefLang=lv) | (EU)
  * [Ethical guidelines on the use of artificial intelligence and data in teaching and learning for Educators](https://school-education.ec.europa.eu/system/files/2023-12/ethical_guidelines_on_the_use_of_artificial_intelligence-nc0722649enn_0.pdf)
  * [Ethics By Design and Ethics of Use Approaches for Artificial Intelligence](https://ec.europa.eu/info/funding-tenders/opportunities/docs/2021-2027/horizon/guidance/ethics-by-design-and-ethics-of-use-approaches-for-artificial-intelligence_he_en.pdf) | Version 1.0, November 25, 2021
  * [Ethics Guidelines for Trustworthy AI](https://www.aepd.es/sites/default/files/2019-12/ai-ethics-guidelines.pdf) | European Commission Independent High-Level Expert Group on Artificial Intelligence, April 8, 2019
  * [European approach to artificial intelligence](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence)
  * [First Draft of the General-Purpose AI Code of Practice published, written by independent experts](https://ec.europa.eu/newsroom/dae/redirection/document/109946)
  * [A Framework to Categorise Modified General-Purpose AI Models as New Models Based on Behavioural Changes](https://publications.jrc.ec.europa.eu/repository/bitstream/JRC143257/JRC143257_01.pdf)
  * [Hiroshima Process International Guiding Principles for Advanced AI system](https://digital-strategy.ec.europa.eu/en/library/hiroshima-process-international-guiding-principles-advanced-ai-system)
  * [Ethics Guidelines for Trustworthy AI](https://www.europarl.europa.eu/cmsdata/196377/AI%20HLEG_Ethics%20Guidelines%20for%20Trustworthy%20AI.pdf) | Independent High-Level Expert Group on Artificial Intelligence
  * [Living Guidelines on the Responsible Use of Generative AI in Research](https://research-and-innovation.ec.europa.eu/document/download/2b6cf7e5-36ac-41cb-aab5-0d32050143dc_en?filename=ec_rtd_ai-guidelines.pdf) | (ERA Forum Stakeholders' document, First Version, March 2024)
  * [Living repository to foster learning and exchange on AI literacy](https://digital-strategy.ec.europa.eu/en/library/living-repository-foster-learning-and-exchange-ai-literacy)
  * [Living Repository of AI Literacy Practices](https://ec.europa.eu/newsroom/dae/redirection/document/112203) | v. 31.01.2025
  * [Policy and Investment Recommendations for Trustworthy AI](https://www.europarl.europa.eu/cmsdata/196378/AI%20HLEG_Policy%20and%20Investment%20Recommendations.pdf) | Independent High-Level Expert Group on Artificial Intelligence
  * [Procurement of AI Updated EU AI model contractual clauses](https://public-buyers-community.ec.europa.eu/communities/procurement-ai/resources/updated-eu-ai-model-contractual-clauses)
* [Proposal for a directive on adapting non-contractual civil liability rules to artificial intelligence: Complementary impact assessment](https://tinyurl.com/3yvcp8pa)
* [Proposal for a Regulation laying down harmonised rules on artificial intelligence](https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence-artificial-intelligence) | (Artificial Intelligence Act)
  * [Amendments adopted by the European Parliament on 14 June 2023 on the proposal for a regulation of the European Parliament and of the Council on laying down harmonised rules on artificial intelligence and amending certain Union legislative acts](https://www.europarl.europa.eu/doceo/document/TA-9-2023-0236_EN.html) |  (Artificial Intelligence Act)
* [The Digital Services Act package](https://digital-strategy.ec.europa.eu/en/policies/digital-services-act-package) | (EU Digital Services Act and Digital Markets Act)
* [The impact of the General Data Protection Regulation on artificial intelligence](https://tinyurl.com/ynf3m8zf) | (GDPR), European Parliament
* [Roadmap for lawful and effective access to data for law enforcement](https://data.consilium.europa.eu/doc/document/ST-10806-2025-INIT/en/pdf) | June 24, 2025

#### European Council

* [Artificial intelligence act: Council and Parliament strike a deal on the first rules for AI in the world](https://www.consilium.europa.eu/en/press/press-releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-rules-for-ai/)
* [Council Conclusions on the Use of Artificial Intelligence in the Field of Justice](https://data.consilium.europa.eu/doc/document/ST-16933-2024-INIT/en/pdf) | December 16, 2024

#### European Data Protection Authorities

* [AI Auditing documents](https://www.edpb.europa.eu/our-work-tools/our-documents/support-pool-expert-projects/ai-auditing_en) | European Data Protection Board (EDPB)
* [Artificial Intelligence and Algorithms in Risk Assessment: Addressing Bias, Discrimination and other Legal and Ethical Issues](https://www.ela.europa.eu/sites/default/files/2023-08/ELA-Handbook-AI-training.pdf) | European Labour Authority (ELA)
* [Data Protection Authority of Belgium General Secretariat, Artificial Intelligence Systems and the GDPR: A Data Protection Perspective](https://www.autoriteprotectiondonnees.be/publications/artificial-intelligence-systems-and-the-gdpr---a-data-protection-perspective.pdf)
* [First EDPS Orientations for EUIs using Generative AI](https://www.edps.europa.eu/data-protection/our-work/publications/guidelines/2024-06-03-first-edps-orientations-euis-using-generative-ai_en) | European Data Protection Supervisor
* [Generative AI and the EUDPR. First EDPS Orientations for ensuring data protection compliance when using Generative AI systems](https://www.edps.europa.eu/system/files/2024-06/24-06-03_genai_orientations_en.pdf) | European Data Protection Supervisor, June 3, 2024
* [Opinion 28/2024 on certain data protection aspects related to the processing of personal data in the context of AI models](https://www.edpb.europa.eu/system/files/2024-12/edpb_opinion_202428_ai-models_en.pdf) | European Data Protection Board (EDPB)
* [Training curriculum on AI and data protection: Fundamentals of Secure AI Systems with Personal Data](https://www.edpb.europa.eu/system/files/2025-06/spe-training-on-ai-and-data-protection-technical_en.pdf)

#### European Union entities (various)

* [Analysis of EU AI Office stakeholder consultations: defining AI systems and prohibited applications](https://ec.europa.eu/newsroom/dae/redirection/document/115454) | Centre for European Policy Studies, May 2025
  * [Multi-Stakeholder Consultation for Commission Guidelines on the Application of the Definition of an AI System and the Prohibited AI Practices Established in the AI Act](https://ec.europa.eu/newsroom/dae/redirection/document/115453)
* [The changing DNA of serious and organised crime](https://www.europol.europa.eu/cms/sites/default/files/documents/EU-SOCTA-2025.pdf) | Europol 2025
* [Guiding Principles for Automated Decision-Making in the EU](https://www.europeanlawinstitute.eu/fileadmin/user_upload/p_eli/Publications/ELI_Innovation_Paper_on_Guiding_Principles_for_ADM_in_the_EU.pdf) | European Law Institute (ELI), 2022
* [Trustworthiness for AI in Defence: Developing Responsible, Ethical, and Trustworthy AI Systems for European Defence](https://eda.europa.eu/docs/default-source/brochures/taid-white-paper-final-09052025.pdf) | European Defence Agency, May 9, 2025

#### OECD
* [AI, data governance and privacy: Synergies and areas of international co-operation](https://www.oecd.org/en/publications/ai-data-governance-and-privacy_2476b1a4-en.html) | June 26, 2024
* [Algorithm Impact Assessment Toolkit](https://oecd.ai/en/catalogue/tools/algorithm-impact-assessment-toolkit)
* [OECD.AI Catalogue of Tools & Metrics for Trustworthy AI, Anekanta AI, Responsible AI Governance Framework for boards](https://oecd.ai/en/catalogue/tools/responsible-ai-governance-framework-for-boards)
* [OECD Artificial Intelligence Papers](https://www.oecd-ilibrary.org/science-and-technology/oecd-artificial-intelligence-papers_dee339a8-en)
  * [No. 1, September 18, 2023, Initial policy considerations for generative artificial intelligence](https://www.oecd-ilibrary.org/deliver/fae2d1e6-en.pdf?itemId=%2Fcontent%2Fpaper%2Ffae2d1e6-en&mimeType=pdf)
  * [No. 2, October 17, 2023, Emerging trends in AI skill demand across 14 OECD countries](https://www.oecd-ilibrary.org/deliver/7c691b9a-en.pdf?itemId=%2Fcontent%2Fpaper%2F7c691b9a-en&mimeType=pdf)
  * [No. 3, October 27, 2023, The state of implementation of the OECD AI Principles four years on](https://www.oecd-ilibrary.org/deliver/835641c9-en.pdf?itemId=%2Fcontent%2Fpaper%2F835641c9-en&mimeType=pdf)
  * [No. 4, October 27, 2023, Stocktaking for the development of an AI incident definition](https://www.oecd-ilibrary.org/deliver/c323ac71-en.pdf?itemId=%2Fcontent%2Fpaper%2Fc323ac71-en&mimeType=pdf)
  * [No. 5, November 7, 2023, Common guideposts to promote interoperability in AI risk management](https://www.oecd-ilibrary.org/deliver/ba602d18-en.pdf?itemId=%2Fcontent%2Fpaper%2Fba602d18-en&mimeType=pdf)
  * [No. 6, November 13, 2023, What technologies are at the core of AI?](https://www.oecd-ilibrary.org/deliver/32406765-en.pdf?itemId=%2Fcontent%2Fpaper%2F32406765-en&mimeType=pdf)
  * [No. 7, November 24, 2023, Using AI to support people with disability in the labour market](https://www.oecd-ilibrary.org/deliver/008b32b7-en.pdf?itemId=%2Fcontent%2Fpaper%2F008b32b7-en&mimeType=pdf)
  * [No. 8, March 5, 2024, Explanatory memorandum on the updated OECD definition of an AI system](https://www.oecd-ilibrary.org/deliver/623da898-en.pdf?itemId=%2Fcontent%2Fpaper%2F623da898-en&mimeType=pdf)
  * [No. 9, December 15, 2023, Generative artificial intelligence in finance](https://www.oecd-ilibrary.org/deliver/ac7149cc-en.pdf?itemId=%2Fcontent%2Fpaper%2Fac7149cc-en&mimeType=pdf)
  * [No. 10, January 19, 2024, Collective action for responsible AI in health](https://www.oecd-ilibrary.org/deliver/f2050177-en.pdf?itemId=%2Fcontent%2Fpaper%2Ff2050177-en&mimeType=pdf)
  * [No. 11, March 15, 2024, Using AI in the workplace](https://www.oecd-ilibrary.org/deliver/73d417f9-en.pdf?itemId=%2Fcontent%2Fpaper%2F73d417f9-en&mimeType=pdf)
  * [No. 12, March 22, 2024, Generative AI for anti-corruption and integrity in government](https://www.oecd-ilibrary.org/deliver/657a185a-en.pdf?itemId=%2Fcontent%2Fpaper%2F657a185a-en&mimeType=pdf)
  * [No. 13, April 10, 2024, Artificial intelligence and wage inequality](https://www.oecd-ilibrary.org/deliver/bf98a45c-en.pdf?itemId=%2Fcontent%2Fpaper%2Fbf98a45c-en&mimeType=pdf)
  * [No. 14, April 10, 2024, Artificial intelligence and the changing demand for skills in the labour market](https://www.oecd-ilibrary.org/deliver/88684e36-en.pdf?itemId=%2Fcontent%2Fpaper%2F88684e36-en&mimeType=pdf)
  * [No. 15, April 16, 2024, The impact of Artificial Intelligence on productivity, distribution and growth](https://www.oecd-ilibrary.org/deliver/8d900037-en.pdf?itemId=%2Fcontent%2Fpaper%2F8d900037-en&mimeType=pdf)
  * [No. 16, May 6, 2024, Defining AI incidents and related terms](https://www.oecd-ilibrary.org/deliver/d1a8d965-en.pdf?itemId=%2Fcontent%2Fpaper%2Fd1a8d965-en&mimeType=pdf)
  * [No. 17, May 30, 2024, Artificial intelligence and the changing demand for skills in Canada](https://www.oecd-ilibrary.org/deliver/1b20cdb6-en.pdf?itemId=%2Fcontent%2Fpaper%2F1b20cdb6-en&mimeType=pdf)
  * [No. 18, May 24, 2024, Artificial intelligence, data and competition](https://www.oecd-ilibrary.org/deliver/e7e88884-en.pdf?itemId=%2Fcontent%2Fpaper%2Fe7e88884-en&mimeType=pdf)
  * [No. 19, June 13, 2024, A new dawn for public employment services](https://www.oecd-ilibrary.org/deliver/5dc3eb8e-en.pdf?itemId=%2Fcontent%2Fpaper%2F5dc3eb8e-en&mimeType=pdf)
  * [No. 20, June 13, 2024, Governing with Artificial Intelligence](https://www.oecd-ilibrary.org/deliver/26324bc2-en.pdf?itemId=%2Fcontent%2Fpaper%2F26324bc2-en&mimeType=pdf)
  * [No. 21, June 24, 2024, Using AI to manage minimum income benefits and unemployment assistance](https://www.oecd-ilibrary.org/deliver/718c93a1-en.pdf?itemId=%2Fcontent%2Fpaper%2F718c93a1-en&mimeType=pdf)
  * [No. 22, June 26, 2024, AI, data governance and privacy](https://www.oecd-ilibrary.org/deliver/2476b1a4-en.pdf?itemId=%2Fcontent%2Fpaper%2F2476b1a4-en&mimeType=pdf)
  * [No. 23, August 14, 2024, The potential impact of Artificial Intelligence on equity and inclusion in education](https://www.oecd-ilibrary.org/deliver/15df715b-en.pdf?itemId=%2Fcontent%2Fpaper%2F15df715b-en&mimeType=pdf)
  * [No. 24, September 5, 2024, Regulatory approaches to Artificial Intelligence in finance](https://www.oecd-ilibrary.org/deliver/f1498c02-en.pdf?itemId=%2Fcontent%2Fpaper%2Ff1498c02-en&mimeType=pdf)
  * [No. 25, September 5, 2024, Measuring the demand for AI skills in the United Kingdom](https://www.oecd-ilibrary.org/deliver/1d6474ef-en.pdf?itemId=%2Fcontent%2Fpaper%2F1d6474ef-en&mimeType=pdf)
  * [No. 26, October 31, 2024, Who will be the workers most affected by AI?](https://www.oecd-ilibrary.org/deliver/14dc6f89-en.pdf?itemId=%2Fcontent%2Fpaper%2F14dc6f89-en&mimeType=pdf)
  * [No. 27, November 14, 2024, Assessing potential future artificial intelligence risks, benefits and policy imperatives](https://www.oecd-ilibrary.org/deliver/3f4e3dfb-en.pdf?itemId=%2Fcontent%2Fpaper%2F3f4e3dfb-en&mimeType=pdf)
  * [No. 28, November 20, 2024, Artificial Intelligence and the health workforce](https://www.oecd-ilibrary.org/deliver/9a31d8af-en.pdf?itemId=%2Fcontent%2Fpaper%2F9a31d8af-en&mimeType=pdf)
  * [No. 29, November 22, 2024, Miracle or Myth? Assessing the macroeconomic productivity gains from Artificial Intelligence](https://www.oecd-ilibrary.org/deliver/b524a072-en.pdf?itemId=%2Fcontent%2Fpaper%2Fb524a072-en&mimeType=pdf)
  * [No. 30, December 12, 2024, A Sectoral Taxonomy of AI Intensity](https://www.oecd.org/content/dam/oecd/en/publications/reports/2024/12/a-sectoral-taxonomy-of-ai-intensity_c2baae71/1f6377b5-en.pdf)
  * [No. 31, February 6, 2025, Algorithmic Management in the Workplace: New Evidence from an OECD Employer Survey](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/algorithmic-management-in-the-workplace_3c84ed6d/287c13c4-en.pdf)
  * [No. 32, February 7, 2025, Steering AI's Future: Strategies for Anticipatory Governance](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/steering-ai-s-future_70e4a856/5480ff0a-en.pdf)
  * [No. 33, February 9, 2025, Intellectual Property Issues in Artificial Intelligence Trained on Scraped Data](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/intellectual-property-issues-in-artificial-intelligence-trained-on-scraped-data_a07f010b/d5241a23-en.pdf)
  * [No. 34, February 28, 2025, Towards a Common Reporting Framework for AI Incidents](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/towards-a-common-reporting-framework-for-ai-incidents_8c488fdb/f326d4ac-en.pdf)
  * [No. 35, February 28, 2025, AI Skills and Capabilities in Canada](https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/ai-skills-and-capabilities-in-canada_09294563/87f76682-en.pdf)
* [OECD-Bericht zu Künstlicher Intelligenz in Deutschland](https://www.ki-strategie-deutschland.de/files/downloads/OECD-Bericht_K%C3%BCnstlicher_Intelligenz_in_Deutschland.pdf)
* [OECD Digital Economy Papers, No. 341, November 2022, Measuring the Environmental Impacts of Artificial Intelligence Computer and Applications: The AI Footprint](https://www.oecd.org/content/dam/oecd/en/publications/reports/2022/11/measuring-the-environmental-impacts-of-artificial-intelligence-compute-and-applications_3dddded5/7babf571-en.pdf)
* [OECD Legal Instruments, Recommendation of the Council on Artificial Intelligence, adopted May 22, 2019, amended May 3, 2024](https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449)
* [Open, Useful and Re-usable data Index: 2019](https://www.oecd.org/content/dam/oecd/en/publications/reports/2020/03/open-useful-and-re-usable-data-ourdata-index-2019_4c070c33/45f6de2d-en.pdf) |  (OURdata)
* [Measuring the environmental impacts of artificial intelligence compute and applications](https://www.oecd.org/en/publications/measuring-the-environmental-impacts-of-artificial-intelligence-compute-and-applications_7babf571-en.html)
* [The Bias Assessment Metrics and Measures Repository](https://oecd.ai/en/catalogue/tools/the-bias-assessment-metrics-and-measures-repository)

#### OSCE

* [#SAIFE Resource Hub: Spotlight on Artificial Intelligence and Freedom of Expression](https://www.osce.org/saife/index.html)
* [Artificial Intelligence and Disinformation: State-Aligned Information Operations and the Distortion of the Public Sphere](https://www.osce.org/files/f/documents/e/b/522166.pdf) | July 2022
* [Spotlight on Artificial Intelligence and Freedom of Expression: A Policy Manual](https://www.osce.org/files/f/documents/8/f/510332_1.pdf)

#### NATO

* [AI in Precision Persuasion. Unveiling Tactics and Risks on Social Media](https://stratcomcoe.org/publications/ai-in-precision-persuasion-unveiling-tactics-and-risks-on-social-media/309)
* [Narrative Detection and Topic Modelling in the Baltics](https://stratcomcoe.org/publications/narrative-detection-and-topic-modelling-in-the-baltics/303)
* ["NATO-Mation": Strategies for Leading in the Age of Artificial Intelligence](https://www.ulib.sk/files/english/nato-library/collections/monographs/ndc-research-paper/ndc_rp_15.pdf), NDC Research Paper No. 15, December 2020
* [Summary of the NATO Artificial Intelligence Strategy](https://www.nato.int/cps/en/natohq/official_texts_187617.htm) | October 22, 2021
  * [An Artificial Intelligence Strategy for NATO](https://www.nato.int/docu/review/articles/2021/10/25/an-artificial-intelligence-strategy-for-nato/index.html) | October 25, 2021
* [Summary of NATO's revised Artificial Intelligence strategy](https://www.nato.int/cps/en/natohq/official_texts_227237.htm) | July 10, 2024
* [Virtual Manipulation Brief 2025: From War and Fear to Confusion and Uncertainty](https://stratcomcoe.org/publications/download/VMB-Final-5aa5d.pdf) | NATO Strategic Communications Centre of Excellence, June 2, 2025

#### Indigenous and Tribal Governments and Nations

* [Report of the Artificial Intelligence, Data Sovereignty, and Cybersecurity Task Force](https://www.cherokee.org/media/0ipldvul/task-force-report-on-ai-data-sovereignty-cybersecurity.pdf) | Cherokee Nation, 2025

#### United Nations

* [A Framework for Ethical AI at the United Nations, March 15, 2021](https://unite.un.org/sites/unite.un.org/files/unite_paper_-_ethical_ai_at_the_un.pdf) | Office for Information and Communications Technology
* [A matter of choice: People and possibilities in the age of AI](https://hdr.undp.org/system/files/documents/global-report-document/hdr2025reporten.pdf) | UNDP Human Development Report 2025
* [Casinos, cyber fraud, and trafficking in persons for forced criminality in Southeast Asia](https://www.unodc.org/roseap/uploads/documents/Publications/2023/TiP_for_FC_Policy_Report.pdf) | United Nations Office on Drugs and Crime (UNODC), September 2023
* [Governing AI for Humanity, Final Report](https://digitallibrary.un.org/record/4062495/files/1416782-EN.pdf?ln=en) | September 2024
* [High-Level Advisory Body on Artificial Intelligence](https://www.un.org/techenvoy/ai-advisory-body) | Office of the Secretary-General's Envoy on Technology
* [Office of the United Nations High Commissioner for Human Rights](https://www.ohchr.org/sites/default/files/documents/issues/business/b-tech/taxonomy-GenAI-Human-Rights-Harms.pdf)
* UNESCO
  * [AI and education: guidance for policy-makers](https://unesdoc.unesco.org/ark:/48223/pf0000376709)
  * [AI and the future of education: disruptions, dilemmas and directions](https://unesdoc.unesco.org/ark:/48223/pf0000395236/PDF/395236eng.pdf.multi) | September 2025
  * [Artificial Intelligence: examples of ethical dilemmas](https://www.unesco.org/en/artificial-intelligence/recommendation-ethics/cases)
  * [Caribbean Artificial Intelligence Policy Roadmap](https://unesdoc.unesco.org/ark:/48223/pf0000391996/PDF/391996eng.pdf.multi)
  * [Consultation paper on AI regulation: emerging approaches across the world](https://unesdoc.unesco.org/ark:/48223/pf0000390979)
  * [Global AI Ethics and Governance Observatory](https://www.unesco.org/ethics-ai/en)
  * [Readiness assessment methodology: a tool of the Recommendation on the Ethics of Artificial Intelligence](https://www.unesco.org/en/articles/readiness-assessment-methodology-tool-recommendation-ethics-artificial-intelligence)
  * [Recommendation on the Ethics of Artificial Intelligence](https://unesdoc.unesco.org/ark:/48223/pf0000381137/PDF/381137eng.pdf.multi) | Adopted on 23 November 2021
  * [Smarter, smaller, stronger: resource-efficient generative Al & the future of digital transformation](https://unesdoc.unesco.org/ark:/48223/pf0000394521.locale=en) | 2025
* [Policy guidance on AI for children, Recommendations for building AI policies and systems that uphold child rights](https://www.unicef.org/innocenti/media/1341/file/UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf) | UNICEF
* [Principles for the ethical use of artificial intelligence in the United Nations system](https://unsceb.org/sites/default/files/2023-03/CEB_2022_2_Add.1%20%28AI%20ethics%20principles%29.pdf) | Chief Executives Board for Coordination, 2022-10-27
* [Terms of Reference and Modalities for the Establishment and Functioning of the Independent International Scientific Panel on Artificial Intelligence and the Global Dialogue on Artificial Intelligence Governance](https://documents.un.org/doc/undoc/ltd/n25/222/68/pdf/n2522268.pdf)

### Documents in Legal Genres

Legislation, litigation, and other legal materials relevant to AI policy and governance.

* [AI Learning Agenda](https://www.ncleg.gov/Sessions/2025/Bills/Senate/PDF/S747v0.pdf) | General Assembly of North Carolina, Session 2025, Senate Bill DRS245362-LR-142A	
* [An Act Addressing Innovations in Artificial Intelligence](https://www.cga.ct.gov/2025/ba/pdf/2025SB-01249-R000606-BA.pdf) | OLR Bill Analysis SB 1249
* [An Act relating to artificial intelligence; requiring disclosure of deepfakes in campaign communications; relating to cybersecurity; and relating to data privacy.](https://www.akleg.gov/basis/Bill/Detail/33?Root=HB306) | Alaska State Legislature, HB 306
* [Agenda Book for Advisory Committee on Evidence Rules – Panel on Artificial Intelligence and the Rules of Evidence](https://www.uscourts.gov/sites/default/files/2024-04_agenda_book_for_evidence_rules_meeting_final.pdf) | April 19, 2024  
* [Algorithmic Accountability Act of 2023](https://www.govinfo.gov/app/details/BILLS-118hr5628ih/)
* [Arizona, House Bill 2685](https://www.azleg.gov/legtext/55leg/2r/bills/hb2685h.htm)
* [Australia, Privacy Act 1988](https://www.legislation.gov.au/Details/C2014C00076)
* [California, Civil Rights Council - First Modifications to Proposed Employment Regulations on Automated-Decision Systems, Title 2, California Code of Regulations](https://calcivilrights.ca.gov/wp-content/uploads/sites/32/2024/10/First-Modifications-to-Text-of-Proposed-Modifications-to-Employment-Regulations-Regarding-Automated-Decision-Systems.pdf)
* [California, Consumer Privacy Act of 2018](https://leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&title=1.81.5) | Civil Code - DIVISION 3. OBLIGATIONS [1427 - 3273.69]
* [Cherkin et al. v. PowerSchool Holdings Inc. N.D. Cal. May 2024 – EdTech Privacy Class Action](https://s3.documentcloud.org/documents/25260275/cherkin-v-powerschool-complaint-20240506.pdf)
* [Colorado, SB24-205 Consumer Protections for Artificial Intelligence, Concerning consumer protections in interactions with artificial intelligence systems](https://leg.colorado.gov/bills/SB24-205)
* [Decoupling America’s Artificial Intelligence Capabilities from China Act](https://www.hawley.senate.gov/wp-content/uploads/2025/01/Hawley-Decoupling-Americas-Artificial-Intelligence-Capabilities-from-China-Act.pdf) | U.S. Senate, 119th Congress, introduced by Senator Josh Hawley, January 2025
* [European Union, General Data Protection Regulation](https://gdpr-info.eu/) |  (GDPR)
  * [Article 22 EU GDPR "Automated individual decision-making, including profiling"](https://www.privacy-regulation.eu/en/article-22-automated-individual-decision-making-including-profiling-GDPR.htm)
* [Popa v. Harriet Carter Gifts Inc. W.D. Pa. Mar. 2025 – Class Action on Digital Wiretapping](https://app.midpage.ai/document/popa-v-harriet-carter-gifts-10829535?refG=true)
* [Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government](https://www.federalregister.gov/documents/2020/12/08/2020-27065/promoting-the-use-of-trustworthy-artificial-intelligence-in-the-federal-government) | Executive Order 13960 (2020-12-03)
* [Facial Recognition and Biometric Technology Moratorium Act of 2020](https://drive.google.com/file/d/1gkTcjFtieMQdsQ01dmDa49B6HY9ZyKr8/view)
* [Federal Consumer Online Privacy Rights Act](https://www.consumerprivacyact.com/federal/) | (COPRA)
* [GDPR Complaint Filed by noyb Against OpenAI](https://noyb.eu/sites/default/files/2024-04/OpenAI%20Complaint_EN_redacted.pdf) | Austria DSB, April 2024
* [Germany, Bundesrat Drucksache 222/24 - Entwurf eines Gesetzes zum strafrechtlichen Schutz von Persönlichkeitsrechten vor Deepfakes](https://tinyurl.com/d7r8baz8) |  (Draft Law on the Criminal Protection of Personality Rights from Deepfakes)
* [Illinois, Biometric Information Privacy Act](https://www.ilga.gov/legislation/ilcs/ilcs3.asp?ActID=3004&ChapterID=57)
* [In re Clearview AI Inc. N.D. Ill. Aug. 2022 – MDL Opinion on Amended Complaint & Retail Defendants](https://cases.justia.com/federal/district-courts/illinois/ilndce/1:2021cv00135/395030/407/0.pdf?ts=1660231616)
* [Justice in Policing Act](https://democrats-judiciary.house.gov/issues/issue/?IssueID=14924)
* [National Conference of State Legislatures 2020 Consumer Data Privacy Legislation](https://www.ncsl.org/technology-and-communication/2020-consumer-data-privacy-legislation) | (NCSL)
* [Nebraska, LB1203 - Regulate artificial intelligence in media and political advertisements under the Nebraska Political Accountability and Disclosure Act](https://nebraskalegislature.gov/bills/view_bill.php?DocumentID=55088)
* [The New York Times Company v. Microsoft Corp. OpenAI Inc. et al. December 2023 – Complaint](https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf)
* [The New York Times Company v. Microsoft Corporation OpenAI Inc. et al. November 2024 – Opinion & Order on Discovery Dispute](https://www.sdnyblog.com/files/2024/11/23-cv-11195-SHS-OTW-NYT-v.-Microsoft-Opinion.pdf)
* [Rhode Island, Executive Order 24-06: Artificial Intelligence and Data Centers of Excellence](https://governor.ri.gov/executive-orders/executive-order-24-06)
* [Silverman et al. v. Meta Platforms Inc. N.D. Cal. 2023 Class Action Complaint](https://storage.courtlistener.com/recap/gov.uscourts.cand.415175/gov.uscourts.cand.415175.1.0_3.pdf)
* [State of North Carolina Executive Order No. 24, Advancing Trustworthy Artificial Intelligence That Benefits All North Carolinians](https://governor.nc.gov/executive-order-no-24-advancing-trustworthy-artificial-intelligence-benefits-all-north-carolinians/open) | September 2, 2025
* [Texas draft of responsible AI bill by Capriglione](https://www.mba.org/docs/default-source/policy/state-relations/draft_texas-ai_10.28.24.pdf?sfvrsn=9f83267e_1) | October 28, 2024
* [Thaler v. Perlmutter March 2025 – Appellate Opinion on Copyright and Artificial Intelligence](https://media.cadc.uscourts.gov/opinions/docs/2025/03/23-5233.pdf)
* [Virginia, Consumer Data Protection Act](https://law.lis.virginia.gov/vacodefull/title59.1/chapter53/)
* [Washington State, SB 6513 - 2019-20](https://apps.leg.wa.gov/billsummary/?BillNumber=6513&Year=2020&Initiative=false)
* [United States Congress, 118th Congress, H.R.5586 - DEEPFAKES Accountability Act](https://www.congress.gov/bill/118th-congress/house-bill/5586/text) | 2023-2024
* [United States Congress, 118th Congress, H.R. 9720, AI Incident Reporting and Security Enhancement Act](https://science.house.gov/bills?ID=95D5A008-EA1A-4D43-A363-DC2D129DFDCD) | 2023-2024
* [United States Congress, 118th Congress, S.4769 - VET Artificial Intelligence Act](https://www.congress.gov/bill/118th-congress/senate-bill/4769/text) | 2023-2024
* [Willis v. Bank National Association as Trustee Igloo Series Trust LLC](https://caselaw.findlaw.com/court/us-dis-crt-n-d-tex-dal-div/117272437.html) | 2025

## Education Resources

### Comprehensive Software Examples and Tutorials

This section is a curated collection of guides and tutorials that simplify responsible ML implementation. It spans from basic model interpretability to advanced fairness techniques. Suitable for both novices and experts, the resources cover topics like COMPAS fairness analyses and explainable machine learning via counterfactuals.

* [COMPAS Analysis Using Aequitas](https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb) | ![](https://img.shields.io/github/stars/dssg/aequitas?style=social)
* [Explaining Quantitative Measures of Fairness with SHAP](https://github.com/slundberg/shap/blob/master/notebooks/overviews/Explaining%20quantitative%20measures%20of%20fairness.ipynb) | ![](https://img.shields.io/github/stars/slundberg/shap?style=social)
* [Getting a Window into your Black Box Model](http://projects.rajivshah.com/inter/ReasonCode_NFL.html)
* H20.ai
* [From GLM to GBM Part 1](https://www.h2o.ai/blog/from-glm-to-gbm-part-1/)
* [From GLM to GBM Part 2](https://www.h2o.ai/blog/from-glm-to-gbm-part-2/)
* [IML](https://mybinder.org/v2/gh/christophM/iml/master?filepath=./notebooks/tutorial-intro.ipynb)
* [Interpretable Machine Learning with Python](https://github.com/jphall663/interpretable_machine_learning_with_python) | ![](https://img.shields.io/github/stars/jphall663/interpretable_machine_learning_with_python?style=social)
* [Interpreting Machine Learning Models with the iml Package](http://uc-r.github.io/iml-pkg)
* [Interpretable Machine Learning using Counterfactuals](https://docs.seldon.io/projects/alibi/en/v0.2.0/examples/cf_mnist.html)
* [Machine Learning Explainability by Kaggle Learn](https://www.kaggle.com/learn/machine-learning-explainability)
* [Model Interpretability with DALEX](http://uc-r.github.io/dalex)
  * **Model Interpretation series by Dipanjan (DJ) Sarkar**
    * [Hands-on Machine Learning Model Interpretation](https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608)
    * [Interpreting Deep Learning Models for Computer Vision](https://medium.com/google-developer-experts/interpreting-deep-learning-models-for-computer-vision-f95683e23c1d)
    * [Model Interpretation Strategies](https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739)
    * [The Importance of Human Interpretable Machine Learning](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)
* [Partial Dependence Plots in R](https://journal.r-project.org/archive/2017/RJ-2017-016/)
* PiML
  * [PiML Medium Tutorials](https://piml.medium.com)
  * [PiML-Toolbox Examples](https://github.com/SelfExplainML/PiML-Toolbox/tree/main/examples) | ![](https://img.shields.io/github/stars/SelfExplainML/PiML-Toolbox?style=social)
* [Reliable-and-Trustworthy-AI-Notebooks](https://github.com/ClementSicard/Reliable-and-Trustworthy-AI-Notebooks) | ![](https://img.shields.io/github/stars/ClementSicard/Reliable-and-Trustworthy-AI-Notebooks?style=social)
* [Saliency Maps for Deep Learning](https://medium.com/@thelastalias/saliency-maps-for-deep-learning-part-1-vanilla-gradient-1d0665de3284)
* [Visualizing ML Models with LIME](http://uc-r.github.io/lime)
* [Visualizing and debugging deep convolutional networks](https://rohitghosh.github.io/2018/01/05/visualising-debugging-deep-neural-networks/)
* [What does a CNN see?](https://colab.research.google.com/drive/1xM6UZ9OdpGDnHBljZ0RglHV_kBrZ4e-9)

### Free-ish Books

This section contains books that can be reasonably described as free, including some "historical" books dealing broadly with ethical and responsible tech.

* [Adversarial Model Analysis](https://ama.drwhy.ai/) | Przemyslaw Biecek, 2023
* [An Introduction to Machine Learning Interpretability: An Applied Perspective on Fairness, Accountability, Transparency, and Explainable AI](https://h2o.ai/content/dam/h2o/en/marketing/documents/2019/08/An-Introduction-to-Machine-Learning-Interpretability-Second-Edition.pdf) | Patrick Hall and Navdeep Gill, 2019, Second Edition
* [Artificial Intelligence and Fundamental Rights: The AI Act of the European Union and its implications for global technology regulation](https://irdt-schriften.uni-trier.de/index.php/irdt/catalog/view/6/6/50) | Trier Studies on Digital Law, Volume 4
* [Case Studies in Information and Computer Ethics](https://archive.org/details/unset0000unse_l0l0) | Richard A. Spinello, 1997
* [Case Studies in Information Technology Ethics](https://archive.org/details/casestudiesininf02edspin) | Richard A. Spinello, 2003, Second Edition
* [Computer and Information Ethics](https://archive.org/details/computerinformat0000wood_q3r6) | Marsha Cook Woodbury, 2003
* [Computer Ethics: Analyzing Information Technology](https://archive.org/details/computerethicsan0004edjohn) | Deborah G. Johnson and Keith W. Miller, 2009,  Fourth Edition
* [Computer Power and Human Reason: From Judgment to Calculation](https://archive.org/details/computerpowerhum0000weiz_v0i3/mode/2up) | Joseph Weizenbaum, 1976
* [Computers, Ethics, and Society](https://archive.org/details/computersethicss0000unse) | M. David Ermann, Mary B. Williams, and Claudio Gutierrez, 1990
* [Controlling Technology: Ethics and the Responsible Engineer](https://archive.org/details/controllingtechn0000unge_y4t3) | Stephen H. Unger, 1982, First Edition
* [Controlling Technology: Ethics and the Responsible Engineer](https://archive.org/details/controllingtechn0000unge) | Stephen H. Unger, 1994, Second Edition
* [Ethical Aspects of Information Technology](https://archive.org/details/ethicalaspectsof00spin) | Richard A. Spinello, 1995
* [Ethics for people who work in tech](https://ethicsforpeoplewhoworkintech.com/)
* [Ethics in Information Technology](https://archive.org/details/ethicsininformat0000reyn) | George Reynolds, 2002, Instructor's Edition
* [Ethics in Information Technology](https://archive.org/details/ethicsininformat00reyn) | George Reynolds, 2002
* [Explanatory Model Analysis: Explore, Explain, and Examine Predictive Models. With examples in R and Python](https://ema.drwhy.ai/) | Przemyslaw Biecek and Tomasz Burzykowski, 2020
* [Fairness and Machine Learning: Limitations and Opportunities](https://fairmlbook.org/) | Solon Barocas, Moritz Hardt, and Arvind Narayanan, 2022
* [Fueling Our Future: A Dialogue about Technology, Ethics, Public Policy, and Remedial Action](https://archive.org/details/fuelingourfuture0000unse/mode/2up) | Ed Dreby and Keith Helmuth, contributors, and Judy Lumb, editor, 2009
* [How Humans Judge Machines](https://archive.org/details/mit_press_book_9780262363266) | César A. Hidalgo, Diana Orghian, Jordi Albo-Canals, Filipa de Almeida, and Natalia Martin, 2021
* [Information Technology Ethics: Cultural Perspectives](https://archive.org/details/informationtechn0000unse_k8c9) | Soraj Hongladarom and Charles Ess, 2007
* [Interpretable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/) | Christoph Molnar, 2021
   * [christophM/interpretable-ml-book](https://github.com/christophM/interpretable-ml-book) | ![](https://img.shields.io/github/stars/christophM/interpretable-ml-book?style=social)
* [Normal Accidents: Living with High-Risk Technologies with a New Afterword and a Postscript on the Y2K Problem](https://archive.org/details/normalaccidentsl00perr) | Charles Perrow, 1999
* [Normal Accidents: Living with High-Risk Technologies](https://archive.org/details/normalaccidentsl0000perr) | Charles Perrow, 1984
* [Regulating under Uncertainty: Governance Options for Generative AI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4918704) | Florence G'sell
* [Responsible Machine Learning: Actionable Strategies for Mitigating Risks & Driving Adoption](https://info.h2o.ai/rs/644-PKX-778/images/OReilly_Responsible_ML_eBook.pdf) | Patrick Hall, Navdeep Gill, and Benjamin Cox, 2021
* [Science and Technology Ethics](https://archive.org/details/sciencetechnolog0000unse_k7m6) | Raymond E. Spier (editor), 200
* [Society, Ethics, and Technology](https://archive.org/details/societyethicstec0000unse) | Morton E. Winston and Ralph D. Edelbach, 2003, Second Edition
* [Society, Ethics, and Technology](https://archive.org/details/societyethicstec00edel) | Morton E. Winston and Ralph D. Edelbach, 2006, Third Edition
* [Society, Ethics, and Technology](https://archive.org/details/societyethicstec00wins) | Morton E. Winston and Ralph D. Edelbach, 2000, First Edition
* [The Cambridge Handbook of the Law, Ethics and Policy of Artificial Intelligence](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0AD007641DE27F837A3A16DBC0888DD1/9781009367813AR.pdf/The_Cambridge_Handbook_of_the_Law__Ethics_and_Policy_of_Artificial_Intelligence.pdf?event-type=FTLA) | Nathalie A. Smuha, ed., 2025
* [Towards a Code of Ethics for Artificial Intelligence](https://archive.org/details/towardscodeofeth0000bodd) | Paula Boddington, 2017
* [Trustworthy AI: African Perspectives](https://link.springer.com/book/10.1007/978-3-031-75674-0) | Damian Okaibedi Eke, Kutoma Wakunuma, Simisola Akintoye, and George Ogoh, eds., 2025
* [Trustworthy Machine Learning: Concepts for Developing Accurate, Fair, Robust, Explainable, Transparent, Inclusive, Empowering, and Beneficial Machine Learning Systems](http://www.trustworthymachinelearning.com/) | Kush R. Varshney, 2022
* [Who Shall Live? Medicine, Technology, Ethics](https://archive.org/details/whoshalllivemedi0000hous) | Kenneth Vaux (editor), 1970

### Glossaries and Dictionaries

This section features a collection of glossaries and dictionaries that are geared toward defining terms in ML, including some "historical" dictionaries.

* [50 AI terms every beginner should know](https://www.telusinternational.com/insights/ai-data/article/50-beginner-ai-terms-you-should-know) | TELUS International
* [A Glossary of AI Jargon: 29 AI Terms You Should Know](https://www.makeuseof.com/glossary-ai-jargon-terms/) | MakeUseOf
* [A Multilingual Dictionary of Artificial Intelligence](https://archive.org/details/multilingualdict0000voll) | Otto Vollnhals, 1992 (English, German, French, Spanish, Italian)
* [A.I. For Anyone: The A-Z of AI](https://www.aiforanyone.org/glossary)
* [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations](https://csrc.nist.gov/pubs/ai/100/2/e2023/final) | National Institute of Standards and Technology (NIST), NIST AI 100-2 E2023
* [AI dictionary: Be a native speaker of Artificial Intelligence](https://dataconomy.com/2022/04/23/artificial-intelligence-terms-ai-glossary/) | Dataconomy
* [AI From A to Z: The Generative AI Glossary for Business Leaders](https://www.salesforce.com/blog/generative-ai-glossary/) | Salesforce
* [AI Terms Glossary](https://www.moveworks.com/us/en/resources/ai-terms-glossary) | Moveworks
* [Appen Artificial Intelligence Glossary](https://appen.com/ai-glossary/)
* [Artificial intelligence  glossary](https://post.parliament.uk/artificial-intelligence-ai-glossary/) | UK Parliament
* [Artificial Intelligence  Terms: A to Z Glossary](https://www.coursera.org/articles/ai-terms) | Coursera
* [Artificial intelligence and illusions of understanding in scientific research](https://www.nature.com/articles/s41586-024-07146-0.epdf?sharing_token=cbht6Q72InY18AtY6FiVM9RgN0jAjWel9jnR3ZoTv0Ni_LuMWrIZy_SmHlNQlu9tG1u0SCK_wTYxy6bvMe6U_BE3vc5yFmZEpTbIVJozkVYsOei9LdPpNr_wZzvTp4stmzGM54z-riqwhUCk0DD6_YkY_jcgZBnXR8P_8vyFvYpiCtjFrvczN9Lm6NhmrePm) | (glossary on second page)
* [Artificial Intelligence Definitions](https://hai.stanford.edu/sites/default/files/2023-03/AI-Key-Terms-Glossary-Definition.pdf) | Stanford University HAI
* [Artificial Intelligence Glossary](https://www.siemens.com/global/en/company/stories/artificial-intelligence/ai-glossary.html) | Siemens
* [Artificial Intelligence Terminology: A Glossary for Beginners](https://connect.comptia.org/content/articles/artificial-intelligence-terminology) | CompTIA
* [Brookings: The Brookings glossary of AI and emerging technologies](https://www.brookings.edu/articles/the-brookings-glossary-of-ai-and-emerging-technologies/)
* [Built In, Responsible AI Explained](https://builtin.com/artificial-intelligence/responsible-ai)
* [Center for Security and Emerging Technology: Glossary](https://cset.georgetown.edu/glossary/)
* [Collins Dictionary of Artificial Intelligence](https://archive.org/details/collinsdictionar0000unse_w3w7) | Raoul Smith, 1990
* [Council of Europe Artificial Intelligence Glossary](https://www.coe.int/en/web/artificial-intelligence/glossary)
* [Dictionary of Artificial Intelligence & Robotics](https://archive.org/details/dictionaryofarti00rose) | Jerry M. Rosenberg, 1986
* [Dictionary of Artificial Intelligence](https://archive.org/details/dictionaryofarti0000merc) | Dennis Mercadal, 1990
* [Dictionary of Cognitive Science: Neuroscience, Psychology, Artificial Intelligence, Linguistics, and Philosophy](https://archive.org/details/dictionaryofcogn0000unse) | Oliver Houdé, 2004
* [EU-U.S. Terminology and Taxonomy for Artificial Intelligence](https://digital-strategy.ec.europa.eu/en/library/eu-us-terminology-and-taxonomy-artificial-intelligence-second-edition) | European Commission, Second Edition
* [G2: 70+ A to Z Artificial Intelligence Terms in Technology](https://www.g2.com/articles/artificial-intelligence-terms)
* [General Services Administration: AI Guide for Government: Key AI terminology](https://coe.gsa.gov/coe/ai-guide-for-government/what-is-ai-key-terminology/)
* [Glossary for Discussion of Ethics of Autonomous and Intelligent Systems](https://standards.ieee.org/wp-content/uploads/import/documents/other/eadv2_glossary.pdf) | IEEE, Version 1
* [Glossary of artificial intelligence](https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence) | Wikipedia
* [Glossary of human-centric artificial intelligence](https://publications.jrc.ec.europa.eu/repository/handle/JRC129614) | European Commission
* [Google Developers Machine Learning Glossary](https://developers.google.com/machine-learning/glossary)
* [H2O.ai Glossary](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/glossary.html)
* IAPP
  * [Glossary of Privacy Terms](https://iapp.org/resources/glossary/)
  * [International Definitions of Artificial Intelligence](https://iapp.org/media/pdf/resource_center/international_definitions_of_ai.pdf)
  * [Key Terms for AI Governance](https://iapp.org/resources/article/key-terms-for-ai-governance/)
* [IBM AI glossary](https://www.ibm.com/cloud/architecture/architecture/practices/cognitive-glossary/)
* [International Dictionary of Artificial Intelligence](https://archive.org/details/internationaldic0000rayn_t1n5/mode/2up) | William J. Raynor, Jr, 2009, Second Edition
* [ISO/IEC DIS 22989 Information technology — Artificial intelligence — Artificial intelligence concepts and terminology](https://www.iso.org/obp/ui/fr/#iso:std:iso-iec:22989:dis:ed-1:v1:en)
* [Lexicon](https://www.ai.mil/Lexicon/) | Chief Digital and Artificial Intelligence Office (CDAO)
* [Open Access Vocabulary](https://repository.ifla.org/bitstream/123456789/3272/1/Open%20Access%20Vocabulary%20Feb2024%20v2.pdf)
* [TechTarget: Artificial intelligence glossary: 60+ terms to know](https://www.techtarget.com/whatis/feature/Artificial-intelligence-glossary-60-terms-to-know)
* [The Alan Turing Institute: Data science and AI glossary](https://www.turing.ac.uk/news/data-science-and-ai-glossary)
* [The Facts on File Dictionary of Artificial Intelligence](https://archive.org/details/factsonfiledicti00smit) | Raoul Smith, 1989
* [The International Dictionary of Artificial Intelligence](https://archive.org/details/internationaldic0000rayn/mode/2up) | William J. Raynor, Jr, 1999, First Edition
* [The Language of Trustworthy AI: An In-Depth Glossary of Terms](https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary) | National Institute of Standards and Technology (NIST)
* [The Machine Learning Dictionary](https://www.cse.unsw.edu.au/~billw/mldict.html) | University of New South Wales, Bill Wilson,
* [Towards AI, Generative AI Terminology — An Evolving Taxonomy To Get You Started](https://towardsai.net/p/machine-learning/generative-ai-terminology-an-evolving-taxonomy-to-get-you-started)
* [Vocabulary of AI Risks](https://delaramglp.github.io/vair/) | VAIR

### Open-ish Classes

This section features a selection of educational courses focused on ethical considerations and best practices in ML. The classes range from introductory courses on data ethics to specialized training in fairness and trustworthy deep learning.

* [An Introduction to Data Ethics](https://www.scu.edu/ethics/focus-areas/technology-ethics/resources/an-introduction-to-data-ethics/)
* [Awesome LLM Courses](https://github.com/wikit-ai/awesome-llm-courses) | ![](https://img.shields.io/github/stars/wikit-ai/awesome-llm-courses?style=social)
* [AWS Skill Builder](https://skillbuilder.aws/)
* [Build a Large Language Model - From Scratch](https://github.com/rasbt/LLMs-from-scratch/tree/main) | ![](https://img.shields.io/github/stars/rasbt/LLMs-from-scratch?style=social)
* [Certified Ethical Emerging Technologist](https://certnexus.com/certification/ceet/)
* [Computational Ethics for NLP](http://demo.clab.cs.cmu.edu/ethical_nlp/) | Carnegie Mellon University
* [CS 4910 - Special Topics in Computer Science: Algorithm Audits](https://sapiezynski.com/cs4910.html) | Piotr Sapieżyński
* [CS103F: Ethical Foundations of Computer Science](https://www.cs.utexas.edu/~ans/classes/cs109/schedule.html)
* [Data Ethics course](http://ethics.fast.ai/syllabus) | Fast.ai
* [DeepLearning.AI](https://www.deeplearning.ai/courses/)
* [Disability-Centered AI And Ethics MOOC](https://oecd.ai/en/catalogue/tools/disability-centered-ai-and-ethics-mooc) | OECD.AI
* [ETH Zürich ReliableAI 2022 Course Project repository](https://github.com/angelognazzo/Reliable-Trustworthy-AI) | ![](https://img.shields.io/github/stars/angelognazzo/Reliable-Trustworthy-AI?style=social)
* [Fairness in Machine Learning](https://fairmlclass.github.io/)
* [Generative AI for Educators](https://grow.google/ai-for-educators/) | Grow with Google
* [Generative AI for Everyone](https://www.coursera.org/learn/generative-ai-for-everyone) | Coursera, DeepLearning.AI
* [Generative AI with Large Language Models](https://www.coursera.org/learn/generative-ai-with-llms) | Coursera, DeepLearning.AI
* [Google Cloud Skills Boost](https://www.cloudskillsboost.google/)
  * [Attention Mechanism](https://www.cloudskillsboost.google/course_templates/537)
  * [Create Image Captioning Models](https://www.cloudskillsboost.google/course_templates/542)
  * [Encoder-Decoder Architecture](https://www.cloudskillsboost.google/course_templates/543)
  * [Introduction to Generative AI](https://www.cloudskillsboost.google/course_templates/536)
  * [Introduction to Image Generation](https://www.cloudskillsboost.google/course_templates/541)
  * [Introduction to Large Language Models](https://www.cloudskillsboost.google/course_templates/539)
  * [Introduction to Responsible AI](https://www.cloudskillsboost.google/course_templates/554)
  * [Introduction to Vertex AI Studio](https://www.cloudskillsboost.google/course_templates/552)
  * [Transformer Models and BERT Model](https://www.cloudskillsboost.google/course_templates/538)
* [Human-Centered Machine Learning](http://courses.mpi-sws.org/hcml-ws18/)
* [IBM SkillsBuild](https://sb-auth.skillsbuild.org/)
* [INFO 4270: Ethics and Policy in Data Science](https://docs.google.com/document/d/1GV97qqvjQNvyM2I01vuRaAwHe9pQAZ9pbP7KkKveg1o/)
* [Introduction to AI Ethics](https://www.kaggle.com/code/var0101/introduction-to-ai-ethics)
* [Introduction to Generative AI](https://www.coursera.org/learn/introduction-to-generative-ai) | Coursera, Google Cloud
* [Introduction to Responsible Machine Learning](https://jphall663.github.io/GWU_rml/)
* [Machine Learning Fairness by Google](https://developers.google.com/machine-learning/crash-course/fairness/video-lecture)
* [Prompt Engineering for ChatGPT](https://www.coursera.org/learn/prompt-engineering) | Coursera, Vanderbilt University
* [Tech & Ethics Curricula](https://docs.google.com/spreadsheets/d/1Z0DqQeZ-Aeq6LmD17J5m8zeeIR6ywWnH-WO-jWtXE9M/edit#gid=0)
* [Trustworthy Deep Learning](https://berkeley-deep-learning.github.io/cs294-131-s19/)
* [Visualizing A Neural Machine Translation Model - Mechanics of Seq2seq Models With Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) | Jay Alammar  

### Podcasts and Channels

This section features podcasts and channels (such as on YouTube) that offer insightful commentary and explanations on responsible AI and machine learning interpretability.

* [Internet of Bugs](https://www.youtube.com/@InternetOfBugs/videos)
* [Tech Won't Save Us](https://techwontsave.us/)
* [This Is Technology Ethics: An Introduction](https://technologyethicspod.wordpress.com/)

## AI Incidents, Critiques, and Research Resources

### AI Incident Information Sharing Resources

This section houses initiatives, networks, repositories, and publications that facilitate collective and interdisciplinary efforts to enhance AI safety. It includes platforms where experts and practitioners come together to share insights, identify potential vulnerabilities, and collaborate on developing robust safeguards for AI systems, including AI incident trackers.

* [AI Incident Database](https://incidentdatabase.ai/) | Responsible AI Collaborative
* [AI Vulnerability Database](https://avidml.org/) | (AVID)
* [AIAAIC](https://www.aiaaic.org/)
* [AI Badness: An open catalog of generative AI badness](https://badness.ai/)
* [AI Risk Database](https://airisk.io/)
* [Atlas of AI Risks](https://social-dynamics.net/atlas/)
* [Brennan Center for Justice, Artificial Intelligence Legislation Tracker](https://www.brennancenter.org/our-work/research-reports/artificial-intelligence-legislation-tracker)
* [EthicalTech@GW, Deepfakes & Democracy Initiative](https://blogs.gwu.edu/law-eti/deepfakes-disinformation-democracy/)
* [George Washington University Law School's AI Litigation Database](https://blogs.gwu.edu/law-eti/ai-litigation-database/)
* [Merging AI Incidents Research with Political Misinformation Research: Introducing the Political Deepfakes Incidents Database](https://osf.io/fvqg3/)
* [Mitre's AI Risk Database](https://github.com/mitre-atlas/ai-risk-database) | ![](https://img.shields.io/github/stars/mitre-atlas/ai-risk-database?style=social)
* [OECD AI Incidents Monitor](https://oecd.ai/en/incidents)
* [Resemble.AI Deepfake Incident Database](https://www.resemble.ai/deepfake-database/)
* [Verica Open Incident Database](https://www.thevoid.community/) | (VOID)

#### Bibliography of Papers on AI Incidents and Failures

* [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/pdf/2508.01781)
* [AI Ethics Issues in Real World: Evidence from AI Incident Database](https://doi.org/10.48550/arXiv.2206.07635)
* [Artificial Intelligence Incidents & Ethics: A Narrative Review](https://doi.org/10.54489/ijtim.v2i2.80)
* [Artificial Intelligence Safety and Cybersecurity: A Timeline of AI Failures](https://doi.org/10.48550/arXiv.1610.07997)
* [Center for Countering Digital Hate, YouTube's Anorexia Algorithm: How YouTube Recommends Eating Disorders Videos to Young Girls](https://counterhate.com/wp-content/uploads/2024/12/CCDH.YoutubeED.Nov24.Report_FINAL.pdf) | (CCDH)
* [Deepfake Pornography Goes to Washington: Measuring the Prevalence of AI-Generated Non-Consensual Intimate Imagery Targeting Congress](https://static1.squarespace.com/static/6612cbdfd9a9ce56ef931004/t/67586997eaec5c6ae3bb5e24/1733847451191/ASP+DFP+Report.pdf) | American Sunlight Project, December 11, 2024
* [Deployment Corrections: An Incident Response Framework for Frontier AI Models](https://doi.org/10.48550/arXiv.2310.00328)
* [Exploring Trust With the AI Incident Database](https://doi.org/10.1177/21695067231198084)
* [Indexing AI Risks with Incidents, Issues, and Variants](https://doi.org/10.48550/arXiv.2211.10384)
* [Good Systems, Bad Data?: Interpretations of AI Hype and Failures](https://doi.org/10.1002/pra2.275)
* [Hidden Risks: Artificial Intelligence and Hermeneutic Harm](https://link.springer.com/article/10.1007/s11023-025-09733-0)
* [How Does AI Fail Us? A Typological Theorization of AI Failures](https://aisel.aisnet.org/icis2023/aiinbus/aiinbus/25/)
* [New Noodlophile Stealer Distributes Via Fake AI Video Generation Platforms](https://engage.morphisec.com/hubfs/Noodlophile_Ransomware_ThreatAnalysis.pdf) | Morphisec Threat Analysis
* [Omission and Commission Errors Underlying AI Failures](https://doi.org/10.1007/s00146-022-01585-x)
* [Ontologies for Reasoning about Failures in AI Systems](https://mclumd.github.io/ALMECOM%20Papers/2007/Schmill%20et%20al.%20-%202007%20-%20Ontologies%20for%20reasoning%20about%20failures%20in%20AI%20syst.pdf)
* [Planning for Natural Language Failures with the AI Playbook](https://doi.org/10.1145/3411764.3445735)
* [Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database](https://arxiv.org/abs/2011.08512)
* [SoK: How Artificial-Intelligence Incidents Can Jeopardize Safety and Security](https://doi.org/10.1145/3664476.3664510)
* [The Atlas of AI Incidents in Mobile Computing: Visualizing the Risks and Benefits of AI Gone Mobile](https://doi.org/10.48550/arXiv.2407.15685)
* [Understanding and Avoiding AI Failures: A Practical Guide](https://doi.org/10.3390/philosophies6030053)
* [When Your AI Becomes a Target: AI Security Incidents and Best Practices](https://doi.org/10.1609/aaai.v38i21.30347)
* [Why We Need to Know More: Exploring the State of AI Incident Documentation Practices](https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700)

### AI Law, Policy, and Guidance Trackers

This section contains trackers, databases, and repositories of laws, policies, and guidance pertaining to AI.

* [AI Governance and Regulatory Archive](https://agora.eto.tech/?) | Emerging Technology Observatory, ETO AGORA
* [Artificial Intelligence  Policy Collection](https://digital.library.unt.edu/explore/collections/AIPC/) | University of North Texas
* [Ethical AI Standards in Chile: Responsible and Transparent Algorithms](https://goblab.uai.cl/en/ethical-algorithms/) | GobLab UAI
* [George Washington University Law School's AI Litigation Database](https://blogs.gwu.edu/law-eti/ai-litigation-database/)
* [Global AI Governance Tracker](https://vidhisharmaai.com/global-ai-governance-tracker/) | VidhiSharma.AI
* [Global AI Regulation Tracker](https://www.runwaystrategies.co/global-ai-regulation-tracker) | Runway Strategies
* [Global AI Regulation Tracker](https://www.techieray.com/GlobalAIRegulationTracker) | Raymond Sun
* [Global regulatory tracker - United States](https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-united-states) | White & Case, AI Watch
* International Association of Privacy Professionals (IAPP)
  * [Global AI Legislation Tracker](https://iapp.org/resources/article/global-ai-legislation-tracker/)
  * [UK data protection reform: An overview](https://iapp.org/resources/article/uk-data-protection-reform-an-overview/)
  * [US State Privacy Legislation Tracker](https://iapp.org/resources/article/us-state-privacy-legislation-tracker/)
* [Legal Nodes, Global AI Regulations Tracker: Europe, Americas & Asia-Pacific Overview](https://legalnodes.com/article/global-ai-regulations-tracker)
* [MIT AI Risk Repository](https://airisk.mit.edu/)
* [multistate.ai](https://www.multistate.ai/)
* [National AI policies & strategies](https://oecd.ai/en/dashboards/overview) | OECD.AI
* [National Conference of State Legislatures, Deceptive Audio or Visual Media ‘Deepfakes’ 2024 Legislation](https://www.ncsl.org/technology-and-communication/deceptive-audio-or-visual-media-deepfakes-2024-legislation)
* [Regulatory Mapping on Artificial Intelligence in Latin America: Regional AI Public Policy Report](https://www.accessnow.org/wp-content/uploads/2024/07/TRF-LAC-Reporte-Regional-IA-JUN-2024-V3.pdf) | Access Now
* [The Ethical AI Database](https://www.eaidb.org/)
* [Tracking international legislation relevant to AI at work](https://www.ifow.org/publications/legislation-tracker) | Institute for the Future of Work

### Challenges and Competitions

This section contains challenges and competitions related to responsible ML.

* [FICO Explainable Machine Learning Challenge](https://community.fico.com/s/explainable-machine-learning-challenge)
* [OSD Bias Bounty](https://osdbiasbounty.com/)
* [National Fair Housing Alliance Hackathon](https://nationalfairhousing.org/hackathon2023/)
* [Twitter Algorithmic Bias](https://hackerone.com/twitter-algorithmic-bias?type=team)

### Critiques of AI

This section contains an assortment of papers, articles, essays, and general resources that take critical stances toward generative AI.

* [Against predictive optimization](https://predictive-optimization.cs.princeton.edu/)
* [AI as Normal Technology](https://knightcolumbia.org/content/ai-as-normal-technology) | Arvind Narayanan and Sayash Kapoor, April 15, 2025
* [AI Bias is Not Ideological. It's Science.](https://www.techpolicy.press/ai-bias-is-not-ideological-its-science/)
* [AI can only do 5% of jobs, says MIT economist who fears tech stock crash](https://torontosun.com/business/money-news/ai-can-only-do-5-of-jobs-says-mit-economist-who-fears-tech-stock-crash)
* [AI chatbots use racist stereotypes even after anti-racism training](https://www.newscientist.com/article/2421067-ai-chatbots-use-racist-stereotypes-even-after-anti-racism-training/)
* [AI coding assistants do not boost productivity or prevent burnout, study finds](https://www.techspot.com/news/104945-ai-coding-assistants-do-not-boost-productivity-or.html)
* [AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business](https://link.springer.com/article/10.1007/s43681-024-00443-4)
* [AI hype, promotional culture, and affective capitalism](https://link.springer.com/article/10.1007/s43681-024-00483-w)
* [AI Is a Lot of Work](https://nymag.com/intelligencer/article/ai-artificial-intelligence-humans-technology-business-factory.html)
* [AI is effectively ‘useless’—and it’s created a ‘fake it till you make it’ bubble that could end in disaster, veteran market watcher warns](https://finance.yahoo.com/news/ai-effectively-useless-created-fake-194008129.html)
* [AI Safety Is a Narrative Problem](https://hdsr.mitpress.mit.edu/pub/wz35dvpo/release/1?readingCollection=3974b7e6)
* [AI Snake Oil](https://www.aisnakeoil.com/)
* [AI Tools Still Permitting Political Disinfo Creation, NGO Warns](https://www.barrons.com/news/ai-tools-still-permitting-political-disinfo-creation-ngo-warns-ac791521)
* [Anthropomorphism in AI: hype and fallacy](https://link.springer.com/article/10.1007/s43681-024-00419-4)
* [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/pdf/2304.15004.pdf)
* [Are Language Models Actually Useful for Time Series Forecasting?](https://arxiv.org/abs/2406.16964v1)
* [Artificial Hallucinations in ChatGPT: Implications in Scientific Writing](https://assets.cureus.com/uploads/editorial/pdf/138667/20230219-28928-6kcyip.pdf)
* [Artificial Hype](https://egve.hu/downloads/health_management/health_management_2019_2_szam.pdf) | HealthManagement.org, The Journal, Volume 19, Issue 2, 2019
* [Artificial intelligence and illusions of understanding in scientific research](https://rdcu.be/dAw4I)
* [Artificial intelligence-powered chatbots in search engines: a cross-sectional study on the quality and risks of drug information for patients](https://qualitysafety.bmj.com/content/early/2024/09/18/bmjqs-2024-017476)
* [Artificial Intelligence: Hope for Future or Hype by Intellectuals?](https://ieeexplore.ieee.org/abstract/document/9596410)
* [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/pdf/2402.11753.pdf)
* [Authoritarian by Design: AI, Big Tech, and the Architecture of Control](https://thegoodtechproject.addpotion.com/authoritarian-by-design-ai-big-tech-and-the-architecture-of-control)
* [Aylin Caliskan's publications](https://faculty.washington.edu/aylin/publications.html)
* [Beyond Metrics: A Critical Analysis of the Variability in Large Language Model Evaluation Frameworks](https://arxiv.org/abs/2407.21072)
* [Beyond Preferences in AI Alignment](https://arxiv.org/pdf/2408.16984)
* [Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics](https://arxiv.org/abs/2411.08881)
* [Chatbots in consumer finance](https://www.consumerfinance.gov/data-research/research-reports/chatbots-in-consumer-finance/chatbots-in-consumer-finance/)
* [ChatGPT is bullshit](https://link.springer.com/article/10.1007/s10676-024-09775-5)
* [Companies like Google and OpenAI are pillaging the internet and pretending it’s progress](https://bgr.com/business/companies-like-google-and-openai-are-pillaging-the-internet-and-pretending-its-progress/)
* [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness](https://arxiv.org/abs/2308.08708)
* [Data and its discontents: A survey of dataset development and use in machine learning research](https://tinyurl.com/2rx43mrz)
* [Does current AI represent a dead end?](https://www.bcs.org/articles-opinion-and-research/does-current-ai-represent-a-dead-end/) | BCS
* [Ed Zitron's Where's Your Ed At](https://www.wheresyoured.at/)
* [Emergent and Predictable Memorization in Large Language Models](https://arxiv.org/abs/2304.11158)
* [Evaluating Language-Model Agents on Realistic Autonomous Tasks](https://arxiv.org/pdf/2312.11671.pdf)
* [Explainable AI: The What’s and Why’s, Part 1: The What](https://ryanallen42.medium.com/explainable-ai-the-whats-and-why-s-175ea344bf3a) | Ryan Allen
* [FABLES: Evaluating faithfulness and content selection in book-length summarization](https://arxiv.org/abs/2404.01261)
* [Futurism, Disillusioned Businesses Discovering That AI Kind of Sucks](https://futurism.com/the-byte/businesses-discovering-ai-sucks)
* [Gen AI: Too Much Spend, Too Little Benefit?](https://www.goldmansachs.com/intelligence/pages/gs-research/gen-ai-too-much-spend-too-little-benefit/report.pdf)
* [Generative AI: UNESCO study reveals alarming evidence of regressive gender stereotypes](https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes)
* [Get Ready for the Great AI Disappointment](https://www.wired.com/story/get-ready-for-the-great-ai-disappointment/)
* [Ghost in the Cloud: Transhumanism’s simulation theology](https://www.nplusonemag.com/issue-28/essays/ghost-in-the-cloud/)
* [Handling the hype: Implications of AI hype for public interest tech projects](https://www.tatup.de/index.php/tatup/article/view/7080)
* [How AI hype impacts the LGBTQ + community](https://link.springer.com/article/10.1007/s43681-024-00423-8)
* [How AI lies, cheats, and grovels to succeed - and what we need to do about it](https://www.zdnet.com/article/how-ai-lies-cheats-and-grovels-to-succeed-and-what-we-need-to-do-about-it/)
* [How to Tell if Something is AI-Written](https://hollisrobbinsanecdotal.substack.com/p/how-to-tell-if-something-is-ai-written) | Anecdotal Value, Hollis Robbins, August 12, 2025
* [I Will Fucking Piledrive You If You Mention AI Again](https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/)
* [Identifying and Eliminating CSAM in Generative ML Training Data and Models](https://stacks.stanford.edu/file/druid:kh752sm9123/ml_training_data_csam_report-2023-12-23.pdf)
* [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf)
* [Insanely Complicated, Hopelessly Inadequate](https://www.lrb.co.uk/the-paper/v43/n02/paul-taylor/insanely-complicated-hopelessly-inadequate)
* [Internet of Bugs, Debunking Devin: "First AI Software Engineer" Upwork lie exposed!](https://www.youtube.com/watch?v=tNmgmwEtoWE)
* [It’s Time to Stop Taking Sam Altman at His Word](https://www.theatlantic.com/technology/archive/2024/10/sam-altman-mythmaking/680152/)
* [Large Language Models are Unreliable for Cyber Threat Intelligence](https://arxiv.org/pdf/2503.23175)
* [Large Language Models Do Not Simulate Human Psychology](https://arxiv.org/pdf/2508.06950)
* [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models](https://arxiv.org/abs/2401.01301)
* [Lazy use of AI leads to Amazon products called “I cannot fulfill that request”](https://arstechnica.com/ai/2024/01/lazy-use-of-ai-leads-to-amazon-products-called-i-cannot-fulfill-that-request/)
* [Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs](https://arxiv.org/pdf/2402.03927.pdf)
* [LLMs Can’t Plan, But Can Help Planning in LLM-Modulo Frameworks](https://arxiv.org/pdf/2402.01817.pdf)
* [Long-context LLMs Struggle with Long In-context Learning](https://huggingface.co/papers/2404.02060)
* [Low-Resource Languages Jailbreak GPT-4](https://arxiv.org/abs/2310.02446v1)
* [Machine Learning: The High Interest Credit Card of Technical Debt](https://research.google/pubs/machine-learning-the-high-interest-credit-card-of-technical-debt/)
* [Measuring the predictability of life outcomes with a scientific mass collaboration](https://www.pnas.org/doi/10.1073/pnas.1915006117)
* [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1)
* [Meta AI Chief: Large Language Models Won't Achieve AGI](https://www.msn.com/en-us/news/technology/meta-ai-chief-large-language-models-won-t-achieve-agi/ar-BB1mRPa5)
* [Meta’s AI chief: LLMs will never reach human-level intelligence](https://thenextweb.com/news/meta-yann-lecun-ai-behind-human-intelligence)
* [MIT Technology Review, Introducing: The AI Hype Index](https://www.technologyreview.com/2024/10/23/1105192/ai-hype-index-nov-dec-2024/)
* [Most CEOs aren’t buying the hype on generative AI benefits](https://www.itpro.com/business/leadership/most-ceos-arent-buying-the-hype-on-generative-ai-benefits)
* [The Most Dangerous Fiction: The Rhetoric and Reality of the AI Race](https://dx.doi.org/10.2139/ssrn.5278644)
* [Nepotistically Trained Generative-AI Models Collapse](https://arxiv.org/abs/2311.12202)
* [Non-discrimination Criteria for Generative Language Models](https://arxiv.org/abs/2403.08564)
* [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
* [On the Very Real Dangers of the Artificial Intelligence Hype Machine](https://lithub.com/on-the-very-real-dangers-of-the-artificial-intelligence-hype-machine/)
* [OpenAI—written evidence, House of Lords Communications and Digital Select Committee inquiry: Large language models](https://committees.parliament.uk/writtenevidence/126981/pdf/) | (LLM0113)
  * [Former OpenAI Researcher Says the Company Broke Copyright Law](https://www.nytimes.com/2024/10/23/technology/openai-copyright-law.html)
* [Open Problems in Technical AI Governance](https://arxiv.org/pdf/2407.14981)
* [Pivot to AI](https://pivot-to-ai.com/)
* [Press Pause on the Silicon Valley Hype Machine](https://www.nytimes.com/2024/05/15/opinion/artificial-intelligence-ai-openai-chatgpt-overrated-hype.html) | Julia Angwin
* [Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://arxiv.org/pdf/2311.00871.pdf)
* [Promising the future, encoding the past: AI hype and public media imagery](https://link.springer.com/article/10.1007/s43681-024-00474-x)
* [Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/pdf/2503.21934v1) | arXiv, March 2025
* [Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646)
* [Re-evaluating GPT-4’s bar exam performance](https://link.springer.com/article/10.1007/s10506-024-09396-9)
* [Researchers surprised by gender stereotypes in ChatGPT](https://www.dtu.dk/english/news/all-news/researchers-surprised-by-gender-stereotypes-in-chatgpt?id=7e5936d1-dfce-485b-8a90-78f7c757177d)
* [Sam Altman’s imperial reach](https://www.washingtonpost.com/opinions/2024/10/07/sam-altman-ai-power-danger/)
* [Scalable Extraction of Training Data from Production Language Models](https://arxiv.org/pdf/2311.17035.pdf)
* [Speed of AI development stretches risk assessments to breaking point](https://www.ft.com/content/499c8935-f46e-4ec8-a8e2-19e07e3b0438)
* [Talking existential risk into being: a Habermasian critical discourse perspective to AI hype](https://link.springer.com/article/10.1007/s43681-024-00464-z)
* [Task Contamination: Language Models May Not Be Few-Shot Anymore](https://arxiv.org/pdf/2312.16337.pdf)
* [The Cult of AI](https://www.rollingstone.com/culture/culture-features/ai-companies-advocates-cult-1234954528/)
* [The Data Scientific Method vs. The Scientific Method](https://odsc.com/blog/the-data-scientific-method-vs-the-scientific-method/)
* [The Fallacy of AI Functionality](https://dl.acm.org/doi/pdf/10.1145/3531146.3533158)
* [The harms of terminology: why we should reject so-called “frontier AI”](https://link.springer.com/article/10.1007/s43681-024-00438-1)
* [The perpetual motion machine of AI-generated data and the distraction of ChatGPT as a ‘scientist’](https://www.nature.com/articles/s41587-023-02103-0)
* [The Price of Emotion: Privacy, Manipulation, and Bias in Emotional AI](https://www.americanbar.org/groups/business_law/resources/business-law-today/2024-september/price-emotion-privacy-manipulation-bias-emotional-ai/)
* [Theory Is All You Need: AI, Human Cognition, and Decision Making](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4737265)
* [There Is No A.I.](https://www.newyorker.com/science/annals-of-artificial-intelligence/there-is-no-ai)
* [There’s Nothing Magical in the Machine](https://www.nytimes.com/2025/09/25/opinion/artificial-intelligence-magical-thinking.html)
* [This AI Pioneer Thinks AI Is Dumber Than a Cat](https://www.wsj.com/tech/ai/yann-lecun-ai-meta-aa59e2f5)
* [Three different types of AI hype in healthcare](https://link.springer.com/article/10.1007/s43681-024-00465-y)
* [Toward Sociotechnical AI: Mapping Vulnerabilities for Machine Learning in Context](https://link.springer.com/article/10.1007/s11023-024-09668-y)
* [We still don't know what generative AI is good for](https://www.msn.com/en-us/news/technology/we-still-dont-know-what-generative-ai-is-good-for/ar-AA1nz1QH)
* [What’s in a Name? Experimental Evidence of Gender Bias in Recommendation Letters Generated by ChatGPT](https://www.jmir.org/2024/1/e51837/)
* [Which Humans?](https://osf.io/preprints/psyarxiv/5b26t)
* [Why the AI Hype is Another Tech Bubble](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4960826)
* [Why We Must Resist AI’s Soft Mind Control]( https://www.theatlantic.com/ideas/archive/2024/03/artificial-intelligence-google-gemini-mind-control/677683/)
* [Winner's Curse? On Pace, Progress, and Empirical Rigor](https://openreview.net/pdf?id=rJWF0Fywf)
* [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/pdf/2506.08872v1)
* [YouTube's Anorexia Algorithm: How YouTube Recommends Eating Disorders Videos to Young Girls](https://counterhate.com/wp-content/uploads/2024/12/CCDH.YoutubeED.Nov24.Report_FINAL.pdf) | Center for Countering Digital Hate (CCDH),

#### Environmental Costs of AI

* [A bottle of water per email: the hidden environmental costs of using AI chatbots](https://www.washingtonpost.com/technology/2024/09/18/energy-ai-use-electricity-water-data-centers/)
* [AI already uses as much energy as a small country. It’s only the beginning.](https://www.vox.com/climate/2024/3/28/24111721/ai-uses-a-lot-of-energy-experts-expect-it-to-double-in-just-a-few-years)
* [AI, Climate, and Regulation: From Data Centers to the AI Act](https://arxiv.org/abs/2410.06681)
* [Artificial Intelligence and Environmental Impact: Moving Beyond Humanizing Vocabulary and Anthropocentrism](https://www.liebertpub.com/doi/abs/10.1089/omi.2024.0197)
* [Beyond AI as an environmental pharmakon: Principles for reopening the problem-space of machine learning's carbon footprint](https://doi.org/10.1177/25148486251332087)
* [Beyond CO2 Emissions: The Overlooked Impact of Water Consumption of Information Retrieval Models](https://dl.acm.org/doi/abs/10.1145/3578337.3605121)
* [The Climate and Sustainability Implications of Generative AI](https://mit-genai.pubpub.org/pub/8ulgrckc/release/2)
* [Data centre water consumption](https://www.nature.com/articles/s41545-021-00101-w)
* [Ecological footprints, carbon emissions, and energy transitions: the impact of artificial intelligence ](https://www.nature.com/articles/s41599-024-03520-5.pdf)
* [Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI](https://arxiv.org/abs/2309.02065)
* [Ensuring a carbon-neutral future for artificial intelligence](https://www.the-innovation.org/data/article/energy/preview/pdf/XINNENERGY-2024-0095.pdf)
* [Environment and sustainability development: A ChatGPT perspective](https://www.taylorfrancis.com/chapters/oa-edit/10.1201/9781003471059-8/environment-sustainability-development-chatgpt-perspective-priyanka-bhaskar-neha-seth)
* [Generative AI’s environmental costs are soaring — and mostly secret](https://www.nature.com/articles/d41586-024-00478-x)
* [Green Intelligence Resource Hub](https://docs.google.com/spreadsheets/d/1UCsgAqgonjpP9uPVyssXU0VE0G6Fs7ydxt_mmrpcd1o/edit?usp=sharing)
* [Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models](https://arxiv.org/abs/2304.03271)
* [Measuring the Environmental Impact of Delivering AI at Google Scale](https://arxiv.org/pdf/2508.15734)
* [Measuring the environmental impacts of artificial intelligence compute and applications](https://www.oecd.org/en/publications/measuring-the-environmental-impacts-of-artificial-intelligence-compute-and-applications_7babf571-en.html) | OECD
* [Microsoft’s Hypocrisy on AI](https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/)
* [Power Hungry Processing: Watts Driving the Cost of AI Deployment?](https://dl.acm.org/doi/pdf/10.1145/3630106.3658542)
* [Powering artificial intelligence: A study of AI's environmental footprint—today and tomorrow, November 2024](https://www.deloitte.com/content/dam/assets-shared/docs/about/2024/powering-artificial-intelligence.pdf) | Deloitte
* [Promoting Sustainability: Mitigating the Water Footprint in AI-Embedded Data Centres](https://www.igi-global.com/chapter/promoting-sustainability/341617)
* [Sustainable AI: AI for sustainability and the sustainability of AI](https://link.springer.com/article/10.1007/s43681-021-00043-6)
* [Sustainable AI: Environmental Implications, Challenges and Opportunities](https://proceedings.mlsys.org/paper_files/paper/2022/file/462211f67c7d858f663355eff93b745e-Paper.pdf)
* [The AI Carbon Footprint and Responsibilities of AI Scientists](https://www.mdpi.com/2409-9287/7/1/4)
* [The Carbon Footprint of Artificial Intelligence](https://dl.acm.org/doi/pdf/10.1145/3603746)
* [The carbon impact of artificial intelligence](https://www.nature.com/articles/s42256-020-0219-9)
* [The Environmental Impact of AI: A Case Study of Water Consumption by Chat GPT](https://puiij.com/index.php/research/article/view/39)
* [The Environmental Price of Intelligence: Evaluating the Social Cost of Carbon in Machine Learning](https://ieeexplore.ieee.org/abstract/document/10553496)
* [The growing energy footprint of artificial intelligence](https://www.cell.com/action/showPdf?pii=S2542-4351%2823%2900365-3)
* [The Hidden Cost of AI: Carbon Footprint and Mitigation Strategies](https://dx.doi.org/10.2139/ssrn.5036344)
* [The Hidden Cost of AI: Unraveling the Power-Hungry Nature of Large Language Models](https://www.preprints.org/frontend/manuscript/30dc8badac9e44da699113e5b5cd6737/download_pub)
* [The Hidden Costs of AI-driven Data Center Demand: Five Systemic Tensions](https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1039&context=amcis2025)
* [The Hidden Environmental Impact of AI](https://jacobin.com/2024/06/ai-data-center-energy-usage-environment/)
* [The mechanisms of AI hype and its planetary and social costs](https://link.springer.com/article/10.1007/s43681-024-00461-2)
* [Toward Responsible AI Use: Considerations for Sustainability Impact Assessment](https://arxiv.org/abs/2312.11996)
* [Towards A Comprehensive Assessment of AI's Environmental Impact](https://arxiv.org/abs/2405.14004)
* [Towards Environmentally Equitable AI via Geographical Load Balancing](https://arxiv.org/abs/2307.05494)
* [Towards green and sustainable artificial intelligence: quantifying the energy footprint of logistic regression and decision tree algorithms](https://ieeexplore.ieee.org/abstract/document/10700922)
* [Tracking the carbon footprint of global generative artificial intelligence](https://www.cell.com/action/showPdf?pii=S2666-6758%2825%2900069-4)
* [Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions](https://www.mdpi.com/2071-1050/14/9/5172)
* [We did the math on AI's energy footprint. Here's the story you haven't heard.](https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/) | MIT Technology Review, May 20, 2025

#### Language Diversity and Resource Gaps

* [Health Care Misinformation: An artificial intelligence challenge for low-resource languages](https://ceur-ws.org/Vol-2884/paper_131.pdf)
* [The Serendipity of Claude AI: Case of the 13 Low-Resource National Languages of Mali](https://arxiv.org/pdf/2503.03380)

#### AI Slop Genre

* [AI Slop Might Finally Cure Our Internet Addiction](https://www.theatlantic.com/technology/archive/2025/07/ai-slop-internet-addiction/683619/)
* [Living the Slop Life](https://www.nytimes.com/2025/05/19/style/ai-slop-slop-bowls-shein-slop-hauls.html) | The New York Times, Emma Goldberg, 5/19/2025

#### Measurement Critiques

* [The Leaderboard Illusion](https://arxiv.org/pdf/2504.20879)

### Groups and Organizations

* [Aapti Institute](https://aapti.in/)
* [Ada Lovelace Institute](https://www.adalovelaceinstitute.org) 
* [AI & Faith](https://aiandfaith.org)
* [AI Ethics Lab](https://aiethicslab.com)
* [AI for Good Foundation](https://ai4good.org)
* [AI Forum New Zealand, AI Governance Working Group](https://aiforum.org.nz/our-work/working-groups/ai-governance-working-group/)
* [AI Hub for Sustainable Development](https://www.aihubfordevelopment.org/)
* [AI Now Institute](https://ainowinstitute.org)
* [AI Policy Exchange](https://aipolicyexchange.org/)
* [AI Transparency Institute](https://aitransparencyinstitute.com/)
* [AI Village](https://aivillage.org/)
* [The Alan Turing Institute](https://www.turing.ac.uk/)
* [Algorithmic Justice League](https://www.ajl.org/)
* [Berkman Klein Center for Internet & Society at Harvard University](https://cyber.harvard.edu/)
* [Center for Advancing Safety of Machine Intelligence](https://casmi.northwestern.edu/)
* [Center for AI and Digital Policy](https://www.caidp.org)
* [Center for Democracy and Technology](https://cdt.org/)
* [Center for Humane Technology](https://www.humanetech.com/)
* [Center for Security and Emerging Technology](https://cset.georgetown.edu/)
* [Convergence Analysis](https://www.convergenceanalysis.org/about-us)
* [Data & Society](https://datasociety.net)
* [Distributed AI Research Institute](https://www.dair-institute.org) |  (DAIR)
* [Future of Life Institute](https://futureoflife.org/)
* [Global Center on AI Governance](https://www.globalcenter.ai/)
* [Indigenous Protocol and Artificial Intelligence Working Group](https://www.indigenous-ai.net/)
* [Institute for Advanced Study, AI Policy and Governance Working Group](https://www.ias.edu/stsv-lab/aipolicy)
* [Institute for Ethics and the Common Good, Notre Dame-IBM Technology Ethics Lab](https://ethics.nd.edu/labs-and-centers/notre-dame-ibm-technology-ethics-lab/)
* [Leverhulme Centre for the Future of Intelligence](https://lcfi.ac.uk)
* [Montreal AI Ethics Institute](https://montrealethics.ai)
* [Partnership on AI](https://partnershiponai.org/)
* [Responsible Artificial Intelligence Institute](https://responsible.ai)
* [Stanford University Human-Centered Artificial Intelligence](https://hai.stanford.edu/) |  (HAI)
* [TheGovLab](https://thegovlab.org/)

### Curated Bibliographies

We are seeking curated bibliographies related to responsible ML across various topics, see [issue 115](https://github.com/jphall663/awesome-machine-learning-interpretability/issues/115).

* [Artificial Intelligence Policy Supplementary Reading List](https://www.blairaf.com/library/resources/teaching/2024-INF1005H1S/INF1005-Supplementary-Reading-List.pdf) | Blair Attard-Frost, INF1005H1S
* [Global regulatory tracker - United States](https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-united-states) | White & Case, AI Watch
* [Green Intelligence Resource Hub](https://docs.google.com/spreadsheets/d/1UCsgAqgonjpP9uPVyssXU0VE0G6Fs7ydxt_mmrpcd1o/edit?usp=sharing)
* [LLM Security & Privacy](https://github.com/chawins/llm-sp) | ![](https://img.shields.io/github/stars/chawins/llm-sp?style=social)
* [Responsible Computing](https://www.internetruleslab.com/responsible-computing) | Internet Rules Lab
* [Membership Inference Attacks and Defenses on Machine Learning Models Literature](https://github.com/HongshengHu/membership-inference-machine-learning-literature) | ![](https://img.shields.io/github/stars/HongshengHu/membership-inference-machine-learning-literature?style=social)

* **BibTeX**
  * [A Responsible Machine Learning Workflow](https://github.com/h2oai/article-information-2019/blob/master/back_up/article-information-2019.bib.bak) | ![](https://img.shields.io/github/stars/h2oai/article-information-2019?style=social) | (paper, bibliography)
  * [Proposed Guidelines for Responsible Use of Explainable Machine Learning](https://github.com/jphall663/kdd_2019/blob/master/bibliography.bib) | ![](https://img.shields.io/github/stars/jphall663/kdd_2019?style=social) |  (presentation, bibliography)
  * [Proposed Guidelines for Responsible Use of Explainable Machine Learning](https://github.com/jphall663/responsible_xai/blob/master/responsible_xai.bib) | ![](https://img.shields.io/github/stars/jphall663/responsible_xai?style=social) | (paper, bibliography)

* **Web**
  * [Fairness, Accountability, and Transparency in Machine Learning Scholarship](https://www.fatml.org/resources/relevant-scholarship) | (FAT/ML)

### List of Lists

This section links to other lists of responsible ML or related resources.

* [2024 AI Resources](https://docs.google.com/document/d/1M--GEa5G4pxMHG5FMeUZKbMtIwAtrWJsBtWZTVGVIqI/edit?tab=t.0) | Chris Kraft
* [A Living and Curated Collection of Explainable AI Methods](https://utwente-dmb.github.io/xai-papers/#/)
* [A review of 200 guidelines and recommendations for AI governance](https://doi.org/10.1016/j.patter.2023.100857) | Worldwide AI ethics
* [AI Ethics & Policy News spreadsheet](https://docs.google.com/spreadsheets/d/11Ps8ILDHH-vojJGyIx7CcaoB5l1mBRHy3OQAgWkm0W4/edit#gid=0) | Casey Fiesler
* [AI Ethics Guidelines Global Inventory](https://algorithmwatch.org/en/project/ai-ethics-guidelines-global-inventory/)
* [AI Ethics Resources](https://www.fast.ai/posts/2018-09-24-ai-ethics-resources.html)
* [AI Guidance Resources](https://wde.instructure.com/courses/826) | Wyoming Department of Education (WDE)
* [AI Tools and Platforms](https://docs.google.com/spreadsheets/u/2/d/10pPQYmyNnYb6zshOKxBjJ704E0XUj2vJ9HCDfoZxAoA/htmlview#)
* [Awesome AI Guidelines](https://github.com/EthicalML/awesome-artificial-intelligence-guidelines) | ![](https://img.shields.io/github/stars/EthicalML/awesome-artificial-intelligence-guidelines?style=social)
* [Awesome interpretable machine learning](https://github.com/lopusz/awesome-interpretable-machine-learning) | ![](https://img.shields.io/github/stars/lopusz/awesome-interpretable-machine-learning?style=social)
* [Awesome MLOps](https://github.com/visenger/awesome-mlops) | ![](https://img.shields.io/github/stars/visenger/awesome-mlops?style=social)
* [Awesome Production Machine Learning](https://github.com/EthicalML/awesome-machine-learning-operations) | ![](https://img.shields.io/github/stars/EthicalML/awesome-machine-learning-operations?style=social)
* [Awesome Responsible AI](https://github.com/AthenaCore/AwesomeResponsibleAI) | ![](https://img.shields.io/github/stars/AthenaCore/AwesomeResponsibleAI?style=social) | AthenaCore
* [Awesome-explainable-AI](https://github.com/wangyongjie-ntu/Awesome-explainable-AI/) | ![](https://img.shields.io/github/stars/wangyongjie-ntu/Awesome-explainable-AI?style=social)
* [Awesome-ML-Model-Governance](https://github.com/visenger/Awesome-ML-Model-Governance) | ![](https://img.shields.io/github/stars/visenger/Awesome-ML-Model-Governance?style=social)
* [awesomelistsio/Awesome AI Ethics](https://github.com/awesomelistsio/awesome-ai-ethics) | ![](https://img.shields.io/github/stars/awesomelistsio/awesome-ai-ethics?style=social)
* [Awful AI](https://github.com/daviddao/awful-ai) | ![](https://img.shields.io/github/stars/daviddao/awful-ai?style=social)
* [Comments Received for RFI on Artificial Intelligence Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework/comments-received-rfi-artificial-intelligence-risk-management) | IMDA-BTG
* [criticalML](https://github.com/rockita/criticalML) | ![](https://img.shields.io/github/stars/rockita/criticalML?style=social)
* [Ethics for people who work in tech](https://ethicsforpeoplewhoworkintech.com/)
* [Evaluation Repository for 'Sociotechnical Safety Evaluation of Generative AI Systems'](https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vQObeTxvXtOs--zd98qG2xBHHuTTJOyNISBJPthZFr3at2LCrs3rcv73d4of1A78JV2eLuxECFXJY43/pubhtml)
* [Inventory of U.S. Department of Education AI Use Cases](https://www.ed.gov/about/ed-overview/artificial-intelligence-ai-guidance)
* [GET Program for AI Ethics and Governance Standards](https://ieeexplore.ieee.org/browse/standards/get-program/page/series?id=93) | IEEE GET Program
* [Global Digital Policy Roundup March 2025](https://www.techpolicy.press/global-digital-policy-roundup-march-2025/)
* [LLM-Evals-Catalogue](https://github.com/IMDA-BTG/LLM-Evals-Catalogue) | ![](https://img.shields.io/github/stars/IMDA-BTG/LLM-Evals-Catalogue?style=social) | IMDA-BTG
* [Machine Learning Ethics References](https://github.com/radames/Machine-Learning-Ethics-References) | ![](https://img.shields.io/github/stars/radames/Machine-Learning-Ethics-References?style=social)
* [Machine Learning Interpretability Resources](https://github.com/h2oai/mli-resources) | ![](https://img.shields.io/github/stars/h2oai/mli-resources?style=social)
* [MIT AI Agent Index](https://aiagentindex.mit.edu/)
* [OECD-NIST Catalogue of AI Tools and Metrics](https://oecd.ai/en/catalogue/overview)
* [OpenAI Cookbook](https://github.com/openai/openai-cookbook/tree/main) | ![](https://img.shields.io/github/stars/openai/openai-cookbook?style=social)
* [private-ai-resources](https://github.com/OpenMined/private-ai-resources) | ![](https://img.shields.io/github/stars/OpenMined/private-ai-resources?style=social)
* [Ravit Dotan's Resources](https://www.techbetter.ai/resources)
* [ResponsibleAI](https://romanlutz.github.io/ResponsibleAI/)
* [Tech & Ethics Curricula](https://docs.google.com/spreadsheets/d/1Z0DqQeZ-Aeq6LmD17J5m8zeeIR6ywWnH-WO-jWtXE9M/edit#gid=0)
* [Ultraopxt/Awesome AI Ethics & Safety](https://github.com/Ultraopxt/Awesome-AI-Ethics-Safety) | ![](https://img.shields.io/github/stars/Ultraopxt/Awesome-AI-Ethics-Safety/?style=social)
* [XAI Resources](https://github.com/pbiecek/xai_resources) | ![](https://img.shields.io/github/stars/pbiecek/xai_resources?style=social)
* [xaience](https://github.com/andreysharapov/xaience) | ![](https://img.shields.io/github/stars/andreysharapov/xaience?style=social)

## Technical Resources

### Benchmarks

This section contains benchmarks or datasets used for benchmarks for ML systems, particularly those related to responsible ML desiderata.

| Resource | Description |
| --- | --- |
| [benchm-ml](https://github.com/szilard/benchm-ml)-![](https://img.shields.io/github/stars/szilard/benchm-ml?style=social) | "A minimal benchmark for scalability, speed and accuracy of commonly used open source implementations (R packages, Python scikit-learn, H2O, xgboost, Spark MLlib etc.) of the top machine learning algorithms for binary classification (random forests, gradient boosted trees, deep neural networks etc.)." |
| [Bias Benchmark for QA dataset-BBQ](https://github.com/nyu-mll/bbq)-![](https://img.shields.io/github/stars/nyu-mll/bbq?style=social) | "Repository for the Bias Benchmark for QA dataset." |
| [Cataloguing LLM Evaluations](https://github.com/IMDA-BTG/LLM-Evals-Catalogue)-![](https://img.shields.io/github/stars/IMDA-BTG/LLM-Evals-Catalogue?style=social) | "This repository stems from our paper, 'Cataloguing LLM Evaluations,' and serves as a living, collaborative catalogue of LLM evaluation frameworks, benchmarks and papers." |
| [DecodingTrust](https://github.com/AI-secure/DecodingTrust)-![](https://img.shields.io/github/stars/huggingface/evaluate?style=social) | "A Comprehensive Assessment of Trustworthiness in GPT Models." |
| [EleutherAI, Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)-![](https://img.shields.io/github/stars/EleutherAI/lm-evaluation-harness?style=social) | "A framework for few-shot evaluation of language models." |
| [Evidently AI 100+ LLM benchmarks and evaluation datasets](https://www.evidentlyai.com/llm-evaluation-benchmarks-datasets) | "A database of LLM benchmarks and datasets to evaluate the performance of language models." |
| [GEM](https://gem-benchmark.com/) | "GEM is a benchmark environment for Natural Language Generation with a focus on its Evaluation, both through human annotations and automated Metrics." |
| [HELM](https://crfm.stanford.edu/helm/latest/) | "A holistic framework for evaluating foundation models." |
| [Hugging Face, evaluate](https://github.com/huggingface/evaluate)-![](https://img.shields.io/github/stars/huggingface/evaluate?style=social) | "Evaluate: A library for easily evaluating machine learning models and datasets." |
| [i-gallegos, Fair-LLM-Benchmark](https://github.com/i-gallegos/Fair-LLM-Benchmark)-![](https://img.shields.io/github/stars/i-gallegos/Fair-LLM-Benchmark?style=social) | Benchmark from "Bias and Fairness in Large Language Models: A Survey" |
| [jphall663, Generative AI Risk Management Resources](https://github.com/jphall663/gai_risk_management)-![](https://img.shields.io/github/stars/jphall663/gai_risk_management?style=social) | "A place for ideas and drafts related to GAI risk management." |
| [MLCommons, AI Luminate: A collaborative, transparent approach to safer AI](https://mlcommons.org/ailuminate/) | "The AILuminate v1.1 benchmark suite is the first AI risk assessment benchmark developed with broad involvement from leading AI companies, academia, and civil society." |
| [MLCommons, Introducing v0.5 of the AI Safety Benchmark from MLCommons](https://arxiv.org/pdf/2404.12241.pdf) | A paper about the MLCommons AI Safety Benchmark v0.5. |
| [MLCommons, MLCommons AI Safety v0.5 Proof of Concept](https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/) | "The MLCommons AI Safety Benchmark aims to assess the safety of AI systems in order to guide development, inform purchasers and consumers, and support standards bodies and policymakers." |
| [ML.ENERGY Leaderboard](https://ml.energy/leaderboard/?__theme=light) | "Large language models (LLMs), especially the instruction-tuned ones, can generate human-like responses to chat prompts. Using Zeus for energy measurement, we created a leaderboard for LLM chat energy consumption." |
| [ModelSlant.com](https://modelslant.com/#tab=topics) | "How politically slanted are Large Language Models?" |
| [Nvidia MLPerf](https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/) | "MLPerf™ benchmarks—developed by MLCommons, a consortium of AI leaders from academia, research labs, and industry—are designed to provide unbiased evaluations of training and inference performance for hardware, software, and services." |
| [OpenML Benchmarking Suites](https://www.openml.org/search?type=benchmark&study_type=task) | OpenML's collection of over two dozen benchmarking suites. |
| [Real Toxicity Prompts - Allen Institute for AI](https://allenai.org/data/real-toxicity-prompts) | "A dataset of 100k sentence snippets from the web for researchers to further address the risk of neural toxic degeneration in models." |
| [SafetyPrompts.com](https://safetyprompts.com/) | "A Living Catalogue of Open Datasets for LLM Safety." |
| [Sociotechnical Safety Evaluation Repository](https://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vQObeTxvXtOs--zd98qG2xBHHuTTJOyNISBJPthZFr3at2LCrs3rcv73d4of1A78JV2eLuxECFXJY43/pubhtml) | An extensive spreadsheet of sociotechnical safety evaluations in a spreadsheet. |
| [Trust-LLM-Benchmark Leaderboard](https://trustllmbenchmark.github.io/TrustLLM-Website/leaderboard.html) | A series of sortable leaderboards of LLMs based on different trustworthiness criteria. |
| [TrustLLM-Benchmark](https://trustllmbenchmark.github.io/TrustLLM-Website/index.html) | "A Comprehensive Study of Trustworthiness in Large Language Models." |
| [TruthfulQA](https://github.com/sylinrl/TruthfulQA)-![](https://img.shields.io/github/stars/sylinrl/TruthfulQA?style=social) | "TruthfulQA: Measuring How Models Imitate Human Falsehoods." |
| [WAVES: Benchmarking the Robustness of Image Watermarks](https://wavesbench.github.io/) | "This paper investigates the weaknesses of image watermarking techniques." |
| [Wild-Time: A Benchmark of in-the-Wild Distribution Shifts over Time](https://github.com/huaxiuyao/Wild-Time)-![](https://img.shields.io/github/stars/huaxiuyao/Wild-Time?style=social) | "Benchmark for Natural Temporal Distribution Shift (NeurIPS 2022)." |
| [Winogender Schemas](https://github.com/rudinger/winogender-schemas)-![](https://img.shields.io/github/stars/rudinger/winogender-schemas?style=social) | "Data for evaluating gender bias in coreference resolution systems." |
| [yandex-research - tabred](https://github.com/yandex-research/tabred)-![](https://img.shields.io/github/stars/yandex-research/tabred?style=social) | "A Benchmark of Tabular Machine Learning in-the-Wild with real-world industry-grade tabular datasets." |

### Common or Useful Datasets

This section contains datasets that are commonly used in responsible ML evaulations or repositories of interesting/important data sources.

* [A dataset on EU legislation for the digital world](https://www.bruegel.org/dataset/dataset-eu-legislation-digital-world) | Bruegel
* [Adult income dataset](https://www.kaggle.com/datasets/wenruliu/adult-income-dataset)
* [Balanced Faces in the Wild](https://github.com/visionjo/facerec-bias-bfw) | ![](https://img.shields.io/github/stars/visionjo/facerec-bias-bfw?style=social)
* [COMPAS Recidivism Risk Score Data and Analysis](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)
  * **Data Repositories**
    * [All Lending Club loan data](https://www.kaggle.com/datasets/wordsforthewise/lending-club)
    * [Amazon Open Data](https://registry.opendata.aws/amazon-reviews/)
    * [Data.gov](https://data.gov/)
    * [Home Mortgage Disclosure Act Data](https://www.consumerfinance.gov/data-research/hmda/)
    * [MIMIC-III Clinical Database](https://physionet.org/content/mimiciii/1.4/)
    * [UCI ML Data Repository](https://archive.ics.uci.edu/)
* [FANNIE MAE Single Family Loan Performance](https://capitalmarkets.fanniemae.com/credit-risk-transfer/single-family-credit-risk-transfer/fannie-mae-single-family-loan-performance-data)
* [German Credit Data](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | Statlog
* [Have I Been Trained?](https://haveibeentrained.com/)
* [nikhgarg / EmbeddingDynamicStereotypes](https://github.com/nikhgarg/EmbeddingDynamicStereotypes) | ![](https://img.shields.io/github/stars/nikhgarg/EmbeddingDynamicStereotypes?style=social)
* [NYPD Stop, Question and Frisk Data](https://www.nyc.gov/site/nypd/stats/reports-analysis/stopfrisk.page)
* [Presidential Deepfakes Dataset](https://www.media.mit.edu/publications/presidential-deepfakes-dataset/)
* [socialfoundations / folktables](https://github.com/socialfoundations/folktables) | ![](https://img.shields.io/github/stars/socialfoundations/folktables?style=social)
* [Wikipedia Talk Labels: Personal Attacks](https://www.kaggle.com/datasets/jigsaw-team/wikipedia-talk-labels-personal-attacks)

### Domain-specific Software

This section curates specialized software tools aimed at responsible ML within specific domains, such as in healthcare, finance, or social sciences.

### Machine Learning Environment Management Tools

This section contains open source or open access ML environment management software.

| Resource | Description |
|----------|-------|
| [dvc](https://dvc.org/) | "Manage and version images, audio, video, and text files in storage and organize your ML modeling process into a reproducible workflow." |
| [gigantum](https://github.com/gigantum)-![gigantum stars](https://img.shields.io/github/stars/gigantum?style=social) | "Building a better way to create, collaborate, and share data-driven science." |
| [mlflow](https://mlflow.org/) | "An open source platform for the machine learning lifecycle." |
| [mlmd](https://github.com/google/ml-metadata)-![mlmd stars](https://img.shields.io/github/stars/google/ml-metadata?style=social) | "For recording and retrieving metadata associated with ML developer and data scientist workflows." |
| [modeldb](https://github.com/VertaAI/modeldb)-![modeldb stars](https://img.shields.io/github/stars/VertaAI/modeldb?style=social) | "Open Source ML Model Versioning, Metadata, and Experiment Management." |
| [neptune](https://neptune.ai/researchers) | "A single place to manage all your model metadata." |
| [Opik](https://github.com/comet-ml/opik)-![](https://img.shields.io/github/stars/comet-ml/opik?style=social) |  "Evaluate, test, and ship LLM applications across your dev and production lifecycles." |

### Personal Data Protection Tools

This section contains tools for personal data protection.

| Name | Description |
|------|-------------|
| [LLM Dataset Inference: Did you train on my dataset?](https://github.com/pratyushmaini/llm_dataset_inference/)-![](https://img.shields.io/github/stars/pratyushmaini/llm_dataset_inference?style=social) | "Official Repository for Dataset Inference for LLMs" |

### Open Source/Access Responsible AI Software Packages

This section contains open source or open access software used to implement responsible ML. As much as possible, descriptions are quoted verbatim from the respective repositories themselves. In rare instances, we provide our own descriptions (unmarked by quotes).

#### Browser

| Name | Description |
|------|-------------|
| [DiscriLens](https://github.com/wangqianwen0418/DiscriLens)-![](https://img.shields.io/github/stars/wangqianwen0418/DiscriLens?style=social) | "Discrimination in Machine Learning." |
| [Hugging Face, BiasAware: Dataset Bias Detection](https://huggingface.co/spaces/avid-ml/biasaware) | "BiasAware is a specialized tool for detecting and quantifying biases within datasets used for Natural Language Processing (NLP) tasks." |
| [manifold](https://github.com/uber/manifold)-![](https://img.shields.io/github/stars/uber/manifold?style=social) | "A model-agnostic visual debugging tool for machine learning." |
| [PAIR-code - datacardsplaybook](https://github.com/PAIR-code/datacardsplaybook)-![](https://img.shields.io/github/stars/PAIR-code/datacardsplaybook?style=social) | "The Data Cards Playbook helps dataset producers and publishers adopt a people-centered approach to transparency in dataset documentation." |
| [PAIR-code - facets](https://github.com/PAIR-code/facets)-![](https://img.shields.io/github/stars/PAIR-code/facets?style=social) | "Visualizations for machine learning datasets." |
| [PAIR-code - knowyourdata](https://github.com/pair-code/knowyourdata)-![](https://img.shields.io/github/stars/PAIR-code/knowyourdata?style=social) | "A tool to help researchers and product teams understand datasets with the goal of improving data quality, and mitigating fairness and bias issues." |
| [TensorBoard Projector](http://projector.tensorflow.org) | "Using the TensorBoard Embedding Projector, you can graphically represent high dimensional embeddings. This can be helpful in visualizing, examining, and understanding your embedding layers." |
| [What-if Tool](https://pair-code.github.io/what-if-tool/index.html#about) | "Visually probe the behavior of trained machine learning models, with minimal coding." |

#### C/C++

| Name | Description |
|------|-------------|
| [Born-again Tree Ensembles](https://github.com/vidalt/BA-Trees)-![](https://img.shields.io/github/stars/vidalt/BA-Trees?style=social) | "Born-Again Tree Ensembles: Transforms a random forest into a single, minimal-size, tree with exactly the same prediction function in the entire feature space (ICML 2020)." |)
| [Certifiably Optimal RulE ListS](https://github.com/nlarusstone/corels)-![](https://img.shields.io/github/stars/nlarusstone/corels?style=social) | "CORELS is a custom discrete optimization technique for building rule lists over a categorical feature space." |
| [Secure-ML](https://github.com/shreya-28/Secure-ML)-![](https://img.shields.io/github/stars/shreya-28/Secure-ML?style=social) | "Secure Linear Regression in the Semi-Honest Two-Party Setting." |

#### JavaScript

| Name | Description |
|------|-------------|
| [LDNOOBW](https://github.com/LDNOOBW)-![](https://img.shields.io/github/stars/LDNOOBW?style=social) | "List of Dirty, Naughty, Obscene, and Otherwise Bad Words" |

#### Python

| Name | Description |
|------|-------------|
| [acd](https://github.com/csinva/hierarchical_dnn_interpretations)-![](https://img.shields.io/github/stars/csinva/hierarchical_dnn_interpretations?style=social) | "Produces hierarchical interpretations for a single prediction made by a pytorch neural network. Official code for *Hierarchical interpretations for neural network predictions*.” |
| [aequitas](https://github.com/dssg/aequitas)-![](https://img.shields.io/github/stars/dssg/aequitas?style=social) | "Aequitas is an open-source bias audit toolkit for data scientists, machine learning researchers, and policymakers to audit machine learning models for discrimination and bias, and to make informed and equitable decisions around developing and deploying predictive tools.” |
| [AI Explainability 360](https://github.com/IBM/AIX360)-![](https://img.shields.io/github/stars/IBM/AIX360?style=social) | "Interpretability and explainability of data and machine learning models.” |
| [AI Fairness 360](https://github.com/Trusted-AI/AIF360)-![](https://img.shields.io/github/stars/Trusted-AI/AIF360?style=social) | "A comprehensive set of fairness metrics for datasets and machine learning models, explanations for these metrics, and algorithms to mitigate bias in datasets and models.” |
| [ALEPython](https://github.com/blent-ai/ALEPython)-![](https://img.shields.io/github/stars/blent-ai/ALEPython?style=social) | "Python Accumulated Local Effects package.” |
| [Aletheia](https://github.com/SelfExplainML/Aletheia)-![](https://img.shields.io/github/stars/SelfExplainML/Aletheia?style=social) | "A Python package for unwrapping ReLU DNNs.” |
| [algofairness](https://github.com/algofairness)-![](https://img.shields.io/github/stars/algofairness?style=social) | See [Algorithmic Fairness](http://fairness.haverford.edu/). |
| [Alibi](https://github.com/SeldonIO/alibi)-![](https://img.shields.io/github/stars/SeldonIO/alibi?style=social) | "Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models.” |
| [allennlp](https://github.com/allenai/allennlp)-![](https://img.shields.io/github/stars/allenai/allennlp?style=social) | "An open-source NLP research library, built on PyTorch.” |
| [anchor](https://github.com/marcotcr/anchor)-![](https://img.shields.io/github/stars/marcotcr/anchor?style=social) | "Code for 'High-Precision Model-Agnostic Explanations' paper.” |
| [Bayesian Case Model](https://users.cs.duke.edu/~cynthia/code/BCM.zip)
| [Bayesian Ors-Of-Ands](https://github.com/wangtongada/BOA)-![](https://img.shields.io/github/stars/wangtongada/BOA?style=social) | "This code implements the Bayesian or-of-and algorithm as described in the BOA paper. We include the tictactoe dataset in the correct formatting to be used by this code.” |
| [Bayesian Rule List - BRL](https://users.cs.duke.edu/~cynthia/code/BRL_supplement_code.zip) | Rudin group at Duke Bayesian case model implementation |
| [BlackBoxAuditing](https://github.com/algofairness/BlackBoxAuditing)-![](https://img.shields.io/github/stars/algofairness/BlackBoxAuditing?style=social) | "Research code for auditing and exploring black box machine-learning models.” |
| [CalculatedContent, WeightWatcher](https://github.com/calculatedcontent/weightwatcher)-![](https://img.shields.io/github/stars/calculatedcontent/weightwatcher?style=social) | "The WeightWatcher tool for predicting the accuracy of Deep Neural Networks." |
| [captum](https://github.com/pytorch/captum)-![](https://img.shields.io/github/stars/pytorch/captum?style=social) | "Model interpretability and understanding for PyTorch.” |
| [casme](https://github.com/kondiz/casme)-![](https://img.shields.io/github/stars/kondiz/casme?style=social) | "contains the code originally forked from the ImageNet training in PyTorch that is modified to present the performance of classifier-agnostic saliency map extraction, a practical algorithm to train a classifier-agnostic saliency mapping by simultaneously training a classifier and a saliency mapping.” |
| [Causal Discovery Toolbox](https://github.com/FenTechSolutions/CausalDiscoveryToolbox)-![](https://img.shields.io/github/stars/FenTechSolutions/CausalDiscoveryToolbox?style=social) | "Package for causal inference in graphs and in the pairwise settings. Tools for graph structure recovery and dependencies are included.” |
| [causalml](https://github.com/uber/causalml)-![](https://img.shields.io/github/stars/uber/causalml?style=social) | "Uplift modeling and causal inference with machine learning algorithms.” |
| [cdt15, Causal Discovery Lab., Shiga University](https://github.com/cdt15)-![](https://img.shields.io/github/stars/cdt15?style=social) | "LiNGAM is a new method for estimating structural equation models or linear causal Bayesian networks. It is based on using the non-Gaussianity of the data." |
| [checklist](https://github.com/marcotcr/checklist)-![](https://img.shields.io/github/stars/marcotcr/checklist?style=social) | "Beyond Accuracy: Behavioral Testing of NLP models with CheckList.” |
| [cleverhans](https://github.com/cleverhans-lab/cleverhans)-![](https://img.shields.io/github/stars/cleverhans-lab/cleverhans?style=social) | "An adversarial example library for constructing attacks, building defenses, and benchmarking both.” |
| [contextual-AI](https://github.com/SAP/contextual-ai)-![](https://img.shields.io/github/stars/SAP/contextual-ai?style=social) | "Contextual AI adds explainability to different stages of machine learning pipelines | data, training, and inference | thereby addressing the trust gap between such ML systems and their users. It does not refer to a specific algorithm or ML method — instead, it takes a human-centric view and approach to AI.” |
| [ContrastiveExplanation - Foil Trees](https://github.com/MarcelRobeer/ContrastiveExplanation)-![](https://img.shields.io/github/stars/MarcelRobeer/ContrastiveExplanation?style=social) | "provides an explanation for why an instance had the current outcome (fact) rather than a targeted outcome of interest (foil). These counterfactual explanations limit the explanation to the features relevant in distinguishing fact from foil, thereby disregarding irrelevant features.” |
| [counterfit](https://github.com/Azure/counterfit/)-![](https://img.shields.io/github/stars/Azure/counterfit?style=social) | "a CLI that provides a generic automation layer for assessing the security of ML models.” |
| [dalex](https://github.com/ModelOriented/DALEX)-![](https://img.shields.io/github/stars/ModelOriented/DALEX?style=social) | "moDel Agnostic Language for Exploration and eXplanation.” |
| [debiaswe](https://github.com/tolga-b/debiaswe)-![](https://img.shields.io/github/stars/tolga-b/debiaswe?style=social) | "Remove problematic gender bias from word embeddings.” |
| [DeepExplain](https://github.com/marcoancona/DeepExplain)-![](https://img.shields.io/github/stars/marcoancona/DeepExplain?style=social) | "provides a unified framework for state-of-the-art gradient and perturbation-based attribution methods. It can be used by researchers and practitioners for better undertanding the recommended existing models, as well for benchmarking other attribution methods.” |
| [DeepLIFT](https://github.com/kundajelab/deeplift)-![](https://img.shields.io/github/stars/kundajelab/deeplift?style=social) | "This repository implements the methods in 'Learning Important Features Through Propagating Activation Differences' by Shrikumar, Greenside & Kundaje, as well as other commonly-used methods such as gradients, gradient-times-input (equivalent to a version of Layerwise Relevance Propagation for ReLU networks), guided backprop and integrated gradients.” |
| [deepvis](https://github.com/yosinski/deep-visualization-toolbox)-![](https://img.shields.io/github/stars/yosinski/deep-visualization-toolbox?style=social) | "the code required to run the Deep Visualization Toolbox, as well as to generate the neuron-by-neuron visualizations using regularized optimization.” |
| [DIANNA](https://github.com/dianna-ai/dianna)-![](https://img.shields.io/github/stars/dianna-ai/dianna?style=social) | "DIANNA is a Python package that brings explainable AI (XAI) to your research project. It wraps carefully selected XAI methods in a simple, uniform interface. It's built by, with and for (academic) researchers and research software engineers working on machine learning projects.” |
| [DiCE](https://github.com/interpretml/DiCE)-![](https://img.shields.io/github/stars/interpretml/DiCE?style=social) | "Generate Diverse Counterfactual Explanations for any machine learning model.” |
| [DoWhy](https://github.com/microsoft/dowhy)-![](https://img.shields.io/github/stars/microsoft/dowhy?style=social) | "DoWhy is a Python library for causal inference that supports explicit modeling and testing of causal assumptions. DoWhy is based on a unified language for causal inference, combining causal graphical models and potential outcomes frameworks.” |
| [dtreeviz](https://github.com/parrt/dtreeviz)-![](https://img.shields.io/github/stars/parrt/dtreeviz?style=social) | "A python library for decision tree visualization and model interpretation.” |
| [ecco](https://github.com/jalammar/ecco)-![](https://img.shields.io/github/stars/jalammar/ecco?style=social) | "Explain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).” |
| [effector](https://github.com/givasile/effector)-![](https://img.shields.io/github/stars/givasile/effector?style=social) | "eXplainable AI for Tabular Data" |
| [eli5](https://github.com/TeamHG-Memex/eli5)-![](https://img.shields.io/github/stars/TeamHG-Memex/eli5?style=social) | "A library for debugging/inspecting machine learning classifiers and explaining their predictions.” |
| [explabox](https://github.com/MarcelRobeer/explabox)-![](https://img.shields.io/github/stars/MarcelRobeer/explabox?style=social) | "aims to support data scientists and machine learning (ML) engineers in explaining, testing and documenting AI/ML models, developed in-house or acquired externally. The explabox turns your ingestibles (AI/ML model and/or dataset) into digestibles (statistics, explanations or sensitivity insights).” |
| [Explainable Boosting Machine EBM/GA2M](https://github.com/interpretml/interpret)-![](https://img.shields.io/github/stars/interpretml/interpret?style=social) | "an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model's global behavior, or understand the reasons behind individual predictions.” |
| [ExplainaBoard](https://github.com/neulab/ExplainaBoard)-![](https://img.shields.io/github/stars/neulab/ExplainaBoard?style=social) | "a tool that inspects your system outputs, identifies what is working and what is not working, and helps inspire you with ideas of where to go next.” |
| [explainerdashboard](https://github.com/oegedijk/explainerdashboard)-![](https://img.shields.io/github/stars/oegedijk/explainerdashboard?style=social) | "Quickly build Explainable AI dashboards that show the inner workings of so-called "blackbox" machine learning models.” |
| [explainX](https://github.com/explainX/explainx)-![](https://img.shields.io/github/stars/explainX/explainx?style=social) | "Explainable AI framework for data scientists. Explain & debug any blackbox machine learning model with a single line of code.” |
| [fair-classification](https://github.com/mbilalzafar/fair-classification)-![](https://img.shields.io/github/stars/mbilalzafar/fair-classification?style=social) | "Python code for training fair logistic regression classifiers.” |
| [fairlearn](https://github.com/fairlearn/fairlearn)-![](https://img.shields.io/github/stars/fairlearn/fairlearn?style=social) | "a Python package that empowers developers of artificial intelligence (AI) systems to assess their system's fairness and mitigate any observed unfairness issues. Fairlearn contains mitigation algorithms as well as metrics for model assessment. Besides the source code, this repository also contains Jupyter notebooks with examples of Fairlearn usage.” |
| [fairml](https://github.com/adebayoj/fairml)-![](https://img.shields.io/github/stars/adebayoj/fairml?style=social) | "a python toolbox auditing the machine learning models for bias.” |
| [fairness_measures_code](https://github.com/megantosh/fairness_measures_code)-![](https://img.shields.io/github/stars/megantosh/fairness_measures_code?style=social) | "contains implementations of measures used to quantify discrimination.” |
| [fairness-comparison](https://github.com/algofairness/fairness-comparison)-![](https://img.shields.io/github/stars/algofairness/fairness-comparison?style=social) | "meant to facilitate the benchmarking of fairness aware machine learning algorithms.” |
| [Falling Rule List - FRL](https://users.cs.duke.edu/~cynthia/code/falling_rule_list.zip) | Rudin group at Duke falling rule list implementation |
| [foolbox](https://github.com/bethgelab/foolbox)-![](https://img.shields.io/github/stars/bethgelab/foolbox?style=social) | "A Python toolbox to create adversarial examples that fool neural networks in PyTorch, TensorFlow, and JAX.” |
| [Giskard](https://github.com/Giskard-AI/giskard)-![](https://img.shields.io/github/stars/Giskard-AI/giskard?style=social) | "The testing framework dedicated to ML models, from tabular to LLMs. Scan AI models to detect risks of biases, performance issues and errors. In 4 lines of code.” |
| [gplearn](https://github.com/trevorstephens/gplearn)-![](https://img.shields.io/github/stars/trevorstephens/gplearn?style=social) | "implements Genetic Programming in Python, with a scikit-learn inspired and compatible API.” |
| [Grad-CAM](https://github.com/topics/grad-cam)-(GitHub topic) | Grad-CAM is a technique for making convolutional neural networks more transparent by visualizing the regions of input that are important for predictions in computer vision models. |
| [H2O-3 Monotonic GBM](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2ogradientboostingestimator) | "Builds gradient boosted classification trees and gradient boosted regression trees on a parsed data set." |
| [H2O-3 Penalized Generalized Linear Models](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2ogeneralizedlinearestimator) | "Fits a generalized linear model, specified by a response variable, a set of predictors, and a description of the error distribution." |
| [H2O-3 Sparse Principal Components](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2ogeneralizedlowrankestimator) | "Builds a generalized low rank decomposition of an H2O data frame." |
| [h2o-LLM-eval](https://github.com/h2oai/h2o-LLM-eval)-![](https://img.shields.io/github/stars/h2oai/h2o-LLM-eval?style=social) | "Large-language Model Evaluation framework with Elo Leaderboard and A-B testing." |
| [hate-functional-tests](https://github.com/paul-rottger/hate-functional-tests)-![](https://img.shields.io/github/stars/paul-rottger/hate-functional-tests?style=social) | HateCheck: A dataset and test suite from an ACL 2021 paper, offering functional tests for hate speech detection models, including extensive case annotations and testing functionalities. |
| [imodels](https://github.com/csinva/imodels)-![](https://img.shields.io/github/stars/csinva/imodels?style=social) | "Python package for concise, transparent, and accurate predictive modeling. All sklearn-compatible and easy to use.” |
| [iNNvestigate neural nets](https://github.com/albermax/innvestigate)-![](https://img.shields.io/github/stars/albermax/innvestigate?style=social) | A comprehensive Python library to analyze and interpret neural network behaviors in Keras, featuring a variety of methods like Gradient, LRP, and Deep Taylor. |
| [Integrated-Gradients](https://github.com/ankurtaly/Integrated-Gradients)-![](https://img.shields.io/github/stars/ankurtaly/Integrated-Gradients?style=social) | "a variation on computing the gradient of the prediction output w.r.t. features of the input. It requires no modification to the original network, is simple to implement, and is applicable to a variety of deep models (sparse and dense, text and vision).” |
| [interpret_with_rules](https://github.com/clips/interpret_with_rules)-![](https://img.shields.io/github/stars/clips/interpret_with_rules?style=social) | "induces rules to explain the predictions of a trained neural network, and optionally also to explain the patterns that the model captures from the training data, and the patterns that are present in the original dataset.” |
| [interpret](https://github.com/interpretml/interpret)-![](https://img.shields.io/github/stars/interpretml/interpret?style=social) | "an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof.” |
| [InterpretME](https://github.com/SDM-TIB/InterpretME)-![](https://img.shields.io/github/stars/SDM-TIB/InterpretME?style=social) | "integrates knowledge graphs (KG) with machine learning methods to generate interesting meaningful insights. It helps to generate human- and machine-readable decisions to provide assistance to users and enhance efficiency.” |
| [keract](https://github.com/philipperemy/keract/)-![](https://img.shields.io/github/stars/philipperemy/keract?style=social) | Keract is a tool for visualizing activations and gradients in Keras models; it's meant to support a wide range of Tensorflow versions and to offer an intuitive API with Python examples. |
| [Keras-vis](https://github.com/raghakot/keras-vis)-![](https://img.shields.io/github/stars/raghakot/keras-vis?style=social) | "a high-level toolkit for visualizing and debugging your trained keras neural net models.” |
| [L2X](https://github.com/Jianbo-Lab/L2X)-![](https://img.shields.io/github/stars/Jianbo-Lab/L2X?style=social) | "Code for replicating the experiments in the paper [Learning to Explain: An Information-Theoretic Perspective on Model Interpretation](https://arxiv.org/pdf/1802.07814.pdf) at ICML 2018, by Jianbo Chen, Mitchell Stern, Martin J. Wainwright, Michael I. Jordan.” |
| [LangFair](https://github.com/cvs-health/langfair)-![](https://img.shields.io/github/stars/cvs-health/langfair?style=social) | "LangFair is a Python library for conducting use-case level LLM bias and fairness assessments"
| [langtest](https://github.com/JohnSnowLabs/langtest)-![](https://img.shields.io/github/stars/JohnSnowLabs/langtest?style=social) | "LangTest: Deliver Safe & Effective Language Models" |
| [learning-fair-representations](https://github.com/zjelveh/learning-fair-representations)-![](https://img.shields.io/github/stars/zjelveh/learning-fair-representations?style=social) | "Python numba implementation of Zemel et al. 2013 <http://www.cs.toronto.edu/~toni/Papers/icml-final.pdf>" |
| [leeky: Leakage/contamination testing for black box language models](https://github.com/mjbommar/leeky)-![](https://img.shields.io/github/stars/mjbommar/leeky?style=social) | "leeky - training data contamination techniques for blackbox models" |
| [leondz / garak, LLM vulnerability scanner](https://github.com/leondz/garak)-![](https://img.shields.io/github/stars/leondz/garak?style=social) | "LLM vulnerability scanner" |
| [LiFT](https://github.com/linkedin/LiFT)-![](https://img.shields.io/github/stars/linkedin/LiFT?style=social) | "The LinkedIn Fairness Toolkit (LiFT) is a Scala/Spark library that enables the measurement of fairness and the mitigation of bias in large-scale machine learning workflows. The measurement module includes measuring biases in training data, evaluating fairness metrics for ML models, and detecting statistically significant differences in their performance across different subgroups.” |
| [lilac](https://github.com/lilacai/lilac)-![](https://img.shields.io/github/stars/lilacai/lilac?style=social) | "Curate better data for LLMs." |
| [lime](https://github.com/marcotcr/lime)-![](https://img.shields.io/github/stars/marcotcr/lime?style=social) | "explaining what machine learning classifiers (or models) are doing. At the moment, we support explaining individual predictions for text classifiers or classifiers that act on tables (numpy arrays of numerical or categorical data) or images, with a package called lime (short for local interpretable model-agnostic explanations).” |
| [lit](https://github.com/pair-code/lit)-![](https://img.shields.io/github/stars/pair-code/lit?style=social) | "The Learning Interpretability Tool (LIT, formerly known as the Language Interpretability Tool) is a visual, interactive ML model-understanding tool that supports text, image, and tabular data. It can be run as a standalone server, or inside of notebook environments such as Colab, Jupyter, and Google Cloud Vertex AI notebooks.” |
| [LLM Dataset Inference: Did you train on my dataset?](https://github.com/pratyushmaini/llm_dataset_inference/)-![](https://img.shields.io/github/stars/pratyushmaini/llm_dataset_inference?style=social) | "Official Repository for Dataset Inference for LLMs" |
| [lofo-importance](https://github.com/aerdem4/lofo-importance)-![](https://img.shields.io/github/stars/aerdem4/lofo-importance?style=social) | "LOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric.” |
| [lrp_toolbox](https://github.com/sebastian-lapuschkin/lrp_toolbox)-![](https://img.shields.io/github/stars/sebastian-lapuschkin/lrp_toolbox?style=social) | "The Layer-wise Relevance Propagation (LRP) algorithm explains a classifer's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself.” |
| [MindsDB](https://github.com/mindsdb/mindsdb)-![](https://img.shields.io/github/stars/mindsdb/mindsdb?style=social) | "enables developers to build AI tools that need access to real-time data to perform their tasks.” |
| [ml_privacy_meter](https://github.com/privacytrustlab/ml_privacy_meter)-![](https://img.shields.io/github/stars/privacytrustlab/ml_privacy_meter?style=social) | "an open-source library to audit data privacy in statistical and machine learning algorithms. The tool can help in the data protection impact assessment process by providing a quantitative analysis of the fundamental privacy risks of a (machine learning) model.” |
| [ml-fairness-gym](https://github.com/google/ml-fairness-gym)-![](https://img.shields.io/github/stars/google/ml-fairness-gym?style=social) | "a set of components for building simple simulations that explore the potential long-run impacts of deploying machine learning-based decision systems in social environments.” |
| [MLextend](http://rasbt.github.io/mlxtend/) | "Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.” |
| [mllp](https://github.com/12wang3/mllp)-![](https://img.shields.io/github/stars/12wang3/mllp?style=social) | "This is a PyTorch implementation of Multilayer Logical Perceptrons (MLLP) and Random Binarization (RB) method to learn Concept Rule Sets (CRS) for transparent classification tasks, as described in our paper: [Transparent Classification with Multilayer Logical Perceptrons and Random Binarization](https://arxiv.org/abs/1912.04695).” |
| [Monotonic Constraints](http://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html) | Guide on implementing and understanding monotonic constraints in XGBoost models to enhance predictive performance with practical Python examples. |
| [Multilayer Logical Perceptron - MLLP](https://github.com/12wang3/mllp)-![](https://img.shields.io/github/stars/12wang3/mllp?style=social) | "This is a PyTorch implementation of Multilayer Logical Perceptrons (MLLP) and Random Binarization (RB) method to learn Concept Rule Sets (CRS) for transparent classification tasks, as described in our paper: [Transparent Classification with Multilayer Logical Perceptrons and Random Binarization](https://arxiv.org/abs/1912.04695).” |
| [OptBinning](https://github.com/guillermo-navas-palencia/optbinning)-![](https://img.shields.io/github/stars/guillermo-navas-palencia/optbinning?style=social) | "a library written in Python implementing a rigorous and flexible mathematical programming formulation to solve the optimal binning problem for a binary, continuous and multiclass target type, incorporating constraints not previously addressed.” |
| [Optimal Sparse Decision Trees](https://github.com/xiyanghu/OSDT)-![](https://img.shields.io/github/stars/xiyanghu/OSDT?style=social) | "This accompanies the paper, ["Optimal Sparse Decision Trees"](https://arxiv.org/abs/1904.12847) by Xiyang Hu, Cynthia Rudin, and Margo Seltzer.” |
| [parity-fairness](https://pypi.org/project/parity-fairness/) | "This repository contains codes that demonstrate the use of fairness metrics, bias mitigations and explainability tool.” |
| [PDPbox](https://github.com/SauceCat/PDPbox)-![](https://img.shields.io/github/stars/SauceCat/PDPbox?style=social) | "Python Partial Dependence Plot toolbox. Visualize the influence of certain features on model predictions for supervised machine learning algorithms, utilizing partial dependence plots.” |
| [PiML-Toolbox](https://github.com/SelfExplainML/PiML-Toolbox)-![](https://img.shields.io/github/stars/SelfExplainML/PiML-Toolbox?style=social) | "a new Python toolbox for interpretable machine learning model development and validation. Through low-code interface and high-code APIs, PiML supports a growing list of inherently interpretable ML models.” |
| [pjsaelin / Cubist](https://github.com/pjaselin/Cubist?tab=readme-ov-file)-![](https://img.shields.io/github/stars/pjaselin/Cubist?style=social) | "A Python package for fitting Quinlan's Cubist regression model" |
| [Privacy-Preserving-ML](https://github.com/abhinav-bohra/Privacy-Preserving-ML)-![](https://img.shields.io/github/stars/abhinav-bohra/Privacy-Preserving-ML?style=social) | "Implementation of privacy-preserving SVM assuming public model private data scenario (data in encrypted but model parameters are unencrypted) using adequate partial homomorphic encryption.” |
| [ProtoPNet](https://github.com/cfchen-duke/)-![](https://img.shields.io/github/stars/cfchen-duke?style=social) | "This code package implements the prototypical part network (ProtoPNet) from the paper "This Looks Like That: Deep Learning for Interpretable Image Recognition" (to appear at NeurIPS 2019), by Chaofan Chen (Duke University), Oscar Li| (Duke University), Chaofan Tao (Duke University), Alina Jade Barnett (Duke University), Jonathan Su (MIT Lincoln Laboratory), and Cynthia Rudin (Duke University).” |
| [pyBreakDown](https://github.com/MI2DataLab/pyBreakDown)-![](https://img.shields.io/github/stars/MI2DataLab/pyBreakDown?style=social) | See [dalex](https://dalex.drwhy.ai/). |
| [PyCEbox](https://github.com/AustinRochford/PyCEbox)-![](https://img.shields.io/github/stars/AustinRochford/PyCEbox?style=social) | "Python Individual Conditional Expectation Plot Toolbox.” |
| [pyGAM](https://github.com/dswah/pyGAM)-![](https://img.shields.io/github/stars/dswah/pyGAM?style=social) | "Generalized Additive Models in Python.” |
| [pymc3](https://github.com/pymc-devs/pymc3)-![](https://img.shields.io/github/stars/pymc-devs/pymc3?style=social) | "PyMC (formerly PyMC3) is a Python package for Bayesian statistical modeling focusing on advanced Markov chain Monte Carlo (MCMC) and variational inference (VI) algorithms. Its flexibility and extensibility make it applicable to a large suite of problems.” |
| [pySS3](https://github.com/sergioburdisso/pyss3)-![](https://img.shields.io/github/stars/sergioburdisso/pyss3?style=social) | "The SS3 text classifier is a novel and simple supervised machine learning model for text classification which is interpretable, that is, it has the ability to naturally (self)explain its rationale.” |
| [pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam)-![](https://img.shields.io/github/stars/jacobgil/pytorch-grad-cam?style=social) | "a package with state of the art methods for Explainable AI for computer vision. This can be used for diagnosing model predictions, either in production or while developing models. The aim is also to serve as a benchmark of algorithms and metrics for research of new explainability methods.” |
| [pytorch-innvestigate](https://github.com/fgxaos/pytorch-innvestigate)-![](https://img.shields.io/github/stars/fgxaos/pytorch-innvestigate?style=social) | "PyTorch implementation of Keras already existing project: [https://github.com/albermax/innvestigate/](https://github.com/albermax/innvestigate/).” |
| [Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus)-![](https://img.shields.io/github/stars/understandable-machine-intelligence-lab/Quantus?style=social) | "Quantus is an eXplainable AI toolkit for responsible evaluation of neural network explanations." |
| [rationale](https://github.com/taolei87/rcnn/tree/master/code/rationale)-![](https://img.shields.io/github/stars/taolei87/rcnn?style=social) | "This directory contains the code and resources of the following paper: *"Rationalizing Neural Predictions". Tao Lei, Regina Barzilay and Tommi Jaakkola. EMNLP 2016. [PDF](https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf) [Slides](https://people.csail.mit.edu/taolei/papers/emnlp16_rationale_slides.pdf)*. The method learns to provide justifications, i.e. rationales, as supporting evidence of neural networks' prediction.” |
| [responsibly](https://github.com/ResponsiblyAI/responsibly)-![](https://img.shields.io/github/stars/ResponsiblyAI/responsibly?style=social) | "Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems.” |
| [REVISE: REvealing VIsual biaSEs](https://github.com/princetonvisualai/revise-tool)-![](https://img.shields.io/github/stars/princetonvisualai/revise-tool?style=social) | "A tool that automatically detects possible forms of bias in a visual dataset along the axes of object-based, attribute-based, and geography-based patterns, and from which next steps for mitigation are suggested.” |
| [RISE](https://github.com/eclique/RISE)-![](https://img.shields.io/github/stars/eclique/RISE?style=social) | "contains source code necessary to reproduce some of the main results in the paper: [Vitali Petsiuk](http://cs-people.bu.edu/vpetsiuk/), [Abir Das](http://cs-people.bu.edu/dasabir/), [Kate Saenko](http://ai.bu.edu/ksaenko.html) (BMVC, 2018) [and] [RISE: Randomized Input Sampling for Explanation of Black-box Models](https://arxiv.org/abs/1806.07421).” |
| [Risk-SLIM](https://github.com/ustunb/risk-SLIM)-![](https://img.shields.io/github/stars/ustunb/risk-SLIM?style=social) | "a machine learning method to fit simple customized risk scores in python.” |
| [robustness](https://github.com/MadryLab/robustness)-![](https://img.shields.io/github/stars/MadryLab/robustness?style=social) | "a package we (students in the [MadryLab](http://madry-lab.ml/)) created to make training, evaluating, and exploring neural networks flexible and easy.” |
| [SAGE](https://github.com/iancovert/sage/)-![](https://img.shields.io/github/stars/iancovert/sage?style=social) | "SAGE (Shapley Additive Global importancE) is a game-theoretic approach for understanding black-box machine learning models. It quantifies each feature's importance based on how much predictive power it contributes, and it accounts for complex feature interactions using the Shapley value.” |
| [SALib](https://github.com/SALib/SALib)-![](https://img.shields.io/github/stars/SALib/SALib?style=social) | "Python implementations of commonly used sensitivity analysis methods. Useful in systems modeling to calculate the effects of model inputs or exogenous factors on outputs of interest.” |
| [Scikit-Explain](https://scikit-explain.readthedocs.io/en/latest/index.html) | "User-friendly Python module for machine learning explainability," featuring PD and ALE plots, LIME, SHAP, permutation importance and Friedman's H, among other methods. |
| [scikit-fairness](https://github.com/koaning/scikit-fairness)-![](https://img.shields.io/github/stars/koaning/scikit-fairness?style=social) | Historical link. Merged with [fairlearn](https://fairlearn.org/). |
| [Scikit-learn](https://scikit-learn.org/stable/) [Decision Trees](http://scikit-learn.org/stable/modules/tree.html) | "a non-parametric supervised learning method used for classification and regression.” |
| [Scikit-learn](https://scikit-learn.org/stable/) [Generalized Linear Models](http://scikit-learn.org/stable/modules/linear_model.html) | "a set of methods intended for regression in which the target value is expected to be a linear combination of the features.” |
| [Scikit-learn](https://scikit-learn.org/stable/) [Sparse Principal Components](http://scikit-learn.org/stable/modules/decomposition.html#sparse-principal-components-analysis-sparsepca-and-minibatchsparsepca) | "a variant of [principal component analysis, PCA], with the goal of extracting the set of sparse components that best reconstruct the data.” |
| [scikit-multiflow](https://scikit-multiflow.github.io/) | "a machine learning package for streaming data in Python.” |
| [shap](https://github.com/slundberg/shap)-![](https://img.shields.io/github/stars/slundberg/shap?style=social) | "a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions"
| [shapley](https://github.com/benedekrozemberczki/shapley)-![](https://img.shields.io/github/stars/benedekrozemberczki/shapley?style=social) | "a Python library for evaluating binary classifiers in a machine learning ensemble.” |
| [sklearn-expertsys](https://github.com/tmadl/sklearn-expertsys)-![](https://img.shields.io/github/stars/tmadl/sklearn-expertsys?style=social) | "a scikit-learn compatible wrapper for the Bayesian Rule List classifier developed by Letham et al., 2015, extended by a minimum description length-based discretizer (Fayyad & Irani, 1993) for continuous data, and by an approach to subsample large datasets for better performance.” |
| [skope-rules](https://github.com/scikit-learn-contrib/skope-rules)-![](https://img.shields.io/github/stars/scikit-learn-contrib/skope-rules?style=social) | "a Python machine learning module built on top of scikit-learn and distributed under the 3-Clause BSD license.” |
| [solas-ai-disparity](https://github.com/SolasAI/solas-ai-disparity)-![](https://img.shields.io/github/stars/SolasAI/solas-ai-disparity?style=social) | "a collection of tools that allows modelers, compliance, and business stakeholders to test outcomes for bias or discrimination using widely accepted fairness metrics.” |
| [Super-sparse Linear Integer models - SLIMs](https://github.com/ustunb/slim-python)-![](https://img.shields.io/github/stars/ustunb/slim-python?style=social) | "a package to learn customized scoring systems for decision-making problems.” |
| [tensorflow/fairness-indicators](https://github.com/tensorflow/fairness-indicators)-![](https://img.shields.io/github/stars/tensorflow/fairness-indicators?style=social) | "designed to support teams in evaluating, improving, and comparing models for fairness concerns in partnership with the broader Tensorflow toolkit.” |
| [tensorflow/lattice](https://github.com/tensorflow/lattice)-![](https://img.shields.io/github/stars/tensorflow/lattice?style=social) | "a library that implements constrained and interpretable lattice based models. It is an implementation of Monotonic Calibrated Interpolated Look-Up Tables in TensorFlow.” |
| [tensorflow/lucid](https://github.com/tensorflow/lucid)-![](https://img.shields.io/github/stars/tensorflow/lucid?style=social) | "a collection of infrastructure and tools for research in neural network interpretability.” |
| [tensorflow/model-analysis](https://github.com/tensorflow/model-analysis)-![](https://img.shields.io/github/stars/tensorflow/model-analysis?style=social) | "a library for evaluating TensorFlow models. It allows users to evaluate their models on large amounts of data in a distributed manner, using the same metrics defined in their trainer. These metrics can be computed over different slices of data and visualized in Jupyter notebooks.” |
| [tensorflow/model-card-toolkit](https://github.com/tensorflow/model-card-toolkit)-![](https://img.shields.io/github/stars/tensorflow/model-card-toolkit?style=social) | "streamlines and automates generation of Model Cards, machine learning documents that provide context and transparency into a model's development and performance. Integrating the MCT into your ML pipeline enables you to share model metadata and metrics with researchers, developers, reporters, and more.” |
| [tensorflow/model-remediation](https://github.com/tensorflow/model-remediation)-![](https://img.shields.io/github/stars/tensorflow/model-remediation?style=social) | "a library that provides solutions for machine learning practitioners working to create and train models in a way that reduces or eliminates user harm resulting from underlying performance biases.” |
| [tensorflow/privacy](https://github.com/tensorflow/privacy)-![](https://img.shields.io/github/stars/tensorflow/privacy?style=social) | "the source code for TensorFlow Privacy, a Python library that includes implementations of TensorFlow optimizers for training machine learning models with differential privacy. The library comes with tutorials and analysis tools for computing the privacy guarantees provided.” |
| [tensorflow/tcav](https://github.com/tensorflow/tcav)-![](https://img.shields.io/github/stars/tensorflow/tcav?style=social) | "Testing with Concept Activation Vectors (TCAV) is a new interpretability method to understand what signals your neural networks models uses for prediction.” |
| [tensorfuzz](https://github.com/brain-research/tensorfuzz)-![](https://img.shields.io/github/stars/brain-research/tensorfuzz?style=social) | "a library for performing coverage guided fuzzing of neural networks.” |
| [TensorWatch](https://github.com/microsoft/tensorwatch)-![](https://img.shields.io/github/stars/microsoft/tensorwatch?style=social) | "a debugging and visualization tool designed for data science, deep learning and reinforcement learning from Microsoft Research. It works in Jupyter Notebook to show real-time visualizations of your machine learning training and perform several other key analysis tasks for your models and data.” |
| [text_explainability](https://text-explainability.readthedocs.io/) | "text_explainability provides a generic architecture from which well-known state-of-the-art explainability approaches for text can be composed.” |
| [text_sensitivity](https://text-sensitivity.readthedocs.io/) | "Uses the generic architecture of text_explainability to also include tests of safety (how safe it the model in production, i.e. types of inputs it can handle), robustness (how generalizable the model is in production, e.g. stability when adding typos, or the effect of adding random unrelated data) and fairness (if equal individuals are treated equally by the model, e.g. subgroup fairness on sex and nationality).” |
| [TextFooler](https://github.com/jind11/TextFooler)-![](https://img.shields.io/github/stars/jind11/TextFooler?style=social) | "A Model for Natural Language Attack on Text Classification and Inference"
| [tf-explain](https://github.com/sicara/tf-explain)-![](https://img.shields.io/github/stars/sicara/tf-explain?style=social) | "Implements interpretability methods as Tensorflow 2.x callbacks to ease neural network's understanding.” |
| [themis-ml](https://github.com/cosmicBboy/themis-ml)-![](https://img.shields.io/github/stars/cosmicBboy/themis-ml?style=social) | "A Python library built on top of pandas and sklearnthat implements fairness-aware machine learning algorithms.” |
| [Themis](https://github.com/LASER-UMASS/Themis)-![](https://img.shields.io/github/stars/LASER-UMASS/Themis?style=social) | "A testing-based approach for measuring discrimination in a software system.” |
| [TorchUncertainty](https://github.com/ENSTA-U2IS/torch-uncertainty)-![](https://img.shields.io/github/stars/ENSTA-U2IS/torch-uncertainty?style=social) | "A package designed to help you leverage uncertainty quantification techniques and make your deep neural networks more reliable.” |
| [treeinterpreter](https://github.com/andosa/treeinterpreter)-![](https://img.shields.io/github/stars/andosa/treeinterpreter?style=social) | "Package for interpreting scikit-learn's decision tree and random forest predictions.” |
| [TRIAGE](https://github.com/seedatnabeel/TRIAGE)-![](https://img.shields.io/github/stars/seedatnabeel/TRIAGE?style=social) | "This repository contains the implementation of TRIAGE, a "Data-Centric AI" framework for data characterization tailored for regression.” |
| [woe](https://github.com/boredbird/woe)-![](https://img.shields.io/github/stars/boredbird/woe?style=social) | "Tools for WoE Transformation mostly used in ScoreCard Model for credit rating.” |
| [xai](https://github.com/EthicalML/xai)-![](https://img.shields.io/github/stars/EthicalML/xai?style=social) | "A Machine Learning library that is designed with AI explainability in its core.” |
| [xdeep](https://github.com/datamllab/xdeep)-![](https://img.shields.io/github/stars/datamllab/xdeep?style=social) | "An open source Python library for Interpretable Machine Learning.” |
| [XGBoost](http://xgboost.readthedocs.io/en/latest/) | "an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.” |
| [xplique](https://github.com/deel-ai/xplique)-![](https://img.shields.io/github/stars/deel-ai/xplique?style=social) | "A Python toolkit dedicated to explainability. The goal of this library is to gather the state of the art of Explainable AI to help you understand your complex neural network models.” |
| [ydata-profiling](https://github.com/ydataai/ydata-profiling)-![](https://img.shields.io/github/stars/ydataai/ydata-profiling?style=social) | "Provide(s) a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution.” |
| [yellowbrick](https://github.com/DistrictDataLabs/yellowbrick)-![](https://img.shields.io/github/stars/DistrictDataLabs/yellowbrick?style=social) | "A suite of visual diagnostic tools called "Visualizers" that extend the scikit-learn API to allow human steering of the model selection process.” |

#### R

| Name | Description |
|------|-------------|
| [ALEPlot](https://cran.r-project.org/web/packages/ALEPlot/index.html) | "Visualizes the main effects of individual predictor variables and their second-order interaction effects in black-box supervised learning models."  |
| [arules](https://cran.r-project.org/web/packages/arules/index.html) | "Provides the infrastructure for representing, manipulating and analyzing transaction data and patterns (frequent itemsets and association rules). Also provides C implementations of the association mining algorithms Apriori and Eclat. Hahsler, Gruen and Hornik (2005)." |
| [Causal SVM](https://github.com/shangtai/githubcausalsvm)-![](https://img.shields.io/github/stars/shangtai/githubcausalsvm?style=social) | "We present a new machine learning approach to estimate whether a treatment has an effect on an individual, in the setting of the classical potential outcomes framework with binary outcomes." |
| [DALEX](https://github.com/ModelOriented/DALEX)-![](https://img.shields.io/github/stars/ModelOriented/DALEX?style=social) | "moDel Agnostic Language for Exploration and eXplanation." |
| [DALEXtra: Extension for 'DALEX' Package](https://cran.r-project.org/web/packages/DALEXtra/index.html) | "Provides wrapper of various machine learning models." |
| [DrWhyAI](https://github.com/ModelOriented/DrWhy)-![](https://img.shields.io/github/stars/ModelOriented/DrWhy?style=social) | "DrWhy is [a] collection of tools for eXplainable AI (XAI). It's based on shared principles and simple grammar for exploration, explanation and visualisation of predictive models." |
| [elasticnet](https://cran.r-project.org/web/packages/elasticnet/index.html) | "Provides functions for fitting the entire solution path of the Elastic-Net and also provides functions for doing sparse PCA." |
| [Explainable Boosting Machine - EBM/GA2M](https://cran.r-project.org/web/packages/interpret/index.html) | "Package for training interpretable machine learning models." |
| [ExplainPrediction](https://github.com/rmarko/ExplainPrediction)-![](https://img.shields.io/github/stars/rmarko/ExplainPrediction?style=social) | "Generates explanations for classification and regression models and visualizes them." |
| [fairmodels](https://github.com/ModelOriented/fairmodels)-![](https://img.shields.io/github/stars/ModelOriented/fairmodels?style=social) | "Flexible tool for bias detection, visualization, and mitigation. Use models explained with DALEX and calculate fairness classification metrics based on confusion matrices using fairness_check() or try newly developed module for regression models using fairness_check_regression()." |
| [fairness](https://cran.r-project.org/web/packages/fairness/index.html) | "Offers calculation, visualization and comparison of algorithmic fairness metrics." |
| [fastshap](https://github.com/bgreenwell/fastshap)-![](https://img.shields.io/github/stars/bgreenwell/fastshap?style=social) | "The goal of fastshap is to provide an efficient and speedy approach (at least relative to other implementations) for computing approximate Shapley values, which help explain the predictions from any machine learning model." |
| [featureImportance](https://github.com/giuseppec/featureImportance)-![](https://img.shields.io/github/stars/giuseppec/featureImportance?style=social) | "An extension for the mlr package and allows to compute the permutation feature importance in a model-agnostic manner." |
| [flashlight](https://github.com/mayer79/flashlight)-![](https://img.shields.io/github/stars/mayer79/flashlight?style=social) | "The goal of this package is [to] shed light on black box machine learning models." |
| [forestmodel](https://cran.r-project.org/web/packages/forestmodel/index.html) | "Produces forest plots using 'ggplot2' from models produced by functions such as stats::lm(), stats::glm() and survival::coxph()." |
| [fscaret](https://cran.r-project.org/web/packages/fscaret/) | "Automated feature selection using variety of models provided by 'caret' package." |
| [gam](https://cran.r-project.org/web/packages/gam/index.html) | "Functions for fitting and working with generalized additive models, as described in chapter 7 of "Statistical Models in S" (Chambers and Hastie (eds), 1991), and "Generalized Additive Models" (Hastie and Tibshirani, 1990)." |
| [glm2](https://cran.r-project.org/web/packages/glm2/) | "Fits generalized linear models using the same model specification as glm in the stats package, but with a modified default fitting method that provides greater stability for models that may fail to converge using glm." |
| [glmnet](https://cran.r-project.org/web/packages/glmnet/index.html) | "Extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression, Cox model, multiple-response Gaussian, and the grouped multinomial regression." |
| [H2O-3 Monotonic GBM](http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.gbm.html) | "Builds gradient boosted classification trees and gradient boosted regression trees on a parsed data set." |
| [H2O-3 Penalized Generalized Linear Models](http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.glm.html) | "Fits a generalized linear model, specified by a response variable, a set of predictors, and a description of the error distribution." |
| [H2O-3 Sparse Principal Components](http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.glrm.html) | "Builds a generalized low rank decomposition of an H2O data frame." |
| [iBreakDown](https://github.com/ModelOriented/iBreakDown)-![](https://img.shields.io/github/stars/ModelOriented/iBreakDown?style=social) | "A model agnostic tool for explanation of predictions from black boxes ML models."|
| [ICEbox: Individual Conditional Expectation Plot Toolbox](https://cran.r-project.org/web/packages/ICEbox/index.html) | "Implements Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm."|
| [iml](https://github.com/christophM/iml)-![](https://img.shields.io/github/stars/christophM/iml?style=social) | "An R package that interprets the behavior and explains predictions of machine learning models."|
| [ingredients](https://github.com/ModelOriented/ingredients)-![](https://img.shields.io/github/stars/ModelOriented/ingredients?style=social) | "A collection of tools for assessment of feature importance and feature effects."|
| [interpret: Fit Interpretable Machine Learning Models](https://cran.r-project.org/web/packages/interpret/index.html) | "Package for training interpretable machine learning models."|
| [lightgbmExplainer](https://github.com/lantanacamara/lightgbmExplainer)-![](https://img.shields.io/github/stars/lantanacamara/lightgbmExplainer?style=social) | "An R package that makes LightGBM models fully interpretable."|
| [lime](https://github.com/thomasp85/lime)-![](https://img.shields.io/github/stars/thomasp85/lime?style=social) | "R port of the Python lime package."|
| [live](https://cran.r-project.org/web/packages/live/index.html) | "Helps to understand key factors that drive the decision made by complicated predictive model (black box model)."|
| [mcr](https://github.com/aaronjfisher/mcr)-![](https://img.shields.io/github/stars/aaronjfisher/mcr?style=social) | "An R package for Model Reliance and Model Class Reliance."|
| [modelDown](https://cran.r-project.org/web/packages/modelDown/index.html) | "Website generator with HTML summaries for predictive models."|
| [modelOriented](https://github.com/ModelOriented)-![](https://img.shields.io/github/stars/ModelOriented?style=social) | GitHub repositories of Warsaw-based MI².AI. |
| [modelStudio](https://github.com/ModelOriented/modelStudio)-![](https://img.shields.io/github/stars/ModelOriented/modelStudio?style=social) | "Automates the explanatory analysis of machine learning predictive models."|
| [Monotonic](http://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html) [XGBoost](http://xgboost.readthedocs.io/en/latest/) | Enforces consistent, directional relationships between features and predicted outcomes, enhancing model performance by aligning with prior data expectations. |
| [quantreg](https://cran.r-project.org/web/packages/quantreg/index.html) | "Estimation and inference methods for models for conditional quantile functions." |
| [rpart](https://cran.r-project.org/web/packages/rpart/index.html) | "Recursive partitioning for classification, regression and survival trees." |
| [RuleFit](http://statweb.stanford.edu/~jhf/R_RuleFit.html) | "Implements the learning method and interpretational tools described in *Predictive Learning via Rule Ensembles*." |
| [Scalable Bayesian Rule Lists -SBRL](https://users.cs.duke.edu/~cynthia/code/sbrl_1.0.tar.gz) | A more scalable implementation of Bayesian rule list from the Rudin group at Duke. |
| [shapFlex](https://github.com/nredell/shapFlex)-![](https://img.shields.io/github/stars/nredell/shapFlex?style=social) | Computes stochastic Shapley values for machine learning models to interpret them and evaluate fairness, including causal constraints in the feature space. |
| [shapleyR](https://github.com/redichh/ShapleyR)-![](https://img.shields.io/github/stars/redichh/ShapleyR?style=social) | "An R package that provides some functionality to use mlr tasks and models to generate shapley values." |
| [shapper](https://cran.r-project.org/web/packages/shapper/index.html) | "Provides SHAP explanations of machine learning models." |
| [smbinning](https://cran.r-project.org/web/packages/smbinning/index.html) | "A set of functions to build a scoring model from beginning to end." |
| [vip](https://github.com/koalaverse/vip)-![](https://img.shields.io/github/stars/koalaverse/vip?style=social) | "An R package for constructing variable importance plots (VIPs)." |
| [xgboostExplainer](https://github.com/AppliedDataSciencePartners/xgboostExplainer)-![](https://img.shields.io/github/stars/AppliedDataSciencePartners/xgboostExplainer?style=social) | "An R package that makes xgboost models fully interpretable. |

### Archived

#### Archived: Official Policy, Frameworks, and Guidance

For official government files pertaining to responsible AI practices that have been taken offline, we provide Wayback Machine mirror links below. If a document is still available on its original official domain, it can currently be found in its respective subsection above, although it may later be incorporated into this list. Documents may be removed for various reasons (whether political or through routine updates), but archiving them ensures they remain accessible for historical reference. If you're a researcher who finds a dead link to an older version of a government document or one that has altogether been deleted without comment, please feel free to submit a pull request drawing our attention to it and we'll consider it for inclusion. Where possible, we provide links to what appear to be the most recent URLs that governments may want the public to access.

* [Artificial Intelligence and Worker Well-Being: Principles and Best Practices for Developers and Employers](https://web.archive.org/web/20250205182942/https://www.dol.gov/sites/dolgov/files/general/ai/AI-Principles-Best-Practices.pdf) | United States, Department of Labor, archived February 5, 2025
* [Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People, HTML](https://web.archive.org/web/20250119213350/https://www.whitehouse.gov/ostp/ai-bill-of-rights/) | United States, The White House, Office of Science and Technology Policy, October 4, 2022, archived January 20, 2025
* [Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People, PDF](https://web.archive.org/web/20250119213350/https://www.whitehouse.gov/ostp/ai-bill-of-rights/) | United States, The White House, Office of Science and Technology Policy, October 4, 2022, archived January 20, 2025
* [CISA Roadmap for Artificial Intelligence 2023 2024](https://web.archive.org/web/20250221181621/https://www.cisa.gov/sites/default/files/2023-11/2023-2024_CISA-Roadmap-for-AI_508c.pdf) | United States, Cybersecurity and Infrastructure Security Agency, November 2023
* [Data Availability and Transparency Act 2022](https://web.archive.org/web/20240314232025/https://www.datacommissioner.gov.au/law/dat-act)| Australia, Office of the National Data Commissioner, April 1, 2022, archived March 14, 2024
* [Developing Financial Sector Resilience in a Digital World: Selected Themes in Technology and Related Risks](https://web.archive.org/web/20230802000841/https://www.osfi-bsif.gc.ca/Eng/Docs/tchrsk.pdf) | Canada, Office of the Superintendent of Financial Institutions of Canada, September 2020, archived August 2, 2023
* [Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence](https://web.archive.org/web/20250120132537/https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) | United States, The White House, October 30, 2023, archived January 20, 2025
* [FACT SHEET: Biden-⁠Harris Administration Announces New AI Actions and Receives Additional Major Voluntary Commitment on AI](https://web.archive.org/web/20250120101805/https://www.whitehouse.gov/briefing-room/statements-releases/2024/07/26/fact-sheet-biden-harris-administration-announces-new-ai-actions-and-receives-additional-major-voluntary-commitment-on-ai/) | United States, The White House, July 26, 2024, archived January 20, 2025
* [FACT SHEET: Biden-⁠Harris Administration Outlines Coordinated Approach to Harness Power of AI for U.S. National Security](https://web.archive.org/web/20250119050242/https://www.whitehouse.gov/briefing-room/statements-releases/2024/10/24/fact-sheet-biden-harris-administration-outlines-coordinated-approach-to-harness-power-of-ai-for-u-s-national-security/) | United States, The White House, October 24, 2024, archived January 19, 2025
* [FACT SHEET: Biden-⁠Harris Administration Secures Voluntary Commitments from Leading Artificial Intelligence Companies to Manage the Risks Posed by AI](https://web.archive.org/web/20250120131235/https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/) | United States, The White House, July 21, 2023, archived January 20, 2025
* [FACT SHEET: Biden-⁠Harris Administration Takes New Steps to Advance Responsible Artificial Intelligence Research, Development, and Deployment](https://web.archive.org/web/20250117044009/https://www.whitehouse.gov/briefing-room/statements-releases/2023/05/23/fact-sheet-biden-harris-administration-takes-new-steps-to-advance-responsible-artificial-intelligence-research-development-and-deployment/) | United States, The White House, May 23, 2023, archived January 17, 2025
* [FACT SHEET: President Biden Issues Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence](https://web.archive.org/web/20250118214923/https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/) | United States, The White House, October 30, 2023, archived January 18, 2025
* [Federal Register of Legislation, Data Availability and Transparency Act 2022](https://www.legislation.gov.au/C2022A00011/latest/text)
* [Generative Artificial Intelligence Lexicon](https://web.archive.org/web/20240926203350/https://www.ai.mil/lexicon_ai_terms.html) | United States, Department of Defense, Chief Digital and Artificial Intelligence Office (CDAO), archived September 26, 2024
* [Generative Artificial Intelligence Risk Assessment SIMM 5305-F](https://web.archive.org/web/20240524154534/https://cdt.ca.gov/wp-content/uploads/2024/03/SIMM-5305-F-Generative-Artificial-Intelligence-Risk-Assessment-FINAL.pdf) | State of California, Department of Technology, Office of Information Security, March 2024, archived May 24, 2024
  * [Generative Artificial Intelligence Risk Assessment SIMM 5305-F February 2025 update](https://cdt.ca.gov/wp-content/uploads/2025/01/SIMM-5305-F-GenAI-Risk-Assessment-2025_0131-final.pdf)
* [Guidelines on the Application of Republic Act No. 10173 or the Data Privacy Act of 2012 DPA, Its Implementing Rules and Regulations, and the Issuances of the Commission to Artificial Intelligence Systems Processing Personal Data NPC Advisory No. 2024-04](https://web.archive.org/web/20250112215325/https://privacy.gov.ph/wp-content/uploads/2024/12/Advisory-2024.12.19-Guidelines-on-Artificial-Intelligence-w-SGD.pdf) | Philippines, National Privacy Commission, December 19, 2024, archived January 12, 2025
* [Introducing the DATA Scheme](https://www.datacommissioner.gov.au/the-data-scheme)
* [M-21-06 Memorandum for the Heads of Executive Departments and Agencies, Guidance for Regulation of Artificial Intelligence Applications](https://web.archive.org/web/20250118013159/https://www.whitehouse.gov/wp-content/uploads/2020/11/M-21-06.pdf) | United States, Executive Office of the President, Office of Management and Budget, November 17, 2020, archived January 18, 2025
* [M-24-18 Memorandum for the Heads of Executive Departments and Agencies, Advancing the Responsible Acquisition of Artificial Intelligence in Government](https://web.archive.org/web/20250118023352/https://www.whitehouse.gov/wp-content/uploads/2024/10/M-24-18-AI-Acquisition-Memorandum.pdf) | United States, Executive Office of the President, Office of Management and Budget, September 24, 2024, archived January 18, 2025
* [Memorandum on Advancing the United States’ Leadership in Artificial Intelligence; Harnessing Artificial Intelligence to Fulfill National Security Objectives; and Fostering the Safety, Security, and Trustworthiness of Artificial Intelligence](https://web.archive.org/web/20250116072308/https://www.whitehouse.gov/briefing-room/presidential-actions/2024/10/24/memorandum-on-advancing-the-united-states-leadership-in-artificial-intelligence-harnessing-artificial-intelligence-to-fulfill-national-security-objectives-and-fostering-the-safety-security/) | United States, The White House, October 24, 2024, archived January 16, 2025
* [National Artificial Intelligence Research and Development Strategic Plan 2023 Update](https://web.archive.org/web/20250116083052/https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf) | United States, Executive Office of the President, National Science and Technology Council, Select Committee on Artificial Intelligence, May 2023, archived January 16, 2025
* [National Science and Technology Council](https://web.archive.org/web/20250118020849/https://www.whitehouse.gov/ostp/ostps-teams/nstc/) | United States, The White House, Office of Science and Technology Policy, January 16, 2021, archived January 18, 2025
* [Office of Science and Technology Policy](https://web.archive.org/web/20250120110259/https://www.whitehouse.gov/ostp/) | United States, The White House, Office of Science and Technology Policy, January 13, 2021, archived January 20, 2025
* [Supervisory Guidance on Model Risk Management](https://www.fdic.gov/news/financial-institution-letters/2017/fil17022a.pdf) | ( United States, Federal Deposit Insurance Corporation, archived February 13, 2024
 * [Aiming for truth, fairness, and equity in your company’s use of AI](https://web.archive.org/web/20250117235232/https://www.ftc.gov/business-guidance/blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai) | United States, Federal Trade Commission, Elisa Jillson, April 19, 2021, archived January 17, 2025
* [Using Artificial Intelligence and Algorithms](https://web.archive.org/web/20240115210007/https://www.ftc.gov/business-guidance/blog/2020/04/using-artificial-intelligence-and-algorithms) | United States, Federal Trade Commission, Andrew Smith, April 8, 2020, archived January 15, 2024
* [Validation of Employee Selection Procedures](https://web.archive.org/web/20250103095140/https://www.dol.gov/agencies/ofccp/faqs/employee-selection-procedures) | Office of Federal Contract Compliance Programs (archived)

### Citing Awesome Machine Learning Interpretability

Contributors with over 100 edits can be named coauthors in the citation of visible names. Otherwise, all contributors with fewer than 100 edits are included under "et al."

#### Bibtex

```
@misc{amli_repo,
  author={Patrick Hall and Daniel Atherton},
  title={Awesome Machine Learning Interpretability},
  year={2024},
  note={\url{https://github.com/jphall663/awesome-machine-learning-interpretability}}
}
```

#### ACM, APA, Chicago, and MLA

* **ACM (Association for Computing Machinery)**

Hall, Patrick, Daniel Atherton, et al. 2024. Awesome Machine Learning Interpretability. GitHub. https://github.com/jphall663/awesome-machine-learning-interpretability.

* **APA (American Psychological Association) 7th Edition**

Hall, Patrick, Daniel Atherton, et al. (2024). Awesome Machine Learning Interpretability [GitHub repository]. GitHub. https://github.com/jphall663/awesome-machine-learning-interpretability.

* **Chicago Manual of Style 17th Edition**

Hall, Patrick, Daniel Atherton, et al. "Awesome Machine Learning Interpretability." GitHub. Last modified 2023. https://github.com/jphall663/awesome-machine-learning-interpretability.

* **MLA (Modern Language Association) 9th Edition**

Hall, Patrick, Daniel Atherton, et al. "Awesome Machine Learning Interpretability." *GitHub*, 2024, https://github.com/jphall663/awesome-machine-learning-interpretability. Accessed 5 March 2024.
